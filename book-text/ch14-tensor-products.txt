364

Advanced Linear Algebra

equal to . We will consider the latter issue in some detail a bit later in the
chapter.
The fact that : is a basis for < n = gives the following.
Theorem 14.2 For finite-dimensional vector spaces < and = ,
dimÂ²< n = Â³ ~ dimÂ²< Â³ h dimÂ²= Â³

Construction II: Coordinate Free
The previous construction of the tensor product is reasonably intuitive, but has
the disadvantage of not being coordinate free. The following approach does not
require the choice of a basis.
Let -< d= be the vector space over - with basis < d = . Let : be the subspace
of -< d= generated by all vectors of the form
Â²"Ã $Â³ b Â²#Ã $Â³ c Â²" b #Ã $Â³

(14.1)

Â²"Ã #Â³ b Â²"Ã $Â³ c Â²"Ã # b $Â³

(14.2)

and

where Ã Â - and "Ã # and $ are in the appropriate spaces. Note that these
vectors are precisely what we must â€œidentifyâ€ as the zero vector in order to
enforce bilinearity. Put another way, these vectors are  if the ordered pairs are
replaced by tensors according to our previous construction.
Accordingly, the quotient space
< n= Â•

-< d=
:

is also sometimes taken as the definition of the tensor product of < and = .
(Strictly speaking, we should not be using the symbol < n = until we have
shown that this is the tensor product.) The elements of < n = have the form
4 Â²" Ã # Â³5 b : ~  Â²" Ã # Â³ b : !
However, since Â²"Ã #Â³ c Â²"Ã #Â³ Â : and Â²"Ã #Â³ c Â²"Ã #Â³ Â : , we can absorb
the scalar in either coordinate, that is,
Â´Â²"Ã #Â³ b :Âµ ~ Â²"Ã #Â³ b : ~ Â²"Ã #Â³ b :
and so the elements of < n = can be written simply as
Â´Â²" Ã # Â³ b :Âµ
It is customary to denote the coset Â²"Ã #Â³ b : by " n #, and so any element of

Tensor Products

365

< n = has the form
" n #
as in the previous construction.
The tensor map !Â¢ < d = Â¦ < n = is defined by
!Â²"Ã #Â³ ~ " n # ~ Â²"Ã #Â³ b :
This map is bilinear, since
!Â²" b #Ã $Â³ ~ Â²" b #Ã $Â³ b :
~ Â´Â²"Ã $Â³ b Â²#Ã $Â³Âµ b :
~ Â´Â²"Ã $Â³ b :Âµ b Â´ Â²#Ã $Â³ b :Âµ
~ !Â²"Ã $Â³ b !Â²#Ã $Â³
and similarly for the second coordinate.
We next prove that the pair Â²< n = Ã !Â¢ < d = Â¦ < n = Â³ is universal for
bilinearity when < n = is defined as a quotient space -< d= Â°: .
Theorem 14.3 Let < and = be vector spaces. The pair
Â²< n = Ã !Â¢ < d = Â¦ < n = Â³
is the tensor product of < and = .
Proof. Consider the diagram in Figure 14.5. Here -< d= is the vector space with
basis < d = .

t
U

j

V
f

FU V
V

S

U

V

W

W
Figure 14.5
Since
 k Â²"Ã #Â³ ~ Â²"Ã #Â³ ~ Â²"Ã #Â³ b : ~ " n # ~ !Â²"Ã #Â³
we have
!~k
The universal property of vector spaces described in Example 14.1 implies that

366

Advanced Linear Algebra

there is a unique linear transformation Â¢ -< d= Â¦ > for which
k~
Note that  sends the vectors (14.1) and (14.2) that generate : to the zero vector
and so : Â‹ kerÂ²Â³. For example,
Â´Â²"Ã $Â³ b Â²#Ã $Â³ c Â²" b #Ã $Â³Âµ
~ Â´Â²"Ã $Â³ b Â²#Ã $Â³ c Â²" b #Ã $Â³Âµ
~ Â²"Ã $Â³ b Â²#Ã $Â³ c Â²" b #Ã $Â³
~  Â²"Ã $Â³ b  Â²#Ã $Â³ c  Â²" b #Ã $Â³
~
and similarly for the second coordinate. Hence, Theorem 3.4 (the universal
property described in Example 14.2) implies that there exists a unique linear
transformation Â¢ < n = Â¦ > for which
 k ~
Hence,
 k!~ kk~k~
As to uniqueness, if  Z k ! ~  , then
 Z Â´Â²"Ã #Â³ b :Âµ ~  Â²"Ã #Â³ ~  Â´Â²"Ã #Â³ b :Âµ
and since the cosets Â²"Ã #Â³ b : generate -< d= Â°: , we conclude that  Z ~  .
Thus,  is the mediating morphism and Â²< n = Ã !Â³ is universal for bilinearity.
Let us take a moment to compare the two previous constructions. Let
Â¸ Â“  Â 0Â¹ and Â¸ Â“  Â 1 Â¹ be bases for < and = , respectively. Let Â²; Z Ã !Z Â³ be
the tensor product as constructed using these two bases and let
Â²; Ã !Â³ ~ Â²-< d= Â°:Ã !Â³ be the tensor product construction using quotient spaces.
Since both of these pairs are universal for bilinearity, Theorem 14.1 implies that
the mediating morphism  for ! with respect to !Z , that is, the map  Â¢ ; Z Â¦ ;
defined by
Â² n  Â³ ~ Â² Ã  Â³ b :
is a vector space isomorphism. Therefore, the basis Â¸Â² n  Â³Â¹ of ; Z is sent to
the set Â¸Â² Ã  Â³ b :Â¹, which is therefore a basis for ; .
In other words, given any two bases Â¸ Â“  Â 0Â¹ and Â¸ Â“  Â 1 Â¹ for < and = ,
respectively, the tensors  n  form a basis for < n = , regardless of which
construction of the tensor product we use. Therefore, we are free to think of
 n  either as a formal symbol belonging to a basis for < n = or as the coset
Â² Ã  Â³ b : belonging to a basis for < n = .

Tensor Products

367

Bilinearity on < d = Equals Linearity on < n =
The universal property for bilinearity says that to each bilinear function
 Â¢ < d = Â¦ > , there corresponds a unique linear function  Â¢ < n = Â¦ > ,
called the mediating morphism for  . Thus, we can define the mediating
morphism map
Â¢ homÂ²< Ã = Ã‚ > Â³ Â¦ BÂ²< n = Ã > Â³
by setting  ~  . In other words,  is the unique linear map for which
Â² Â³Â²" n #Â³ ~  Â²"Ã #Â³
Observe that  is itself linear, since if  Ã  Â homÂ²< Ã = Ã‚ > Â³, then
Â´Â² Â³ b Â²Â³ÂµÂ²" n #Â³ ~  Â²"Ã #Â³ b Â²"Ã #Â³ ~ Â² b Â³Â²"Ã #Â³
and so Â² Â³ b Â²Â³ is the mediating morphism for  b , that is,
Â² Â³ b Â²Â³ ~ Â² b Â³
Also,  is surjective, since if Â¢ < n = Â¦ > is any linear map, then
 ~  k !Â¢ < d = Â¦ > is bilinear and has mediating morphism  , that is,
 ~  . Finally,  is injective, for if  ~ , then  ~  k ! ~ . We have
established the following result.
Theorem 14.4 Let < , = and > be vector spaces over - . Then the mediating
morphism map Â¢ homÂ²< Ã = Ã‚ > Â³ Â¦ BÂ²< n = Ã > Â³, where  is the unique
linear map satisfying  ~  k !, is an isomorphism and so
Â¢ homÂ²< Ã = Ã‚ > Â³ Âš BÂ²< n = Ã > Â³

When Is a Tensor Product Zero?
Armed with the universal property of bilinearity, we can now discuss some of
the basic properties of tensor products. Let us first consider the question of
when a tensor  " n # is zero.
The bilinearity of the tensor product gives
 n # ~ Â² b Â³ n # ~  n # b  n #
and so  n # ~ . Similarly, " n  ~ . Now suppose that
 " n # ~ 


where we may assume that none of the vectors " and # are . Let
 Â¢ < d = Â¦ > be a bilinear map and let  Â¢ < n = Â¦ > be its mediating
morphism, that is,  k ! ~  . Then

368

Advanced Linear Algebra

 ~  4 " n # 5 ~  Â² k !Â³Â²" Ã # Â³ ~   Â²" Ã # Â³






The key point is that this holds for any bilinear function  Â¢ < d = Â¦ > . In
particular, let  Â < i and  Â = i and define  by
 Â²"Ã #Â³ ~ Â²"Â³ Â²#Â³
which is easily seen to be bilinear. Then the previous display becomes
 Â²" Â³ Â²# Â³ ~ 


If, for example, the vectors " are linearly independent, we can take  to be a
dual vector "i to get
 ~  "i Â²" Â³ Â²# Â³ ~  Â²# Â³


and since this holds for all linear functionals  Â = i , it follows that # ~ . We
have proved the following useful result.
Theorem 14.5 If " Ã Ãƒ Ã " are linearly independent vectors in < and
# Ã Ãƒ Ã # are arbitrary vectors in = , then
" n # ~ 

Â¬

# ~  for all 

In particular, " n # ~  if and only if " ~  or # ~ .

Coordinate Matrices and Rank
If 8 ~ Â¸" Â“  Â 0Â¹ is a basis for < and 9 ~ Â¸# Â“  Â 1 Â¹ is a basis for = , then
any vector ' Â < n = has a unique expression as a sum
' ~  Ã Â²" n # Â³
Â0 Â1

where only a finite number of the coefficients Ã are nonzero. In fact, for a
fixed ' Â < n = , we may reindex the bases so that




' ~  Ã Â²" n # Â³
~ ~

where none of the rows or columns of the matrix 9 ~ Â²Ã Â³ consists only of 's.
The matrix 9 ~ Â²Ã Â³ is called a coordinate matrix of ' with respect to the
bases 8 and 9.
Note that a coordinate matrix 9 is determined only up to the order of its rows
and columns. We could remove this ambiguity by considering ordered bases,

Tensor Products

369

but this is not necessary for our discussion and adds a complication, since the
bases may be infinite.
Suppose that M ~ Â¸$ Â“  Â 0Â¹ and N ~ Â¸% Â“  Â 1 Â¹ are also bases for < and
= , respectively, and that




' ~ 

Ã Â²$ n % Â³

~ ~

where : ~ Â² Ã Â³ is a coordinate matrix of ' with respect to these bases. We
claim that the coordinate matrices 9 and : have the same rank, which can then
be defined as the rank of the tensor ' Â < n = .
Each $ Ã Ãƒ Ã $ is a finite linear combination of basis vectors in 8 , perhaps
involving some of " Ã Ãƒ Ã " and perhaps involving other vectors in 8 . We can
further reindex 8 so that each $ is a linear combination of the vectors
8 Z ~ Â²" Ã Ãƒ Ã " Â³, where  Â  and set
< ~ spanÂ²" Ã Ãƒ Ã " Â³
Next, extend Â²$ Ã Ãƒ Ã $ Â³ to a basis M Z ~ Â²$ Ã Ãƒ Ã $ Ã $b Ã Ãƒ Ã $ Â³ for < .
(Since we no longer need the rest of the basis M , we have commandeered the
symbols $b Ã Ãƒ Ã $ , for simplicity.) Hence


$ ~  Ã " for  ~ Ã Ãƒ Ã 
~

where ( ~ Â²Ã Â³ is invertible of size  d .
Now repeat this process on the second coordinate. Reindex the basis 9 so that
the subspace = ~ spanÂ²# Ã Ãƒ Ã # Â³ contains % Ã Ãƒ Ã % and extend to a basis
N Z ~ Â²% Ã Ãƒ Ã % Ã %b Ã Ãƒ Ã % Â³ for = . Then


% ~  Ã # for  ~ Ã Ãƒ Ã 
~

where ) ~ Â²Ã Â³ is invertible of size  d .
Next, write




' ~  Ã Â²" n # Â³
~ ~

by setting Ã ~  for  Â€  or  Â€  . Thus, the  d  matrix 9 ~ Â²Ã Â³ comes
from 9 by adding  c  rows of 's to the bottom and then  c  columns of
's. In particular, 9 and 9 have the same rank.

370

Advanced Linear Algebra

The expression for ' in terms of the basis vectors $ Ã Ãƒ Ã $ and % Ã Ãƒ Ã % can
also be extended using  coefficients to




' ~ 

Ã Â²$ n % Â³

~ ~

where the  d  matrix : ~ Â² Ã Â³ has the same rank as : .
Now at last, we can compute. First, bilinearity gives




$ n % ~  Ã Ã " n # 
~ ~

and so






' ~ 
~ ~





Ã Â²$ n % Â³ ~ 
~ ~






Ã 8 Ã Ã " n # 9
~ ~




~ 8 Â²Ã Ã Â³Ã 9" n # 
~ ~

~ ~







~ 8 Â²(! : Â³Ã Ã 9" n # 
~ ~
 

~

~  2(! : ) 3Ã " n # 
~ ~

Thus








 Ã Â²" n # Â³ ~ ' ~  Â²(! : )Â³Ã Â²" n # Â³
~ ~

~ ~

and so 9 ~ (! : ) . Since ( and ) are invertible, we deduce that
rkÂ²9Â³ ~ rkÂ²9 Â³ ~ rkÂ²: Â³ ~ rkÂ²:Â³
as desired. Moreover, in block matrix terms, we can write
9



 ?block

and

: ~ >

!
(Ã
i

i
?
i block

and

)~>

9 ~ >

:



 ?block

and if we write
(! ~ >

then 9 ~ (! : ) implies that

)Ã
i

i
i ?block

Tensor Products

371

!
9 ~ (Ã
:)Ã

We shall soon have use for the following special case. If




' ~  "  n # ~  $  n % 
~

(14.3)

~

then 9 ~ : ~ 0 and so


$ ~  Ã " for  ~ Ã Ãƒ Ã 
~

and


% ~  Ã # for  ~ Ã Ãƒ Ã 
~

where if (Ã ~ Â²Ã Â³ and )Ã ~ Â²Ã Â³, then
!
0 ~ (Ã
)Ã

The Rank of a Decomposable Tensor
Recall that a tensor of the form " n # is said to be decomposable. If Â¸" Â“  Â 0Â¹
is a basis for < and Â¸# Â“  Â 1 Â¹ is a basis for = , then any decomposable vector
has the form
" n # ~    Â²" n # Â³
Ã

Hence, the rank of a decomposable vector is , since the rank of a matrix whose
Â²Ã Â³th entry is   is .

Characterizing Vectors in a Tensor Product
There are several useful representations of the tensors in < n = .
Theorem 14.6 Let Â¸" Â“  Â 0Â¹ be a basis for < and let Â¸# Â“  Â 1 Â¹ be a basis
for = . By an â€œessentially uniqueâ€ sum, we mean unique up to order and
presence of zero terms.
1) Every ' Â < n = has an essentially unique expression as a finite sum of
the form
 Ã " n #
Ã

where Ã Â - and the tensors " n # are distinct.

372

Advanced Linear Algebra

2) Every ' Â < n = has an essentially unique expression as a finite sum of
the form
" n &


where & Â = and the " 's are distinct.
3) Every ' Â < n = has an essentially unique expression as a finite sum of
the form
% n #


where % Â < and the # 's are distinct.
4) Every nonzero ' Â < n = has an expression of the form


' ~  %  n &
~

where the % 's are distinct, the & 's are distinct and the sets Â¸% Â¹ Â‹ < and
Â¸& Â¹ Â‹ = are linearly independent. As to uniqueness,  is the rank of ' and
so it is unique. Also, the equation




 %  n & ~  $  n ' 
~

~

where the $ 's are distinct, the ' 's are distinct and Â¸$ Â¹ Â‹ < and
Â¸' Â¹ Â‹ = are linearly independent, holds if and only if there exist invertible
 d  matrices ( ~ Â²Ã Â³ and ) ~ Â²Ã Â³ for which (! ) ~ 0 and




$ ~  Ã %

and ' ~  Ã &

~

~

for  ~ Ã Ãƒ Ã .
Proof. Part 1) merely expresses the fact that Â¸" n # Â¹ is a basis for < n = .
From part 2), we write
 Ã " n # ~  @" n Ã # A ~ " n &
Ã







Uniqueness follows from Theorem 14.5. Part 3) is proved similarly. As to part
4), we start with the expression from part 2):


" n &
~

where we may assume that none of the & 's are . If the set Â¸& Â¹ is linearly
independent, we are done. If not, then we may suppose (after reindexing if

Tensor Products

373

necessary) that
c

& ~   &
~

Then


c

c

" n & ~ " n & b 8" n   & 9
~

~
c

~

c

~ " n & b  " n & 
~
c

~

~ Â²" b  " Â³ n &
~

But the vectors Â¸" b  " Â“  Â  Â  c Â¹ are linearly independent. This
reduction can be repeated until the second coordinates are linearly independent.
Moreover, the identity matrix 0 is a coordinate matrix for ' and so
 ~ rkÂ²0 Â³ ~ rkÂ²'Â³. As to uniqueness, one direction was proved earlier; see
(14.3) and the other direction is left to the reader.
The proof of Theorem 14.6 shows that if ' Â£  and
' ~   n !
Â0

where  Â < and ! Â = , then if the multiset Â¸  Â“  Â 0Â¹ is not linearly
independent, we can rewrite ' in the form
' ~   n !Z
Â0

where Â¸  Â“  Â 0 Â¹ is linearly independent. Then we can do the same for the
second coordinate to arrive so at the representation
rkÂ²%Â³

' ~  %  n &
~

where the multisets Â¸% Â¹ and Â¸& Â¹ are linearly independent sets. Therefore,
rkÂ²%Â³ Â (0 ( and so the rank of ' is the smallest integer  for which ' can be
written as a sum of  decomposable tensors. This is often taken as the
definition of the rank of a tensor.
However, we caution the reader that there is another meaning to the word rank
when applied to a tensor, namely, it is the number of indices required to write
the tensor. Thus, a scalar has rank , a vector has rank , the tensor ' above has
rank  and a tensor of the form

374

Advanced Linear Algebra

' ~   n ! n " 
Â0

has rank .

Defining Linear Transformations on a Tensor Product
One of the simplest and most useful ways to define a linear transformation  on
the tensor product < n = is through the universal property, for this property
says precisely that a bilinear function  on < d = gives rise to a unique (and
well-defined) linear transformation on < n = . The proof of the following
theorem illustrates this well.
Theorem 14.7 Let < and = be vector spaces. There is a unique linear
transformation
Â¢ < i n = i Â¦ Â²< n = Â³i
defined by Â² n Â³ ~  p  where
Â² p Â³Â²" n #Â³ ~  Â²"Â³Â²#Â³
Moreover, is an embedding and is an isomorphism if < and = are finitedimensional. Thus, the tensor product  n  of linear functionals is (via this
embedding) a linear functional on tensor products.
Proof. Informally, for fixed  and , the function Â²"Ã #Â³ Â¦  Â²"Â³Â²#Â³ is bilinear
in " and # and so there is a unique linear map  p  taking " n # to  Â²"Â³Â²#Â³.
The function Â² Ã Â³ Â¦  p  is bilinear in  and  since
Â² b Â³ p  ~ Â² p Â³ b Â² p Â³
and so there is a unique linear map

taking  n  to  p .

More formally, for fixed  and , the map - Ã Â¢ < d = Â¦ - defined by
- Ã Â²"Ã #Â³ ~  Â²"Â³Â²#Â³
is bilinear and so the universal property of tensor products implies that there
exists a unique  p  Â Â²< n = Â³i for which
Â² p Â³Â²" n #Â³ ~  Â²"Â³Â²#Â³
Next, the map .Â¢ < i d = i Â¦ Â²< n = Â³i defined by
.Â² Ã Â³ ~  p 

Tensor Products

375

is bilinear since, for example,
Â´Â² b Â³ p ÂµÂ²" n #Â³ ~ Â² b Â³Â²"Â³ h Â²#Â³
~  Â²"Â³Â²#Â³ b Â²"Â³Â²#Â³
~ Â´Â² p Â³ b Â² p Â³ÂµÂ²" n #Â³
which shows that . is linear in its first coordinate. Hence, the universal
property implies that there exists a unique linear map
Â¢ < i n = i Â¦ Â²< n = Â³i
for which
Â² n Â³ ~  p 
To see that
the form

is an injection, if  Â < i n = i is nonzero, then we may write  in


 ~  n 
~

where the  Â < i are nonzero and Â¸ Â“  Â  Â Â¹ Â‹ = i is linearly
independent. If Â²Â³ ~ , then for any " Â < and # Â = , we have




 ~ Â²Â³Â²" n #Â³ ~  Â² n  Â³Â²" n #Â³ ~  Â²"Â³Â²#Â³
~

~

Hence, for each nonzero " Â < , the linear functional


 Â²"Â³
~

is the zero map and so the linear independence of Â¸ Â¹ implies that  Â²"Â³ ~ 
for all . Since " is arbitrary, it follows that  ~  for all  and so  ~ .
Finally, in the finite-dimensional case, the map
i

is a bijection since

i

dimÂ²< n = Â³ ~ dimÂ²Â²< n = Â³i Â³  B
Combining the isomorphisms of Theorem 14.4 and Theorem 14.7, we have, for
finite-dimensional vector spaces < and = ,
< i n = i Âš Â²< n = Â³i Âš homÂ²< Ã = Ã‚ - Â³

The Tensor Product of Linear Transformations
We wish to generalize Theorem 14.7 to arbitrary linear transformations. Let
 Â BÂ²< Ã < Z Â³ and  Â BÂ²= Ã = Z Â³. While the product  Â²"Â³Â²#Â³ does not make
sense, the tensor product  " n # does and is bilinear in " and #, that is, the
following function is bilinear:

376

Advanced Linear Algebra

 Â²"Ã #Â³ ~  " n #
The same argument that we used in the proof of Theorem 14.7 will work here.
Namely, the map Â²"Ã #Â³ Âª  " n # from < d = to < Z n = Z is bilinear in " and
# and so there is a unique linear map Â² p Â³Â¢ < n = Â¦ < Z n = Z for which
Â² p Â³Â²" n #Â³ ~  " n #
The function
Â¢ BÂ²< Ã < Z Â³ d BÂ²= Ã = Z Â³ Â¦ BÂ²< n = Ã < Z n = Z Â³
defined by
Â² Ã Â³ ~  p 
is bilinear, since
Â²Â² b  Â³ p Â³Â²" n #Â³ ~ Â² b  Â³Â²"Â³ n #
~ Â² " b  "Â³ n #
~ Â´ " n #Âµ b Â´" n #Âµ
~ Â² p Â³Â²" n #Â³ b Â² p Â³Â²" n #Â³
~ Â²Â² p Â³ b Â² p Â³Â³Â²" n #Â³
and similarly for the second coordinate. Hence, there is a unique linear
transformation
Â¢ BÂ²< Ã < Z Â³ n BÂ²= Ã = Z Â³ Â¦ BÂ²< n = Ã < Z n = Z Â³
satisfying
Â² n Â³ ~  p 
that is,
Â´ Â² n Â³ÂµÂ²" n #Â³ ~  " n #
To see that
write

is injective, if  Â BÂ²< Ã < Z Â³ n BÂ²= Ã = Z Â³ is nonzero, then we may


 ~  n 
~
Z

where the  Â BÂ²< Ã < Â³ are nonzero and the set Â¸ Â¹ Â‹ BÂ²= Ã = Z Â³ is linearly
independent. If Â²Â³ ~ , then for all " Â < and # Â = we have




 ~ Â²Â³Â²" n #Â³ ~  Â² n  Â³Â²" n #Â³ ~  Â²"Â³ n Â²#Â³
~

~

Since  Â£ , it follows that  Â£  for some  and so we may choose a " Â <
such that  Â²"Â³ Â£  for some . Moreover, we may assume, by reindexing if

Tensor Products

377

necessary, that the set Â¸ Â²"Â³Ã Ãƒ Ã  Â²"Â³Â¹ is a maximal linearly independent
subset of Â¸ Â²"Â³Ã Ãƒ Ã  Â²"Â³Â¹. Hence, for each  Â€ , we have


 Â²"Â³ ~ Ã  Â²"Â³
~

and so


 ~  Â²"Â³ n  Â²#Â³
~






~  Â²"Â³ n  Â²#Â³ b  @Ã  Â²"Â³A n  Â²#Â³
~


~b ~



~  Â²"Â³ n  Â²#Â³ b  Ã  Â²"Â³ n  Â²#Â³!
~

~b ~







~  Â²"Â³ n  Â²#Â³ b  Â²"Â³ n @  Ã  Â²#Â³A
~

~



~b



~  Â²"Â³ n @ Â²#Â³ b  Ã  Â²#Â³A
~

~b

Thus, the linear independence of Â¸ Â²"Â³Ã Ãƒ Ã  Â²"Â³Â¹ implies that for each
 Â ,


 Â²#Â³ b  Ã  Â²#Â³ ~ 
~b

for all # Â = and so


 b  Ã  ~ 
~b

But this contradicts the fact that the set Â¸ Â¹ is linearly independent. Hence, it
cannot happen that Â²Â³ ~  for  Â£  and so is injective.
The embedding of BÂ²< Ã < Z Â³ n BÂ²= Ã = Z Â³ into BÂ²< n = Ã < Z n = Z Â³ means that
each  n  can be thought of as the linear transformation  p  from < n = to
< Z n = Z , defined by
Â² p Â³Â²" n #Â³ ~  " n #
In fact, the notation  n  is often used to denote both the tensor product of
vectors (linear transformations) and the linear map  p  , and we will do this as
well. In summary, we can say that the tensor product  n  of linear
transformations is (up to isomorphism) a linear transformation on tensor
products.

378

Advanced Linear Algebra

Theorem 14.8 There is a unique linear transformation
Â¢ BÂ²< Ã < Z Â³ n BÂ²= Ã = Z Â³ Â¦ BÂ²< n = Ã < Z n = Z Â³
defined by Â² n Â³ ~  p  where
Â² p Â³Â²" n #Â³ ~  " n #
Moreover, is an embedding and is an isomorphism if all vector spaces are
finite-dimensional. Thus, the tensor product  n  of linear transformations is
(via this embedding) a linear transformation on tensor products.
Let us note a few special cases of the previous theorem.
Â—

Corollary 14.9 Let us use the symbol ? Ã† @ to denote the fact that there is an
embedding of ? into @ that is an isomorphism if ? and @ are finitedimensional.
1) Taking < Z ~ - gives
Â—

< i n BÂ²= Ã = Z Â³ Ã† BÂ²< n = Ã = Z Â³
where
Â² n Â³Â²" n #Â³ ~  Â²"Â³Â²#Â³
for  Â < i .
2) Taking < Z ~ - and = Z ~ - gives
Â—

< i n = i Ã† Â²< n = Â³i
where
Â² n Â³Â²" n #Â³ ~  Â²"Â³Â²#Â³
3) Taking = ~ - and noting that BÂ²- Ã = Z Â³ Âš = Z and < n - Âš < gives
(letting > ~ = Z )
Â—

BÂ²< Ã < Z Â³ n > Ã† BÂ²< Ã < Z n > Â³
where
Â² n $Â³Â²"Â³ ~  " n $
4) Taking < Z ~ - and = ~ - gives (letting > ~ = Z )
Â—

< i n > Ã† BÂ²< Ã > Â³
where
Â² n $Â³Â²"Â³ ~  Â²"Â³$

Tensor Products

379

Change of Base Field
The tensor product provides a convenient way to extend the base field of a
vector space that is more general than the complexification of a real vector
space, discussed earlier in the book. We refer to a vector space over a field - as
an - -space and write =- .
Actually, there are several approaches to â€œupgradingâ€ the base field of a vector
space. For instance, suppose that 2 is an extension field of - , that is, - Â‹ 2 .
If Â¸ Â¹ is a basis for =- , then every % Â =- has the form
% ~  
where  Â - . We can define a 2 -space =2 simply by taking all formal linear
combinations of the form
% ~  
where  Â 2 . Note that the dimension of =2 as a 2 -space is the same as the
dimension of =- as an - -space. Also, =2 is an - -space (just restrict the scalars
to - ) and as such, the inclusion map Â¢ =- Â¦ =2 sending % Â =- to
Â²%Â³ ~ % Â =2 is an - -monomorphism.
The approach described in the previous paragraph uses an arbitrarily chosen
basis for =- and is therefore not coordinate free. However, we can give a
coordinate-free approach using tensor products as follows. Since 2 is a vector
space over - , we can form the tensor product
>- ~ 2 n - =It is customary to include the subscript - on n - to denote the fact that the
tensor product is taken with respect to the base field - . (All relevant maps are
- -bilinear and - -linear.) However, since =- is not a 2 -space, the only tensor
product of 2 and =- that makes sense is the - -tensor product and so we will
drop the subscript - .
The tensor product >- is an - -space by definition of tensor product, but we
can make it into a 2 -space as follows. For  Â 2 , the temptation is to â€œabsorbâ€
the scalar  into the first coordinate,
Â² n #Â³ ~ Â² Â³ n #
but we must be certain that this is well-defined, that is,
n#~n$

Â¬

Â² Â³ n # ~ Â² Â³ n $

But for a fixed , the map Â² Ã #Â³ Âª Â² Â³ n # is bilinear and so the universal
property of tensor products implies that there is a unique linear map
 n # Âª Â² Â³ n #, which we define to be scalar multiplication by  .

380

Advanced Linear Algebra

To be absolutely clear, we have two distinct vector spaces: the - -space
>- ~ 2 n =- defined by the tensor product and the 2 -space >2 ~ 2 n =with scalar multiplication by elements of 2 defined as absorption into the first
coordinate. The spaces >- and >2 are identical as sets and as abelian groups.
It is only the â€œpermission to multiply byâ€ that is different. Accordingly, we can
recover >- from >2 simply by restricting scalar multiplication to scalars from
-.
Thus, we can speak of â€œ- -linearâ€ maps  from =- into >2 , with the expected
meaning, that is,
 Â²" b #Â³ ~  " b  #
for all scalars Ã Â - .
If the dimension of 2 as a vector space over - is  , then
dim- Â²>- Â³ ~ dim- Â²2 n =- Â³ ~  h dim- Â²=- Â³
As to the dimension of >2 , it is not hard to see that if Â¸ Â¹ is a basis for =- ,
then Â¸ n  Â¹ is a basis for >2 . Hence
dim2 Â²>2 Â³ ~ dim- Â²=- Â³
The map Â¢ =- Â¦ >- defined by # ~  n # is easily seen to be injective and
- -linear and so >- contains an isomorphic copy of =- . We can also think of 
as mapping =- into >2 , in which case  is called the 2 -extension map of =- .
This map has a universal property of its own, as described in the next theorem.
Theorem 14.10 The - -linear 2 -extension map Â¢ =- Â¦ 2 n =- has the
universal property for the family of all - -linear maps from =- into a 2 -space,
as measured by 2 -linear maps. Specifically, for any - -linear map  Â¢ =- Â¦ @ ,
where @ is a 2 -space, there exists a unique 2 -linear map  Â¢ 2 n =- Â¦ @ for
which the diagram in Figure 14.6 commutes, that is, for which
 k~
Proof. If such a 2 -linear map  Â¢ 2 n =- Â¦ @ is to exist, then it must satisfy,
for any  Â 2 ,
 Â² n #Â³ ~  Â² n #Â³ ~ Â²#Â³ ~   Â²#Â³
This shows that if  exists, it is uniquely determined by  . As usual, when
searching for a linear map  on a tensor product such as 2 n =- , we look for a
bilinear map. The map Â¢ Â²2 d =- Â³ Â¦ @ defined by
Â² Ã #Â³ ~   Â²#Â³

Tensor Products

381

is bilinear and so there exists a unique - -linear map  for which
 Â² n #Â³ ~   Â²#Â³
It is easy to see that  is also 2 -linear, since if  Â 2 , then
 Â´Â² n #Â³Âµ ~  Â² n #Â³ ~   Â²#Â³ ~  Â² n #Â³

P

VF

K

VF
W

f
Y
Figure 14.6
Theorem 14.10 is the key to describing how to extend an - -linear map to a 2 linear map. Figure 14.7 shows an - -linear map  Â¢ = Â¦ > between - -spaces =
and > . It also shows the 2 -extensions for both spaces, where 2 n = and
2 n > are 2 -spaces.

W

V

W

PV

PW

K V

W

K W

Figure 14.7
If there is a unique 2 -linear map  that makes the diagram in Figure 14.7
commute, then this would be the obvious choice for the extension of the - linear map  to a 2 -linear map.
Consider the - -linear map  ~ Â²> k  Â³Â¢ = Â¦ 2 n > into the 2 -space
2 n > . Theorem 14.10 implies that there is a unique 2 -linear map
Â¢ 2 n = Â¦ 2 n > for which
 k = ~ 
that is,
 k = ~  > k 
Now,  satisfies

382

Advanced Linear Algebra

 Â² n #Â³ ~  Â² n #Â³
~  Â² k = Â³Â²#Â³
~  Â²> k  Â³Â²#Â³
~  Â² n  #Â³
~  n #
~ Â²2 n  Â³Â² n #Â³
and so  ~ 2 n  .
Theorem 14.11 Let = and > be - -spaces, with 2 -extension maps = and
> , respectively. (See Figure 14.7.) Then for any - -linear map  Â¢ = Â¦ > , the
map 2 n  Â¢ 2 n = Â¦ 2 n > is the unique 2 -linear map that makes the
diagram in Figure 14.7 commute, that is, for which
 k  ~ Â²2 n  Â³ k 

Multilinear Maps and Iterated Tensor Products
The tensor product operation can easily be extended to more than two vector
spaces. We begin with the extension of the concept of bilinearity.
Definition If = Ã Ãƒ Ã = and > are vector spaces over - , a function
 Â¢ = d Ã„ d = Â¦ > is said to be multilinear if it is linear in each coordinate
separately, that is, if
 Â²" Ã Ãƒ Ã "c Ã # b #Z Ã "b Ã Ãƒ Ã " Â³
~  Â²" Ã Ãƒ Ã "c Ã #Ã "b Ã Ãƒ Ã " Â³ b  Â²" Ã Ãƒ Ã "c Ã #Z Ã "b Ã Ãƒ Ã "Â³
for all  ~ Ã Ãƒ Ã . A multilinear function of  variables is also referred to as
an -linear function. The set of all -linear functions as defined above will be
denoted by homÂ²= Ã Ãƒ Ã = Ã‚ > Â³. A multilinear function from = d Ã„ d = to
the base field - is called a multilinear form or -form.
Example 14.7
1) If ( is an algebra, then the product map Â¢ ( d Ã„ d ( Â¦ ( defined by
Â² Ã Ãƒ Ã  Â³ ~  Ã„ is -linear.
2) The determinant function detÂ¢ C Â¦ - is an -linear form on the columns
of the matrices in C .
The tensor product is defined via its universal property.
Definition As pictured in Figure 14.8, let = d Ã„ d = be the cartesian
product of vector spaces over - . A pair Â²; Ã !Â¢ = d Ã„ d = Â¦ ; Â³ is universal
for multilinearity if for every multilinear map  Â¢ = d Ã„ d = Â¦ > , there is
a unique linear transformation Â¢ ; Â¦ > for which

Tensor Products

383

 ~ k!
The map  is called the mediating morphism for  . If Â²; Ã !Â³ is universal for
multilinearity, then ; is called the tensor product of = Ã Ãƒ Ã = and denoted by
= n Ã„ n = . The map ! is called the tensor map.

V1uÂ˜Â˜Â˜uVn

t

f

V1 Â˜Â˜Â˜ Vn
W
W

Figure 14.8
As we have seen, the tensor product is unique up to isomorphism.
The basis construction and coordinate-free construction given earlier for the
tensor product of two vector spaces carry over to the multilinear case.
In particular, let 8 ~ Â¸Ã Â“  Â 1 Â¹ be a basis for = for  ~ Ã Ãƒ Ã . For each
ordered -tuple Â²Ã Ã Ãƒ Ã Ã Â³, construct a new formal symbol
Ã n Ã„ n Ã and define ; to be the vector space with basis
: ~ Â¸Ã n Ã„ n Ã Â“  Â 1 Â¹
The tensor map !Â¢ = d Ã„ d = Â¦ ; is defined by setting
!Â²Ã Ã Ãƒ Ã Ã Â³ ~ Ã n Ã„ n Ã
and extending by multilinearity. This uniquely defines a multilinear map ! that is
universal for multilinear functions from = d Ã„ d = .
Indeed, if Â¢ = d Ã„ d = Â¦ > is multilinear, the condition  ~  k ! is
equivalent to
Â²Ã n Ã„ n Ã Â³ ~  Â²Ã Ã Ãƒ Ã Ã Â³
which uniquely defines a linear map Â¢ ; Â¦ > . Hence, Â²; Ã !Â³ has the universal
property for multilinearity.
Alternatively, we may take the coordinate-free quotient space approach as
follows.
Definition Let = Ã Ãƒ Ã = be vector spaces over - and let < be the vector space
with basis = d Ã„ d = . Let : be the subspace of < generated by all vectors of
the form

384

Advanced Linear Algebra

Â²# Ã Ãƒ Ã #c Ã "Ã #b Ã Ãƒ Ã # Â³ b Â²# Ã Ãƒ Ã #c Ã "Z Ã #b Ã Ãƒ Ã #Â³
c Â²# Ã Ãƒ Ã #c Ã " b "Z Ã #b Ã Ãƒ Ã # Â³
for Ã Â - , "Ã "Z Â = and # Â = for  Â£  . The quotient space < Â°: is the
tensor product of = Ã Ãƒ Ã = and the tensor map is the map
!Â²# Ã Ãƒ Ã # Â³ ~ Â²# Ã Ãƒ Ã # Â³ b :
As before, we denote the coset Â²# Ã Ãƒ Ã # Â³ b : by # n Ã„ n # and so any
element of = n Ã„ n = is a sum of decomposable tensors, that is,
# n Ã„ n #
where the vector space operations are linear in each variable.
Here are some of the basic properties of multiple tensor products. Proof is left to
the reader.
Theorem 14.12 The tensor product has the following properties. Note that all
vector spaces are over the same field - .
1) (Associativity) There exists an isomorphism
Â¢ Â²= n Ã„ n = Â³ n Â²> n Ã„ n > Â³ Â¦ = n Ã„ n = n > n Ã„ n >
for which
Â´Â²# n Ã„ n # Â³ n Â²$ n Ã„ n $ Â³Âµ ~ # n Ã„ n # n $ n Ã„ n $
In particular,
Â²< n = Â³ n > Âš < n Â²= n > Â³ Âš < n = n >
2) (Commutativity) Let  be any permutation of the indices Â¸Ã Ãƒ Ã Â¹. Then
there is an isomorphism
Â¢ = n Ã„ n = Â¦ =Â²Â³ n Ã„ n =Â²Â³
for which
Â²# n Ã„ n # Â³ ~ #Â²Â³ n Ã„ n #Â²Â³
3) There is an isomorphism  Â¢ - n = Â¦ = for which
 Â² n #Â³ ~ #
and similarly, there is an isomorphism  Â¢ = n - Â¦ = for which
 Â²# n Â³ ~ #
Hence, - n = Âš = Âš = n - .
The analog of Theorem 14.4 is the following.

Tensor Products

385

Theorem 14.13 Let = Ã Ãƒ Ã = and > be vector spaces over - . Then the
mediating morphism map
Â¢ homÂ²= Ã Ãƒ Ã = Ã‚ > Â³ Â¦ BÂ²= n Ã„ n = Ã > Â³
defined by the fact that  is the unique mediating morphism for  is an
isomorphism. Thus,
homÂ²= Ã Ãƒ Ã = Ã‚ > Â³ Âš BÂ²= n Ã„ n = Ã > Â³
Moreover, if all vector spaces are finite-dimensional, then


dimÂ´homÂ²= Ã Ãƒ Ã = Ã‚ > Â³Âµ ~ dimÂ²> Â³ h dimÂ²=Â³
~

Theorem 14.8 and its corollary can also be extended.
Theorem 14.14 The linear transformation
Â¢ BÂ²< Ã <Z Â³ n Ã„ n BÂ²< Ã <Z Â³ Â¦ BÂ²< n Ã„ n < Ã <Z n Ã„ n <Z Â³
defined by
Â² n Ã„ n  Â³Â²" n Ã„ n " Â³ ~  " n Ã„ n  "
is an embedding and is an isomorphism if all vector spaces are finitedimensional. Thus, the tensor product  n Ã„ n  of linear transformations is
(via this embedding) a linear transformation on tensor products. Two important
special cases of this are
Â—

<i n Ã„ n <i Ã† Â²< n Ã„ n < Â³i
where
Â² n Ã„ n  Â³Â²" n Ã„ n " Â³ ~  Â²" Â³Ã„ Â²" Â³
and
Â—

<i n Ã„ n <i n = Ã† BÂ²< n Ã„ n < Ã = Â³
where
Â² n Ã„ n  n #Â³Â²" n Ã„ n " Â³ ~  Â²" Â³Ã„ Â²"Â³#

Tensor Spaces
Let = be a finite-dimensional vector space. For nonnegative integers  and  ,
the tensor product

386

Advanced Linear Algebra

; Â²= Â³ ~ = n Ã„
n= n =inÃ„
n = i ~ = n n Â²= i Â³n
Â’Â•Â•Â•Â“Â•Â•Â•Â”
Â’Â•Â•Â•Â•Â“Â•Â•Â•Â•Â”
 factors

 factors

is called the space of tensors of type Â²Ã Â³, where  is the contravariant type
and  is the covariant type. If  ~  ~ , then ; Â²= Â³ ~ - , the base field. Here
we use the notation = n for the -fold tensor product of = with itself. We will
also write = d for the -fold cartesian product of = with itself.
Since = Âš = ii , we have
; Â²= Â³ ~ = n n Â²= i Â³n Âš Â²Â²= i Â³n n = n Â³i Âš hom- Â²Â²= i Â³d d = d Ã - Â³
which is the space of all multilinear functionals on
=idÃ„
d=i d = dÃ„
d=
Â’Â•Â•Â•Â•Â“Â•Â•Â•Â•Â”
Â’Â•Â•Â•Â“Â•Â•Â•Â”
 factors

 factors

In fact, tensors of type Â²Ã Â³ are often defined as multilinear functionals in this
way.
Note that
dimÂ²; Â²= Â³Â³ ~ Â´dimÂ²= Â³Âµb
Also, the associativity and commutativity of tensor products allows us to write
b
; Â²= Â³ n ;  Â²= Â³ ~ ;b
Â²= Â³

at least up to isomorphism.
Tensors of type Â²Ã Â³ are called contravariant tensors
;  Â²= Â³ ~ ; Â²= Â³ ~ Â’Â•Â•Â•Â“Â•Â•Â•Â”
= nÃ„n=
 factors

and tensors of type Â²Ã Â³ are called covariant tensors
; Â²= Â³ ~ ; Â²= Â³ ~ Â’Â•Â•Â•Â•Â“Â•Â•Â•Â•Â”
=inÃ„n=i
 factors

Tensors with both contravariant and covariant indices are called mixed tensors.
In general, a tensor can be interpreted in a variety of ways as a multilinear map
on a cartesian product, or a linear map on a tensor product. Indeed, the
interpretation we mentioned above that is sometimes used as the definition is
only one possibility. We simply need to decide how many of the contravariant
factors and how many of the covariant factors should be â€œactive participantsâ€
and how many should be â€œpassive participants.â€

Tensor Products

387

More specifically, consider a tensor of type Â²Ã Â³, written
# n Ã„ n # n Ã„ n # n  n Ã„ n  n Ã„ n  Â ; Â²= Â³
where  Â  and  Â  . Here we are choosing the first  vectors and the first
 linear functionals as active participants. This determines the number of
arguments of the map. In fact, we define a map from the cartesian product
=idÃ„
d=i d = dÃ„
d=
Â’Â•Â•Â•Â•Â“Â•Â•Â•Â•Â”
Â’Â•Â•Â•Â“Â•Â•Â•Â”
 factors

 factors

to the tensor product
= nÃ„
n= n =inÃ„
n=i
Â’Â•Â•Â•Â“Â•Â•Â•Â”
Â’Â•Â•Â•Â•Â“Â•Â•Â•Â•Â”
c factors

c factors

of the remaining factors by
Â²# n Ã„ n # n  n Ã„ n  Â³Â² Ã Ãƒ Ã  Ã % Ã Ãƒ Ã %Â³
~  Â²# Â³Ã„ Â²# Â³ Â²% Â³Ã„ Â²% Â³#b n Ã„ n # n b n Ã„ n 
In words, the first group # n Ã„ n # of (active) vectors interacts with the first
group  Ã Ãƒ Ã  of arguments to produce the scalar  Â²# Â³Ã„ Â²# Â³. The first
group  n Ã„ n  of (active) functionals interacts with the second group
% Ã Ãƒ Ã % of arguments to produce the scalar  Â²% Â³Ã„ Â²% Â³. The remaining
(passive) vectors #b n Ã„ n # and functionals b n Ã„ n  are just
â€œcopiedâ€ to the image tensor.
It is easy to see that this map is multilinear and so there is a unique linear map
from the tensor product
=inÃ„
n=i n = nÃ„
n=
Â’Â•Â•Â•Â•Â“Â•Â•Â•Â•Â”
Â’Â•Â•Â•Â“Â•Â•Â•Â”
 factors

 factors

to the tensor product
= nÃ„
n= n =inÃ„
n=i
Â’Â•Â•Â•Â“Â•Â•Â•Â”
Â’Â•Â•Â•Â•Â“Â•Â•Â•Â•Â”
c factors

c factors

defined by
Â²# p Ã„ p # p  p Ã„ p  Â³Â² n Ã„ n  n % n Ã„ n %Â³
~  Â²# Â³Ã„ Â²# Â³ Â²% Â³Ã„ Â²% Â³#b n Ã„ n # n b n Ã„ n 
Moreover, the map
Â¢ = n n Â²= i Â³n Â¦ BÂ²Â²= i Â³n n = n Ã = nÂ²cÂ³ n Â²= i Â³nÂ²cÂ³ Â³
defined by
Â²# n Ã„ n # n  n Ã„ n  Â³ ~ # p Ã„ p # p  p Ã„ p 

388

Advanced Linear Algebra

is an isomorphism, since if # p Ã„ p # p  p Ã„ p  is the zero map then
 Â²# Â³Ã„ Â²# Â³ Â²% Â³Ã„ Â²% Â³#b n Ã„ n # n b n Ã„ n  ~ 
for all  Â = i and % Â = , which implies that
# n Ã„ n # n  n Ã„ n  ~ 
As usual, we denote the map # p Ã„ p # p  p Ã„ p  by
# n Ã„ n # n  n Ã„ n 
Theorem 11.15 For  Â  Â  and  Â  Â  ,
; Â²= Â³ Âš BÂ²Â²= i Â³n n = n Ã = nÂ²cÂ³ n Â²= i Â³nÂ²cÂ³ Â³
When  ~  and  ~  , we get
; Â²= Â³ Âš BÂ²Â²= i Â³n n = n Ã - Â³ ~ Â²Â²= i Â³n n = n Â³i
as before.
Let us look at some special cases. For  ~  we have
; Â²= Â³ Âš BÂ²Â²= i Â³n Ã = nÂ²cÂ³ Â³
where
Â²# n Ã„ n # Â³Â² n Ã„ n  Â³ ~  Â²# Â³Ã„ Â²# Â³#b n Ã„ n #
When  ~  ~ , we get for  ~  and  ~ ,
; Â²= Â³ Âš BÂ²- n = Ã = n - Â³ Âš BÂ²= Â³
where
Â²# n  Â³Â²$Â³ ~  Â²$Â³#
and for  ~  and  ~ ,
; Â²= Â³ Âš BÂ²= i n - Ã - n = i Â³ Âš BÂ²= i Ã = i Â³
where
Â²# n  Â³Â²Â³ ~ Â²#Â³
Finally, when  ~  ~ , we get a multilinear form
Â²# n  Â³Â²Ã $Â³ ~ Â²#Â³ Â²$Â³
Consider also a tensor  n  of type Â²Ã Â³. When  ~  ~  we get a
multilinear functional  n Â¢ Â²= d = Â³ Â¦ - defined by

Tensor Products

389

Â² n Â³Â²#Ã $Â³ ~  Â²#Â³Â²$Â³
This is just a bilinear form on = .

Contraction
Covariant and contravariant factors can be â€œcombinedâ€ in the following way.
Consider the map
c
Â¢ = d d Â²= i Â³d Â¦ ;c
Â²= Â³

defined by
Â²# Ã Ãƒ Ã # Ã  Ã Ãƒ Ã  Â³ ~  Â²# Â³Â²# n Ã„ n # n  n Ã„ n  Â³
This is easily seen to be multilinear and so there is a unique linear map
c
Â¢ ; Â²= Â³ Â¦ ;c
Â²= Â³

defined by
Â²# n Ã„ n # n  n Ã„ n  Â³ ~  Â²# Â³Â²# n Ã„ n # n  n Ã„ n  Â³
This is called the contraction in the contravariant index  and covariant index
. Of course, contraction in other indices (one contravariant and one covariant)
can be defined similarly.
Example 14.8 Let dimÂ²= Â³ Â€  and consider the tensor space ; Â²= Â³, which is
isomorphic to BÂ²= Â³ via the map
Â²# n  Â³Â²$Â³ ~  Â²$Â³#
For a â€œdecomposableâ€ linear operator of the form # n  as defined above with
# Â£  and  Â£ , we have kerÂ²# n  Â³ ~ kerÂ² Â³, which has codimension .
Hence, if  Â²$Â³Â²#Â³ ~ Â²# n  Â³Â²$Â³ Â£ , then
= ~ Âº$Â» l kerÂ² Â³ ~ Âº$Â» l ;
where ; is the eigenspace of # n  associated with the eigenvalue .
In particular, if  Â²#Â³ Â£ , then
Â²# n  Â³Â²#Â³ ~  Â²#Â³#
and so # is an eigenvector for the nonzero eigenvalue  Â²#Â³. Hence,
= ~ Âº#Â» l ; ~ ; Â²#Â³ l ;
and so the trace of # n  is

390

Advanced Linear Algebra

trÂ²# n  Â³ ~  Â²#Â³ ~ Â²# n  Â³
where

is the contraction map.

The Tensor Algebra of =
Consider the contravariant tensor spaces
;  Â²= Â³ ~ ; Â²= Â³ ~ = n
For  ~  we take ;  Â²= Â³ ~ - . The external direct sum
B

; Â²= Â³ ~  ;  Â²= Â³
~

of these tensor spaces is a vector space with the property that
;  Â²= Â³ n ;  Â²= Â³ ~ ; b Â²= Â³
This is an example of a graded algebra, where ;  Â²= Â³ are the elements of grade
. The graded algebra ; Â²= Â³ is called the tensor algebra over = . (We will
formally define graded structures a bit later in the chapter.)
Since
; Â²= Â³ ~ = i n Ã„
n = i ~ ;  Â²= i Â³
Â’Â•Â•Â•Â•Â“Â•Â•Â•Â•Â”
 factors

there is no need to look separately at ; Â²= Â³.

Special Multilinear Maps
The following definitions describe some special types of multilinear maps.
Definition
1) A multilinear map  Â¢ = d Â¦ > is symmetric if interchanging any two
coordinate positions changes nothing, that is, if
 Â²# Ã Ãƒ Ã # Ã Ãƒ Ã # Ã Ãƒ Ã # Â³ ~  Â²# Ã Ãƒ Ã #Ã Ãƒ Ã #Ã Ãƒ Ã #Â³
for any  Â£ .
2) A multilinear map  Â¢ = d Â¦ > is antisymmetric or skew-symmetric if
interchanging any two coordinate positions introduces a factor of c, that
is, if
 Â²# Ã Ãƒ Ã # Ã Ãƒ Ã # Ã Ãƒ Ã # Â³ ~ c Â²# Ã Ãƒ Ã #Ã Ãƒ Ã #Ã Ãƒ Ã #Â³
for  Â£ .

Tensor Products

391

3) A multilinear map  Â¢ =  Â¦ > is alternate or alternating if
# ~ # for some  Â£ 

Â¬

 Â²# Ã Ãƒ Ã # Â³ ~ 

As in the case of bilinear forms, we have some relationships between these
concepts. In particular, if charÂ²- Â³ ~ , then
alternate Â¬ symmetric Â¯ skew-symmetric
and if charÂ²- Â³ Â£ , then
alternate Â¯ skew-symmetric
A few remarks about permutations are in order. A permutation of the set
5 ~ Â¸Ã Ãƒ Ã Â¹ is a bijective function Â¢ 5 Â¦ 5 . We denote the group (under
composition) of all such permutations by : . This is the symmetric group on 
symbols. A cycle of length  is a permutation of the form Â² Ã  Ã Ãƒ Ã  Â³, which
sends " to "b for " ~ Ã Ãƒ Ã  c  and also sends  to  . All other elements
of 5 are left fixed. Every permutation is the product (composition) of disjoint
cycles.
A transposition is a cycle Â²Ã Â³ of length . Every cycle (and therefore every
permutation) is the product of transpositions. In general, a permutation can be
expressed as a product of transpositions in many ways. However, no matter how
one represents a given permutation as such a product, the number of
transpositions is either always even or always odd. Therefore, we can define the
parity of a permutation  Â : to be the parity of the number of transpositions
in any decomposition of  as a product of transpositions. The sign of a
permutation is defined by
sgÂ²Â³ ~ F


c

 has even parity
 has odd parity

If sgÂ²Â³ ~ , then  is an even permutation and if sgÂ²Â³ ~ c, then  is an
odd permutation. The sign of  is often written Â²cÂ³ .
With these facts in mind, it is apparent that  is symmetric if and only if
 Â²# Ã Ãƒ Ã # Â³ ~  Â²#Â²1) Ã Ãƒ Ã #Â²Â³ Â³
for all permutations  Â : and that  is skew-symmetric if and only if
 Â²# Ã Ãƒ Ã # Â³ ~ Â²cÂ³  Â²#Â²1) Ã Ãƒ Ã #Â²Â³ Â³
for all permutations  Â : .
A word of caution is in order with respect to the notation above, which is very
convenient albeit somewhat prone to confusion. It is intended that a permutation
 permutes the coordinate positions in  , not the indices (despite appearances).
Suppose, for example, that  Â¢ s d s Â¦ ? and that Â¸ Ã  Â¹ is a basis for s .

392

Advanced Linear Algebra

If  ~ Â² Â³, then  applied to  Â² Ã  Â³ gives  Â² Ã  Â³ and not  Â² Ã  Â³, since
 permutes the two coordinate positions in  Â²# Ã # Â³.

Graded Algebras
We need to pause for a few definitions that are useful in discussing tensor
algebras. An algebra ( over - is said to be a graded algebra if as a vector
space over - , ( can be written in the form
B

( ~  (
~

for subspaces ( of (, and where multiplication behaves nicely, that is,
( ( Â‹ (b
The elements of ( are said to be homogeneous of degree . If  Â ( is written
 ~  b Ã„ b 
for  Â ( ,  Â£  , then  is called the homogeneous component of  of
degree  .
The ring of polynomials - Â´%Âµ provides a prime example of a graded algebra,
since
B

- Â´%Âµ ~ - Â´%Âµ
~

where - Â´%Âµ is the subspace of - Â´%Âµ consisting of all scalar multiples of % .
More generally, the ring - Â´% Ã Ãƒ Ã % Âµ of polynomials in several variables is a
graded algebra, since it is the direct sum of the subspaces of homogeneous
polynomials of degree . (A polynomial is homogeneous of degree  if each
term has degree . For example,  ~ % % b % % % is homogeneous of degree
.)

The Symmetric and Antisymmetric Tensor Algebras
Our discussion of symmetric and antisymmetric tensors will benefit by a
discussion of a few definitions and setting a bit of notation at the outset.
Let - Â´ Ã Ãƒ Ã  Âµ denote the vector space of all homogeneous polynomials of
degree  (together with the zero polynomial) in the independent variables
 Ã Ãƒ Ã  . As is sometimes done in this context, we denote the product in
- Â´ Ã Ãƒ Ã  Âµ by v , for example, writing    as  v  v  . The algebra of
all polynomials in  Ã Ãƒ Ã  is denoted by - Â´ Ã Ãƒ Ã  Âµ.

Tensor Products

393

We will also need the counterpart of - Â´ Ã Ãƒ Ã  Âµ in which multiplication acts
anticommutatively, that is,   ~ c  .
Definition Let , ~ Â² Ã Ãƒ Ã  Â³ be a sequence of independent variables. For
 Â , let -c Â´ Ã Ãƒ Ã  Âµ be the vector space over - with basis
7 Â²,Â³ ~ Â¸ Ã„ Â“     Ã„   Â¹
consisting of all words of length  over , that are in ascending order. Let
-c Â´ Ã Ãƒ Ã  Âµ ~ - , which we identify with - by identifying  with  Â - .
Define a product on the direct sum


- c Â´ Ã Ãƒ Ã  Âµ ~  -c
~

as follows. First, the product  w  of monomials  ~ % Ã„% Â -c and
 ~ & Ã„& Â - c is defined as follows:
1) If % Ã„% & Ã„& has a repeated factor then  w  ~ .
2) Otherwise, reorder % Ã„% & Ã„& in ascending order, say ' Ã„'b , via the
permutation  and set
 w  ~ Â²cÂ³ ' Ã„'b
Extend the product by distributivity to - c Â´ Ã Ãƒ Ã  Âµ. The resulting product
makes - c Â´ Ã Ãƒ Ã  Âµ into a (noncommutative) algebra over - . This product is
called the wedge product or exterior product on - c Â´ Ã Ãƒ Ã  Âµ.
For example, by definition of wedge product,
 w  w  ~ c w  w 
Let 8 ~ Â¸ Ã Ãƒ Ã  Â¹ be a basis for = . It will be convenient to group the
decomposable basis tensors  n Ã„ n  according to their index multiset.
Specifically, for each multiset 4 ~ Â¸ Ã Ãƒ Ã  Â¹ with  Â  Â , let .4 be the
set of all tensors
 n Ã„ n 
where Â² Ã Ãƒ Ã  Â³ is a permutation of Â¸ Ã Ãƒ Ã  Â¹. For example, if
4 ~ Â¸Ã Ã Â¹, then
.4 ~ Â¸ n  n  Ã  n  n  Ã  n  n  Â¹
If # Â ;  Â²= Â³ has the form
# ~   ÃÃƒÃ  n Ã„ n 
 ÃÃƒÃ

where  ÃÃƒÃ Â£ , then let .4 Â²#Â³ be the subset of .4 whose elements appear

394

Advanced Linear Algebra

in the sum for #. For example, if
# ~  n  n  b  n  n  b  n  n 
then
.Â¸ÃÃÂ¹ Â²#Â³ ~ Â¸ n  n  Ã  n  n  Â¹
Let :4 Â²#Â³ denote the sum of the terms of # associated with .4 Â²#Â³. For
example,
:Â¸ÃÃÂ¹ Â²#Â³ ~  n  n  b  n  n 
Thus, # can be written in the form
p
s
 ! !
# ~ :4 Â²#Â³ ~ 
t
4
4 q!Â.4 Â²#Â³
where the sum is over a collection of multisets 4 with :4 Â²#Â³ Â£ . Note also
that ! Â£  since ! Â .4 Â²#Â³. Finally, let
"4 ~  n Ã„ n 
be the unique member of .4 for which  Â  Â Ã„ Â  .
Now we can get to the business at hand.

Symmetric and Antisymmetric Tensors
Let : be the symmetric group on Â¸Ã Ãƒ Ã Â¹. For each  Â : , the multilinear
map  Â¢ = d Â¦ ;  Â²= Â³ defined by
 Â²% Ã Ãƒ Ã % Â³ ~ % n Ã„ n %
determines a unique linear operator  on ;  Â²= Â³ for which
 Â²% n Ã„ n % Â³ ~ % n Ã„ n %
For example, if  ~  and  ~ Â² Â³, then
Â²Â³ Â²# n # n # Â³ ~ # n # n #
Let Â¸ Ã Ãƒ Ã  Â¹ be a basis for = . Since  is a bijection of the basis
8 ~ Â¸ n Ã„ n  Â“  Â 8 Â¹
it follows that  is an isomorphism of ;  Â²= Â³. Note also that  is a
permutation of each .4 , that is, the sets .4 are invariant under  .
Definition Let = be a finite-dimensional vector space.

Tensor Products

395

1) A tensor ! Â ;  Â²= Â³ is symmetric if
 ! ~ !
for all permutations  Â : . The set of all symmetric tensors
:;  Â²= Â³ ~ Â¸! Â ;  Â²= Â³ Â“  ! ~ ! for all  Â : Â¹
is a subspace of ;  Â²= Â³, called the symmetric tensor space of degree 
over = .
2) A tensor ! Â ;  Â²= Â³ is antisymmetric if
 ! ~ Â²cÂ³ !
The set of all antisymmetric tensors
(;  Â²= Â³ ~ Â¸! Â ;  Â²= Â³ Â“  ! ~ Â²cÂ³ ! for all  Â : Â¹
is a subspace of ;  Â²= Â³, called the antisymmetric tensor space or exterior
product space of degree  over = .
We can develop the theory of symmetric and antisymmetric tensors in tandem.
Accordingly, let us write (anti)symmetric to denote a tensor that is either
symmetric or antisymmetrtic.
Since for any Ã ! Â .4 , there is a permutation  taking
(anti)symmetric tensor # must have .4 Â²#Â³ ~ .4 and so

to !, an

# ~ :4 Â²#Â³ ~ 8  ! !9
4

4

!Â.4

Since  is a permutation of .4 , it follows that # is symmetric if and only if
 Â²:4 Â²#Â³Â³ ~ :4 Â²#Â³
for all  Â : and this holds if and only if the coefficients ! of :4 Â²#Â³ are
equal, say ! ~ 4 for all ! Â .4 . Hence, the symmetric tensors are precisely
the tensors of the form
# ~ 84  !9
4

!Â.4

The tensor # is antisymmetric if and only if
 Â²:4 Â²#Â³Â³ ~ Â²cÂ³ :4 Â²#Â³

(14.4)

In this case, the coefficients ! of :4 Â²#Â³ differ only by sign. Before examining
this more closely, we observe that 4 must be a set. For if 4 has an element 
of multiplicity greater than , we can split .4 into two disjoint parts:

396

Advanced Linear Algebra

Z
ZZ
.4 ~ .4
r .4
Z
where .4
are the tensors that have  in positions  and :
Z
.4
~ Â¸ n Ã„ n

 n Ã„ n  
position 
position

n Ã„ n  Â¹

Z
ZZ
Then Â² Â³ fixes each element of .4
and sends the elements of .4
to other
ZZ
elements of .4 . Hence, applying Â² Â³ to the corresponding decomposition of
:4 Â²#Â³:
Z
ZZ
:4 Â²#Â³ ~ :4
b :4

gives
Z
ZZ
Z
ZZ
cÂ²:4
b :4
Â³ ~ c:4 ~ Â² Â³ :4 ~ :4
b Â² Â³ :4
Z
and so :4
~ , whence :4 Â²#Â³ ~ . Thus, 4 is a set.

Now, since for any  Â : ,
.4 ~ Â¸ ! Â“ ! Â .4 Â¹
equation (14.4) implies that
Â²cÂ³  ! ! ~  8  ! !9 ~  !  ! ~  c ! !
!Â.4

!Â.4

!Â.4

!Â.4

which holds if and only if c ! ~ Â²cÂ³ ! , or equivalently,
 ! ~ Â²cÂ³ !
for all ! Â .4 and  Â : . Choosing " ~ "4 ~  n Ã„ n  , where
  Ã„   , as standard-bearer, if "Ã! denotes the permutation for which
"Ã! Â²"Â³ ~ !, then
! ~ Â²cÂ³"Ã! "
Thus, # is antisymmetric if and only if it has the form
# ~ 84  Â²cÂ³"Ã! !9
4

!Â.4

where 4 ~ " Â£  and the sum is over a family of sets.
In summary, the symmetric tensors are

Tensor Products

397

# ~ 84  !9
4

!Â.4

where 4 is a multiset and the antisymmetric tensors are
# ~ 84  Â²cÂ³"Ã! !9
4

!Â.4

where 4 is a set.
We can simplify these expressions considerably by representing the inside sums
more succinctly. In the symmetric case, define a surjective linear map
Â¢ ;  Â²= Â³ Â¦ - Â´ Ã Ãƒ Ã  Âµ
by
Â² n Ã„ n  Â³ ~  v Ã„ v 
and extending by linearity. Since  takes every member of .4 to the same
monomial " ~  v Ã„ v  , where  Â Ã„ Â  , we have
 # ~  884  !99 ~ 4 (.4 ( "4
4

4

!Â.4

In the antisymmetric case, define a surjective linear map
Â¢ ;  Â²= Â³ Â¦ -c Â´ Ã Ãƒ Ã  Âµ
by
Â² n Ã„ n  Â³ ~  w Ã„ w 
and extending by linearity. Since
 ! ~ Â²cÂ³"Ã!  "4
we have
 # ~ 84  Â²cÂ³"Ã!  !9
4

!Â.4

~ 84   "4 9
4

!Â.4

~ 4 (.4 ( "4
4

398

Advanced Linear Algebra

Thus, in both cases,
 # ~ 4 (.4 ( "4
4

where "4 ~  n Ã„ n  with  Â  Â Ã„ Â  and
 "4 ~  v Ã„ v 

or

 "4 ~  w Ã„ w 

depending on whether # is symmetric or antisymmetric. However, in either case,
the monomials "4 are linearly independent for distinct multisets/sets 4 .
Therefore, if  # ~  then 4 (.4 ( ~  for all multisets/sets 4 . Hence, if
charÂ²- Â³ ~ , then 4 ~  and so # ~ . This shows that the restricted maps
 O:;  Â²= Â³ and  O(;  Â²= Â³ are isomorphisms.
Theorem 14.16 Let = be a finite-dimensional vector space over a field - with
charÂ²- Â³ ~ .
1) The symmetric tensor space :;  Â²= Â³ is isomorphic to the algebra
- Â´ Ã Ãƒ Ã  Âµ of homogeneous polynomials, via the isomorphism
 4 ÃÃƒÃ  n Ã„ n  5 ~  ÃÃƒÃ Â² v Ã„ v  Â³
2) For  Â , the antisymmetric tensor space (;  Â²= Â³ is isomorphic to the
algebra -c Â´ Ã Ãƒ Ã  Âµ of anticommutative homogeneous polynomials of
degree  , via the isomorphism
 4 ÃÃƒÃ  n Ã„ n  5 ~  ÃÃƒÃ Â² w Ã„ w  Â³
The direct sum
B

:; Â²= Â³ ~  :;  Â²= Â³ Âš - Â´ Ã Ãƒ Ã  Âµ
~

is called the symmetric tensor algebra of = and the direct sum


(; Â²= Â³ ~  (;  Â²= Â³ Âš - c Â´ Ã Ãƒ Ã  Âµ
~

is called the antisymmetric tensor algebra or the exterior algebra of = . These
vector spaces are graded algebras, where the product is defined using the vector
space isomorphisms described in the previous theorem to move the products of
- Â´ Ã Ãƒ Ã  Âµ and - c Â´ Ã Ãƒ Ã  Âµ to :; Â²= Â³ and (; Â²= Â³, respectively.
Thus, restricting the domains of the maps  gives a nice description of the
symmetric and antisymmetric tensor algebras, when charÂ²- Â³ ~  . However,
there are many important fields, such as finite fields, that have nonzero
characteristic. We can proceed in a different, albeit somewhat less appealing,

Tensor Products

399

manner that holds regardless of the characteristic of the base field. Namely,
rather than restricting the domain of  in order to get an isomorphism, we can
factor out by the kernel of  .
Consider a tensor
p
s
 ! !
# ~ :4 Â²#Â³ ~ 
t
4
4 q!Â.4 Â²#Â³
Since  sends elements of different groups .4 Â²#Â³ ~ Â¸! Ã Ãƒ Ã ! Â¹ to different
monomials in - Â´ Ã Ãƒ Ã  Âµ or -c Â´ Ã Ãƒ Ã  Âµ, it follows that # Â kerÂ² Â³ if and
only if Â²:4 Â²#Â³Â³ ~  for all 4 , that is, if and only if
!  ! b Ã„ b !  ! ~ 
In the symmetric case,  is constant on .4 Â²#Â³ and so # Â kerÂ² Â³ if and only if
! b Ã„ b ! ~ 
In the antisymmetric case,  ! ~ Â²cÂ³Ã  ! where Ã Â²! Â³ ~ ! and so
# Â kerÂ² Â³ if and only if
Â²cÂ³Ã ! b Ã„ b Â²cÂ³Ã ! ~ 
In both cases, we solve for ! and substitute into :4 Â²#Â³. In the symmetric case,
! ~ c! c Ã„ c !
and so
:4 Â²#Â³ ~ ! ! b Ã„ b ! ! ~ ! Â²! c ! Â³ b Ã„ b ! Â²! c ! Â³
In the antisymmetric case,
! ~ cÂ²cÂ³Ã ! c Ã„ c Â²cÂ³Ã !
and so
:4 Â²#Â³ ~ ! ! b Ã„ b ! !
~ ! Â²Â²cÂ³Ã ! c ! Â³ b Ã„ b ! Â²Â²cÂ³Ã ! c ! Â³
Since ! Â 8 , it follows that :4 Â²#Â³ and therefore #, is in the span of tensors of
the form  Â²!Â³ c ! in the symmetric case and Â²cÂ³  Â²!Â³ c ! in the
antisymmetric case, where  Â : and ! Â 8 .
Hence, in the symmetric case,
kerÂ² Â³ Â‹ 0 Â• Âº Â²!Â³ c ! Â“ ! Â 8 Ã  Â : Â»
and since  Â² Â²!Â³ c !Â³ ~ , it follows that kerÂ² Â³ ~ 0 . In the antisymmetric
case,

400

Advanced Linear Algebra

kerÂ² Â³ Â‹ 0 Â• ÂºÂ²cÂ³  Â²!Â³ c ! Â“ ! Â 8Ã  Â : Â»
and since  Â²Â²cÂ³  Â²!Â³ c !Â³ ~ , it follows that kerÂ² Â³ ~ 0 .
We now have quotient-space characterizations of the symmetric and
antisymmetric tensor spaces that do not place any restriction on the
characteristic of the base field.
Theorem 14.17 Let = be a finite-dimensional vector space over a field - .
1) The surjective linear map Â¢ ;  Â²= Â³ Â¦ - Â´ Ã Ãƒ Ã  Âµ defined by
 4 ÃÃƒÃ  n Ã„ n  5 ~  ÃÃƒÃ  v Ã„ v 
has kernel
0 ~ Âº Â²!Â³ c ! Â“ ! Â 8 Ã  Â : Â»
and so
;  Â²= Â³
Âš - Â´ Ã Ãƒ Ã  Âµ
0
The vector space ;  Â²= Â³Â°0 is also referred to as the symmetric tensor
space of degree  of = .
2) The surjective linear map Â¢ ;  Â²= Â³ Â¦ -c Â´ Ã Ãƒ Ã  Âµ defined by
 4 ÃÃƒÃ  n Ã„ n  5 ~  ÃÃƒÃ  w Ã„ w 
has kernel
0 ~ ÂºÂ²cÂ³  Â²!Â³ c ! Â“ ! Â 8Ã  Â : Â»
and so
;  Â²= Â³
Âš -c Â´ Ã Ãƒ Ã  Âµ
0
The vector space ;  Â²= Â³Â°0 is also referred to as the antisymmetric tensor
space or exterior product space of degree  of = .
The isomorphic exterior spaces (;  Â²= Â³ and ;  Â²= Â³Â°0 are usually denoted by
 = and the isomorphic exterior algebras (; Â²= Â³ and ; Â²= Â³Â°0 are usually
denoted by = .
Theorem 14.18 Let = be a vector space of dimension .

Tensor Products

401

1) The dimension of the symmetric tensor space :;  Â²= Â³ is equal to the
number of monomials of degree  in the variables  Ã Ãƒ Ã  and this is
dimÂ²:;  Â²= Â³Â³ ~ 6

bc
7


2) The dimension of the exterior tensor space  Â²= Â³ is equal to the number of
words of length  in ascending order over the alphabet , ~ Â¸ Ã Ãƒ Ã  Â¹
and this is


dimÂ² Â²= Â³Â³ ~ 6 7


Proof. For part 1), the dimension is equal to the number of multisets of size 
taken from an underlying set Â¸ Ã Ãƒ Ã  Â¹ of size . Such multisets correspond
bijectively to the solutions, in nonnegative integers, of the equation
% b Ã„ b %  ~ 
where % is the multiplicity of  in the multiset. To count the number of
solutions, invent two symbols % and Â°. Then any solution % ~  to the
previous equation can be described by a sequence of %'s and Â°'s consisting of 
%'s followed by one Â°, followed by  %'s and another Â°, and so on. For example,
if  ~ and  ~ , the solution  b  b  b  ~ corresponds to the sequence
%%%Â°%Â°Â°%%
Thus, the solutions correspond bijectively to sequences consisting of  %'s and
 c  Â°'s. To count the number of such sequences, note that such a sequence can
be formed by considering  b  c  â€œblanksâ€ and selecting  of these blanks for
the %'s. This can be done in
6

bc
7


ways.

The Universal Property
We defined tensor products through a universal property, which as we have seen
is a powerful technique for determining the properties of tensor products. It is
easy to show that the symmetric tensor spaces are universal for symmetric
multilinear maps and the antisymmetric tensor spaces are universal for
antisymmetric multilinear maps.
Theorem 14.19 Let = be a finite-dimensional vector space with basis
Â¸ Ã Ãƒ Ã  Â¹.
1) The pair Â²- Â´% Ã Ãƒ Ã % ÂµÃ !Â³, where !Â¢ = d Â¦ - Â´% Ã Ãƒ Ã %Âµ is the
multilinear map defined by

402

Advanced Linear Algebra

!Â² Ã Ãƒ Ã  Â³ ~  v Ã„ v 
is universal for symmetric -linear maps with domain = d ; that is, for any
symmetric -linear map  Â¢ = d Â¦ < where < is a vector space, there is a
unique linear map Â¢ - Â´% Ã Ãƒ Ã % Âµ Â¦ < for which
Â² v Ã„ v  Â³ ~  Â² Ã Ãƒ Ã  Â³
2) The pair Â²-c Â´% Ã Ãƒ Ã % ÂµÃ !Â³, where !Â¢ = d Â¦ -c Â´% Ã Ãƒ Ã %Âµ is the
multilinear map defined by
!Â² Ã Ãƒ Ã  Â³ ~  w Ã„ w 
is universal for antisymmetric -linear maps with domain = d ; that is, for
any antisymmetric -linear map  Â¢ = d Â¦ < where < is a vector space,
there is a unique linear map Â¢ -c Â´% Ã Ãƒ Ã % Âµ Â¦ < for which
Â² w Ã„ w  Â³ ~  Â² Ã Ãƒ Ã  Â³
Proof. For part 1), the property
Â² v Ã„ v  Â³ ~  Â² Ã Ãƒ Ã  Â³
does indeed uniquely define a linear transformation  , provided that it is welldefined. However,
 v Ã„ v  ~  v Ã„ v 
if and only if the multisets Â¸ Ã Ãƒ Ã  Â¹ and Â¸ Ã Ãƒ Ã  Â¹ are the same, which
implies that  Â² Ã Ãƒ Ã  Â³ ~  Â² Ã Ãƒ Ã  Â³, since  is symmetric.
For part 2), since  is antisymmetric, it is completely determined by the fact that
it is alternate and by its values on the basis of ascending words  w Ã„ w  .
Accordingly, the condition
Â² w Ã„ w  Â³ ~  Â² Ã Ãƒ Ã  Â³
uniquely defines a linear transformation  .

The Symmetrization Map
When charÂ²- Â³ ~ , we can define a linear map :Â¢ ;  Â²= Â³ Â¦ :;  Â²= Â³, called
the symmetrization map, by
:! ~


  !
[ Â:


Since   ~   , we have

Tensor Products

 Â²:!Â³ ~

403




   ! ~    ! ~   ! ~ :!
[ Â:
[ Â:
[ Â:






and so :! is symmetric. The reason for the factor Â°[ is that if # is a symmetric
tensor, then  # ~ # and so
:# ~



  # ~  # ~ #
[ Â:
[ Â:




that is, the symmetrization map fixes all symmetric tensors. It follows that for
any tensor ! Â ;  Â²= Â³,
:  ! ~ :!
Thus, : is idempotent and is therefore the projection map of ;  Â²= Â³ onto
imÂ²:Â³ ~ :;  Â²= Â³.

The Determinant
The universal property for antisymmetric multilinear maps has the following
corollary.
Corollary 14.20 Let = be a vector space of dimension  over a field - . Let
, ~ Â² Ã Ãƒ Ã  Â³ be an ordered basis for = . Then there is at most one
antisymmetric -linear form Â¢ = d Â¦ - for which
Â² Ã Ãƒ Ã  Â³ ~ 
Proof. According to the universal property for antisymmetric -linear forms, for
every antisymmetric -linear form  Â¢ = d Â¦ - satisfying  Â² Ã Ãƒ Ã  Â³ ~ ,
there is a unique linear map  Â¢  = Â¦ - for which
 Â² w Ã„ w  Â³ ~  Â² Ã Ãƒ Ã  Â³ ~ 
But  = has dimension  and so there is only one linear map Â¢  = Â¦ with Â² w Ã„ w  Â³ ~ . Therefore, if  and  are two such forms, then
 ~  ~  , from which it follows that
 ~  k ! ~  k ! ~ 
We now wish to construct an antisymmetric form Â¢ = d Â¦ - , which is unique
by the previous theorem. Let 8 be a basis for = . For any # Â = , write Â´#Âµ8Ã for
the th coordinate of the coordinate matrix Â´#Âµ8 . Thus,
# ~  Â´#Âµ8Ã 


For clarity, and since we will not change the basis, let us write Â´#Âµ for Â´#Âµ8Ã .

404

Advanced Linear Algebra

Consider the map Â¢ = d Â¦ - defined by
Â²# Ã Ãƒ Ã # Â³ ~  Â²cÂ³ Â´# Âµ Ã„Â´# Âµ
Â:

Then  is multilinear since
Â²# b " Ã Ãƒ Ã # Â³ ~  Â²cÂ³ Â´# b " Âµ Ã„Â´#Âµ
Â:

~  Â²cÂ³ Â²Â´# Âµ b Â´" Âµ Â³Ã„Â´# Âµ()
Â:

~   Â²cÂ³ Â´# Âµ Ã„Â´# Âµ
Â:

b   Â²cÂ³ Â´" Âµ Ã„Â´# Âµ
Â:

~ Â²# Ã Ãƒ Ã # Â³ b Â²" Ã # Ã Ãƒ Ã # Â³
and similarly for any coordinate position.
To see that  is alternating, and therefore antisymmetric since charÂ²- Â³ Â£ ,
suppose for instance that # ~ # . For any permutation  Â : , let
Z ~ Â² Â³
Then Z % ~ % for % Â£ Ã  and
Z  ~ 

and

Z  ~ 

Hence, Z Â£ . Also, since Â²Z Â³Z ~ , if the sets Â¸Ã Z Â¹ and Â¸Ã Z Â¹ intersect,
then they are identical. Thus, the distinct sets Â¸Ã Z Â¹ form a partition of : . It
follows that
Â²# Ã # Ã # Ã Ãƒ Ã # Â³ ~  Â²cÂ³ Â´# Âµ Â´# Âµ Ã„Â´#Âµ
Â:

~



Z




>Â²cÂ³ Â´# Âµ Â´# Âµ Ã„Â´# Âµ b Â²cÂ³ Â´#ÂµZ Â´#ÂµZ Ã„Â´#ÂµZ  ?

pairs Â¸ÃZ Â¹

But
Â´# Âµ Â´# Âµ ~ Â´# ÂµZ  Â´# ÂµZ 
Z

and since Â²cÂ³ ~ cÂ²cÂ³ , the sum of the two terms involving the pair Â¸Ã Z Â¹
is . Hence, Â²# Ã # Ã Ãƒ Ã # Â³ ~ . A similar argument holds for any coordinate
pair.

Tensor Products

405

Finally,
Â² Ã Ãƒ Ã  Â³ ~  Â²cÂ³ Â´ Âµ Ã„Â´ Âµ
Â:

~  Â²cÂ³ Ã Ã„Ã
Â:

~
Thus, the map  is the unique antisymmetric -linear form on = d for which
Â² Ã Ãƒ Ã  Â³ ~ .
Under the ordered basis ; ~ Â² Ã Ãƒ Ã  Â³, we can view = as the space -  of
coordinate vectors and view = d as the space 4 Â²- Â³ of  d  matrices, via the
isomorphism
Â²# Ã Ãƒ Ã # Â³ Âª

v Â´# Âµ
Ã…
w Â´# Âµ

Ã„

Â´# Âµ y
Ã…
Ã„ Â´# Âµ z

where all coordinate matrices are with respect to ; .
With this viewpoint,  becomes an antisymmetric -form on the columns of a
matrix ( ~ Â²Ã Â³ given by
Â²(Â³ ~  Â²cÂ³ Ã Ã„Ã
Â:

This is called the determinant of the matrix (.

Properties of the Determinant
Let us explore some of the properties of the determinant function.
Theorem 14.21 If ( Â 4 Â²- Â³, then
Â²(Â³ ~ Â²(! Â³
Proof. We have
Â²(Â³ ~  Â²cÂ³ Ã Ã„Ã
Â:
c

~  Â²cÂ³ c Ã Ã„c Ã
c Â:

~  Â²cÂ³ Ã Ã„Ã
Â:

~ Â²(! Â³
as desired.

406

Advanced Linear Algebra

Theorem 14.22 If (Ã ) Â 4 Â²- Â³, then
Â²()Â³ ~ Â²(Â³Â²)Â³
Proof. Consider the map ( Â¢ 4 Â²- Â³ Â¦ - defined by
( Â²?Â³ ~ Â²(?Â³
We can consider ( as a function on the columns of ? and think of it as a
composition




( Â¢ Â²? Â²Â³ Ã Ãƒ Ã ? Â²Â³ Â³ Âª Â²(? Â²Â³ Ã Ãƒ Ã (? Â²Â³ Â³ Âª Â²(?Â³
Each step in this map is multilinear and so ( is multilinear. It is also clear that
( is antisymmetric and so ( is a scalar multiple of the determinant function,
say ( Â²?Â³ ~ Â²?Â³. Then
Â²(?Â³ ~ ( Â²?Â³ ~ Â²?Â³
Setting ? ~ 0 gives Â²(Â³ ~  and so
Â²(?Â³ ~ Â²(Â³Â²?Â³
as desired.
Theorem 14.23 A matrix ( Â 4 Â²- Â³ is invertible if and only if Â²(Â³ Â£ .
Proof. If 7 Â 4 Â²- Â³ is invertible, then 7 7 c ~ 0 and so
Â²7 Â³Â²7 c Â³ ~ 
which shows that Â²7 Â³ Â£  and Â²7 c Â³ ~ Â°Â²7 Â³. Conversely, any matrix
( Â 4 Â²- Â³ is equivalent to a diagonal matrix
( ~ 7 +8
where 7 and 8 are invertible and + is diagonal with 's and 's on the main
diagonal. Hence,
Â²(Â³ ~ Â²7 Â³Â²+Â³Â²8Â³
and so if Â²(Â³ Â£ , then Â²+Â³ Â£ , which happens if and only if + ~ 0 ,
whence ( is invertible.

Exercises
1.
2.
3.

Show that if Â¢ > Â¦ ? is a linear map and Â¢ < d = Â¦ > is bilinear,
then  k Â¢ < d = Â¦ ? is bilinear.
Show that the only map that is both linear and -linear (for  Â‚ ) is the
zero map.
Find an example of a bilinear map Â¢ = d = Â¦ > whose image
imÂ² Â³ ~ Â¸ Â²"Ã #Â³ Â“ "Ã # Â = Â¹ is not a subspace of > .

Tensor Products
4.

407

Let 8 ~ Â¸" Â“  Â 0Â¹ be a basis for < and let 9 ~ Â¸# Â“  Â 1 Â¹ be a basis
for = . Show that the set
: ~ Â¸" n # Â“  Â 0Ã  Â 1 Â¹

is a basis for < n = by showing that it is linearly independent and spans.
Prove that the following property of a pair Â²> Ã Â¢ < d = Â¦ > Â³ with 
bilinear characterizes the tensor product Â²< n = Ã !Â¢ < d = Â¦ < n = Â³ up
to isomorphism, and thus could have been used as the definition of tensor
product: For a pair Â²> Ã Â¢ < d = Â¦ > Â³ with  bilinear if Â¸" Â¹ is a basis
for < and Â¸# Â¹ is a basis for = , then Â¸Â²" Ã # Â³Â¹ is a basis for > .
6. Prove that < n = Âš = n < .
7. Let ? and @ be nonempty sets. Use the universal property of tensor
products to prove that <?d@ Âš <? n <@ .
8. Let "Ã "Z Â < and #Ã #Z Â = . Assuming that " n # Â£ , show that
" n # ~ "Z n #Z if and only if "Z ~ " and #Z ~ c #, for  Â£ .
9. Let 8 ~ Â¸ Â¹ be a basis for < and 9 ~ Â¸ Â¹ be a basis for = . Show that any
function  Â¢ 8 d 9 Â¦ > can be extended to a linear function
 Â¢ < n = Â¦ > . Deduce that the function  can be extended in a unique
way to a bilinear map V Â¢ < d = Â¦ > . Show that all bilinear maps are
obtained in this way.
10. Let : Ã : be subspaces of < . Show that

5.

Â²: n = Â³ q Â²: n = Â³ Âš Â²: q : Â³ n =
11. Let : Â‹ < and ; Â‹ = be subspaces of vector spaces < and = ,
respectively. Show that
Â²: n = Â³ q Â²< n ; Â³ Âš : n ;
12. Let : Ã : Â‹ < and ; Ã ; Â‹ = be subspaces of < and = , respectively.
Show that
Â²: n ; Â³ q Â²: n ; Â³ Âš Â²: q : Â³ n Â²; n ; Â³
13. Find an example of two vector spaces < and = and a nonzero vector
% Â < n = that has at least two distinct (not including order of the terms)
representations of the form


% ~ " n #
~

where the " 's are linearly independent and so are the # 's.
14. Let ? denote the identity operator on a vector space ? . Prove that
= p > ~ = n> .

408

Advanced Linear Algebra

15. Suppose that  Â¢ < Â¦ = Ã  Â¢ = Â¦ > and  Â¢ < Z Â¦ =2 Ã  Â¢ =2 Â¦ > Z .
Prove that
Â² k  Â³ p Â² k  Â³ ~ Â² p  Â³ k Â² p  Â³
16. Connect the two approaches to extending the base field of an - -space = to
2 (at least in the finite-dimensional case) by showing that
-  n - 2 Âš Â²2Â³ .
17. Prove that in a tensor product < n < for which dimÂ²< Â³ Â‚  not all vectors
have the form " n # for some "Ã # Â < . Hint: Suppose that "Ã # Â < are
linearly independent and consider " n # b # n ".
18. Prove that for the block matrix
4 ~>

(


)
* ?block

we have Â²4 Â³ ~ Â²(Â³Â²*Â³.
19. Let (Ã ) Â 4 Â²- Â³. Prove that if either ( or ) is invertible, then the
matrices ( b ) are invertible except for a finite number of 's.

The Tensor Product of Matrices
20. Let ( ~ Â²Ã Â³ be the matrix of a linear operator  Â BÂ²= Â³ with respect to
the ordered basis 7 ~ Â²" Ã Ãƒ Ã " Â³. Let ) ~ Â²Ã Â³ be the matrix of a linear
operator  Â BÂ²= Â³ with respect to the ordered basis 8 ~ Â²# Ã Ãƒ Ã # Â³.
Consider the ordered basis 9 ~ Â²" n # Â³ ordered lexicographically; that is
" n #  "M n # if   M or  ~ M and    . Show that the matrix of
 n  with respect to 9 is
p Ã )
r )
( n ) ~ r Ã
Ã…
q Ã )

Ã )
Ã )
Ã…
Ã )

Ã„
Ã„

Ã ) s
Ã ) u
u
Ã…
Ã„ Ã ) tblock

This matrix is called the tensor product, Kronecker product or direct
product of the matrix ( with the matrix ) .
21. Show that the tensor product is not, in general, commutative.
22. Show that the tensor product ( n ) is bilinear in both ( and ) .
23. Show that ( n ) ~  if and only if ( ~  or ) ~ .
24. Show that
a) Â²( n )Â³! ~ (! n ) !
b) Â²( n )Â³i ~ (i n ) i (when - ~ d)
25. Show that if "Ã # Â -  , then (as row vectors) "! # ~ "! n #.
26. Suppose that (Ã Ã )Ã Ã *Ã and +Ã are matrices of the given sizes.
Prove that
Â²( n )Â³Â²* n +Â³ ~ Â²(*Â³ n Â²)+Â³
Discuss the case  ~  ~ .

Tensor Products

409

27. Prove that if ( and ) are nonsingular, then so is ( n ) and
Â²( n )Â³c ~ (c n ) c
28. Prove that trÂ²( n )Â³ ~ trÂ²(Â³ h trÂ²)Â³.
29. Suppose that - is algebraically closed. Prove that if ( has eigenvalues
 Ã Ãƒ Ã  and ) has eigenvalues  Ã Ãƒ Ã  , both lists including
multiplicity, then ( n ) has eigenvalues Â¸  Â“  Â Ã  Â Â¹, again
counting multiplicity.
30. Prove that detÂ²(Ã n )Ã Â³ ~ Â²detÂ²(Ã Â³Â³ Â²detÂ²)Ã Â³Â³ .

Chapter 15

Positive Solutions to Linear Systems:
Convexity and Separation

It is of interest to determine conditions that guarantee the existence of positive
solutions to homogeneous systems of linear equations
(% ~ 
where ( Â CÃ Â²sÂ³.
Definition Let # ~ Â² Ã Ãƒ Ã  Â³ Â s .
1) # is nonnegative, written # Â‚ , if
 Â‚  for all 
(The term positive is also used for this property.) The set of all nonnegative
vectors in s is the nonnegative orthant in s Ã€
2) # is strictly positive, written # Â€ , if # is nonnegative but not , that is, if
 Â‚  for all  and  Â€  for some 
The set sb of all strictly positive vectors in s is the strictly positive
orthant in s Ã€
3) # is strongly positive, written # Âˆ , if
 Â€  for all 
The set sbb of all strongly positive vectors in s is the strongly positive
orthant in s Ã€
We are interested in conditions under which the system (% ~  has strictly
positive or strongly positive solutions. Since the strictly and strongly positive
orthants in s are not subspaces of s, it is difficult to use strictly linear
methods in studying this issue: we must also use geometric methods, in
particular, methods of convexity.

412

Advanced Linear Algebra

Let us pause briefly to consider an important application of strictly positive
solutions to a system (% ~ . If ? ~ Â²% Ã Ãƒ Ã % Â³ is a strictly positive solution
to this system, then so is the vector
&~



?~
Â²% Ã Ãƒ Ã % Â³ ~ Â² Ã Ãƒ Ã  Â³
'% 
'% 

which is a probability distribution, that is,  Â  Â  and  ~ . Moreover,
if ? is a strongly positive solution, then & has the property that each probability
is positive.
Now, the product (& is the expected value of the columns of ( with respect to
the probability distribution &. Hence, (% ~  has a strictly (strongly) positive
solution if and only if there is a strictly (strongly) positive probability
distribution for which the columns of ( have expected value . If each column
of ( represents the possible payoffs from a game of chance, where each row is a
different possible outcome of the game, then the game is fair when the expected
value of the columns is . Thus, (% ~  has a strictly (strongly) positive
solution ? if and only if the game with payoffs ( and probabilities ? is fair.
As another (related) example, in discrete option pricing models of mathematical
finance, the absence of arbitrage opportunities in the model is equivalent to the
fact that a certain vector describing the gains in a portfolio does not intersect the
strictly positive orthant in s . As we will see in this chapter, this is equivalent
to the existence of a strongly positive solution to a homogeneous system of
equations. This solution, when normalized to a probability distribution, is called
a martingale measure.
Of course, the equation (% ~  has a strictly positive solution if and only if
kerÂ²(Â³ contains a strictly positive vector, that is, if and only if
kerÂ²(Â³ ~ RowSpaceÂ²(Â³Â
meets the strictly positive orthant in s . Thus, we wish to characterize the
subspaces : of s for which : Â meets the strictly positive orthant in s , in
symbols,
: Â q sb Â£ J
for these are precisely the row spaces of the matrices ( for which (% ~  has a
strictly positive solution. A similar statement holds for strongly positive
solutions.
Looking at the real plane s , we can divine the answer with a picture. A onedimensional subspace : of s has the property that its orthogonal complement
: Â meets the strictly positive orthant (quadrant) in s if and only if : is the %axis, the &-axis or a line with negative slope. For the case of the strongly

Positive Solutions to Linear Systems: Convexity and Separation

413

positive orthant, : must have negative slope. Our task is to generalize this to
s .
This will lead us to the following results, which are quite intuitive in s and s :
: Â q sbb Â£ J

Â¯

: Â q sb Â£ J

Â¯


: q sb
~J

(15.1)

: q sbb ~ J

(15.2)

and

Let us translate these statements into the language of the matrix equation
(% ~ . If : ~ RowSpaceÂ²(Â³, then : Â ~ kerÂ²(Â³ and so we have
kerÂ²(Â³ q sbb Â£ J

Â¯

kerÂ²(Â³ q sb Â£ J

Â¯


~J
RowSpaceÂ²(Â³ q sb

and

~J
RowSpaceÂ²(Â³ q sbb

Now,
RowSpaceÂ²(Â³ q sb ~ Â¸#( Â“ #( Â€ Â¹
and
RowSpaceÂ²(Â³ q sbb ~ Â¸#( Â“ #( Âˆ Â¹
and so these statements become
(% ~  has a strongly positive solution

Â¯

Â¸#( Â“ #( Â€ Â¹ ~ J

(% ~  has a strictly positive solution

Â¯

Â¸#( Â“ #( Âˆ Â¹ ~ J

and

We can rephrase these results in the form of a theorem of the alternative, that
is, a theorem that says that exactly one of two conditions holds.
Theorem 15.1 Let ( Â CÃ Â²sÂ³.
1) Exactly one of the following holds:
a) (" ~  for some strongly positive " Â s .
b) #( Â€  for some # Â s .
2) Exactly one of the following holds:
a) (" ~  for some strictly positive " Â s .
b) #( Âˆ  for some # Â s .
Before proving Theorem 15.1, we require some background.

Convex, Closed and Compact Sets
We shall need the following concepts.

414

Advanced Linear Algebra

Definition
1) Let % Ã Ãƒ Ã % Â s . Any linear combination of the form
! % b Ã„ b ! %
where  Â ! Â  and ! b Ã„ b ! ~  is called a convex combination of
the vectors % Ã Ãƒ Ã % .
2) A subset ? Â‹ s is convex if whenever %Ã & Â ? , then the line segment
between % and & also lies in ? , in symbols,
Â¸!% b Â² c !Â³& Â“  Â ! Â Â¹ Â‹ ?
3) A subset ? Â‹ s is closed if whenever Â²% Â³ is a convergent sequence of
elements of ? , then the limit is also in ? .
4) A subset ? Â‹ s is compact if it is both closed and bounded.
5) A subset ? Â‹ s is a cone if % Â ? implies that % Â ? for all  Â‚ .
We will also have need of the following facts from analysis.
1) A continuous function that is defined on a compact set ? in s takes on
maximum and minimum values at some points within the set ? .
2) A subset ? of s is compact if and only if every sequence in ? has a
subsequence that converges to a point in ? .
Theorem 15.2 Let ? and @ be subsets of s . Define
? b @ ~ Â¸ b  Â“  Â ?Ã  Â @ Â¹
1) If ? and @ are convex, then so is ? b @
2) If ? is compact and @ is closed, then ? b @ is closed.
Proof. For 1), let % b & and % b & be in ? b @ . The line segment between
these two points is
!Â²% b & Â³ b Â² c !Â³Â²% b & Â³
~ Â´!% b Â² c !Â³% Âµ b Â´!& b Â² c !Â³& Âµ Â ? b @
for  Â ! Â  and so ? b @ is convex.
For part 2), let % b & be a convergent sequence in ? b @ . Suppose that
% b & Â¦ ' . We must show that ' Â ? b @ . Since % is a sequence in the
compact set ? , it has a convergent subsequence % whose limit % lies in ? .
Since % b & Â¦ ' and % Â¦ % we can conclude that & Â¦ ' c %. Since @
is closed, it follows that ' c % Â @ and so ' ~ % b Â²' c %Â³ Â ? b @ .

Convex Hulls
We will also have use for the notion of the smallest convex set containing a
given set.

Positive Solutions to Linear Systems: Convexity and Separation

415

Definition The convex hull of a set : ~ Â¸% Ã Ãƒ Ã % Â¹ of vectors in s is the
smallest convex set in s that contains : . We will denote the convex hull of :
by 9Â²:Â³.
Here is a characterization of convex hulls.
Theorem 15.3 Let : ~ Â¸% Ã Ãƒ Ã % Â¹ be a set of vectors in s . Then the convex
hull 9Â²:Â³ is the set " of all convex combinations of vectors in : , that is,
9Â²:Â³ ~ " Â• D! % b Ã„ b ! % Â“  Â ! Â Ã ! ~ E
Proof. Clearly, if + is a convex set that contains : , then + also contains ".
Hence " Â‹ 9Â²:Â³. To prove the reverse inclusion, we need only show that " is
convex, since then : Â‹ " implies that 9Â²:Â³ Â‹ ". So let
? ~ ! % b Ã„ b ! %
@ ~  % b Ã„ b  % 
be in ". If  b  ~  and  Â Ã  Â  then
? b @ ~ Â²! % b Ã„ b ! % Â³ b Â²  % b Ã„ b
~ Â²! b   Â³% b Ã„ b Â²! b   Â³%

 % Â³

But this is also a convex combination of the vectors in : , because
 Â ! b   Â Â² b Â³ h maxÂ²  Ã ! Â³ ~ maxÂ² Ã !Â³ Â 
and






Â²! b   Â³ ~ ! b    ~  b  ~ 
~

~

~

Thus, ? b @ Â ".
Theorem 15.4 The convex hull 9Â²:Â³ of a finite set : ~ Â¸% Ã Ãƒ Ã % Â¹ of vectors
in s is a compact set.
Proof. The set
+ ~ DÂ²! Ã Ãƒ Ã ! Â³ Â“  Â ! Â Ã  ! ~ E
is closed and bounded in s and therefore compact. Define a function
 Â¢ + Â¦ s as follows: If ! ~ Â²! Ã Ãƒ Ã ! Â³, then
 Â²!Â³ ~ ! % b Ã„ b ! %
To see that  is continuous, let
 Â€ , if ) c !)  Â°4 then

~ Â²  Ã Ãƒ Ã  Â³ and let 4 ~ maxÂ²)% )Â³. Given

416

Advanced Linear Algebra

(  c ! ( Â ) c !) 


4

and so
) Â² Â³ c  Â²!Â³) ~ )Â²  c ! Â³% b Ã„ b Â²  c ! Â³% )
Â (  c ! ()% ) b Ã„ b (  c ! ()% )
Â 4 ) c !)
~
Finally, since  Â²+Â³ ~ 9Â²:Â³, it follows that 9Â²:Â³ is compact.

Linear and Affine Hyperplanes
We next discuss hyperplanes in s . A linear hyperplane in s is an Â² c Â³dimensional subspace of s . As such, it is the solution set of a linear equation
of the form
 % b Ã„ b  % ~ 
or
Âº5 Ã %Â» ~ 
where 5 ~ Â² Ã Ãƒ Ã  Â³ is nonzero and % ~ Â²% Ã Ãƒ Ã % Â³. Geometrically
speaking, this is the set of all vectors in s that are perpendicular (normal) to
the vector 5 .
An affine hyperplane, or just hyperplane, in s is a linear hyperplane that has
been translated by a vector. Thus, it is the solution set to an equation of the form
 Â²% c  Â³ b Ã„ b  Â²% c  Â³ ~ 
or equivalently,
Âº5 Ã %Â» ~ 
where  ~   b Ã„ b   . We denote this hyperplane by
>Â²5 Ã Â³ ~ Â¸% Â s Â“ Âº5 Ã %Â» ~ Â¹
Note that the hyperplane
>Â²5 Ã )5 ) Â³ ~ Â¸% Â s Â“ Âº5 Ã %Â» ~ )5 ) Â¹
contains the point 5 , which is the point of >Â²5 Ã )5 ) Â³ closest to the origin,
since Cauchy's inequality gives
)5 ) ~ Âº5 Ã %Â» Â )5 ))%)
and so )5 ) Â )%) for all % Â >Â²5 Ã )5 ) Â³. Moreover, we leave it as an

Positive Solutions to Linear Systems: Convexity and Separation

417

exercise to show that any hyperplane has the form >Â²5 Ã )5 ) Â³ for an
appropriate vector 5 .
A hyperplane defines two closed half-spaces
>b Â²5 Ã Â³ ~ Â¸% Â s Â“ Âº5 Ã %Â» Â‚ Â¹
>c Â²5 Ã Â³ ~ Â¸% Â s Â“ Âº5 Ã %Â» Â Â¹
and two disjoint open half-spaces
>kb Â²5 Ã Â³ ~ Â¸% Â s Â“ Âº5 Ã %Â» Â€ Â¹
>kc Â²5 Ã Â³ ~ Â¸% Â s Â“ Âº5 Ã %Â»  Â¹
It is clear that
>b Â²5 Ã Â³ q >c Â²5 Ã Â³ ~ >Â²5 Ã Â³
k
and that the sets >kb Â²5 Ã Â³, >c
Â²5 Ã Â³ and >Â²5 Ã Â³ form a partition of s .

If 5 Â s and ? Â‹ s , we let
Âº5 Ã ?Â» ~ Â¸Âº5 Ã %Â» Â“ % Â ?Â¹
and write
Âº5 Ã ?Â»  
to denote the fact that Âº5 Ã %Â»   for all % Â ? .
Definition Two subsets ? and @ of s are strictly separated by a hyperplane
>Â²5 Ã Â³ if ? lies in one open half-space determined by >Â²5 Ã Â³ and @ lies in
the other open half-space; in symbols, one of the following holds:
1) Âº5 Ã ?Â»    Âº5 Ã @ Â»
2) Âº5 Ã @ Â»    Âº5 Ã ?Â»
Note that 1) holds for 5 and  if and only if 2) holds for c5 and c , and so we
need only consider one of the conditions to demonstrate that two sets ? and @
are not strictly separated. Specifically, if 1) fails for all 5 and  , then the
condition
Âºc5 Ã @ Â»  c  Âºc5 Ã ?Â»
also fails for all 5 and  and so 2) also fails for all 5 and  , whence ? and @
are not strictly separated.
Definition Two subsets ? and @ of s are strongly separated by a hyperplane
>Â²5 Ã Â³ if there is an  Â€  for which one of the following holds:
1) Âº5 Ã ?Â»   c    b   Âº5 Ã @ Â»
2) Âº5 Ã @ Â»   c    b   Âº5 Ã ?Â»

418

Advanced Linear Algebra

As before, we need only consider one of the conditions to show that two sets are
not strongly separated. Note also that if
Âº5 Ã %Â»   Â Âº5 Ã @ Â»
for  Â s, then % Â s and @ Â‹ s are stongly separated by the hyperplane
>65 Ã

 b Âº5 Ã %Â»
7


Separation
Now that we have the preliminaries out of the way, we can get down to some
theorems. The first is a well-known separation theorem that is the basis for
many other separation theorems. It says that if a closed convex set * Â‹ s does
not contain a vector  , then * can be strongly separated from  .
Theorem 15.5 Let * be a closed convex subset of s .
1) * contains a unique vector 5 of minimum norm, that is, there is a unique
vector 5 Â * for which
) 5 )  )% )
for all % Â *Ã % Â£ 5 .
2) If  Â¤ * , then * lies in the closed half-space
>b 25 Ã )5 ) b Âº5 Ã Â»3
that is,
Âº5 Ã *Â» Â‚ )5 ) b Âº5 Ã Â» Â€ Âº5 Ã Â»
where 5 is the unique vector of minimum norm in the closed convex set
* c  ~ Â¸ c  Â“  Â *Â¹
Hence, * and  are strongly separated by the hyperplane
>85 Ã

)5 ) b Âº5 Ã Â»
9


Proof. For part 1), if  Â * then this is the unique vector of minimum norm, so
we may assume that  Â¤ * . It follows that no two distinct elements of * can be
negative scalar multiples of each other. For if % and % were in * , where   Ã
then taking ! ~ cÂ°Â² c Â³ gives
 ~ !% b Â² c !Â³% Â *
which is false.

Positive Solutions to Linear Systems: Convexity and Separation

419

We first show that * contains a vector 5 of minimum norm. Recall that the
Euclidean norm (distance) is a continuous function. Although * need not be
compact, if we choose a real number such that the closed ball
) Â²Â³ ~ Â¸' Â s Â“ )' ) Â Â¹
intersects * , then that intersection * Z ~ * q ) Â²Â³ is both closed and bounded
and so is compact. The norm function therefore achieves its minimum on * Z ,
say at the point 5 Â * Z Â‹ * . It is clear that if )#)  )5 ) for some # Â * , then
# Â * Z , in contradiction to the minimality of 5 . Hence, 5 is a vector of
minimum norm in * .
We establish uniqueness first for closed line segments Â´"Ã #Âµ in s . If " ~ #
where  Â€ , then
)!" b Â² c !Â³#) ~ (! b Â² c !Â³()#)
is smallest when ! ~  for  Â€  and ! ~  for   . Assume that " and # are
not scalar multiples of each other and suppose that % Â£ & in Â´"Ã #Âµ have
minimum norm  Â€ . If ' ~ Â²% b &Â³Â° then since % and & are also not scalar
multiples of each other, the Cauchy-Schwarz inequality is strict and so

)% b & ) 


~ Â²P%P b Âº%Ã &Â» b P&P Â³


 Â²  b )%))&)Â³

~ 

P'P ~

which contradicts the minimality of  . Thus, Â´"Ã #Âµ has a unique point of
minimum norm.
Finally, if % Â * also has minimum norm, then 5 and % are points of minimum
norm in the line segment Â´5 Ã %Âµ Â‹ * and so % ~ 5 . Hence, * has a unique
element of minimum norm.
For part 2), suppose the result is true when  Â¤ * . Then  Â¤ * implies that
 Â¤ * c  and so if 5 Â * c  has smallest norm, then
Âº5 Ã * c Â» Â‚ )5 ) Â€ 
Therefore,
Âº5 Ã *Â» Â‚ )5 ) b Âº5 Ã Â» Â€ Âº5 Ã Â»
and so * and  are strongly separated by the hyperplane

