438

Advanced Linear Algebra

planes Âº3 Â» and Âº3 Â» have the property that their intersection is a line through
the origin, even if the lines are parallel. We are now ready to define projective
geometries.
Let = be a vector space of any dimension and let / be a hyperplane in = not
containing the origin. To each flat ? in / , we associate the subspace Âº?Â» of =
generated by ? . Thus, the linear span function 7 Â¢ 7Â²/Â³ Â¦ I Â²= Â³ maps affine
subspaces ? of / to subspaces Âº?Â» of = . The span function is not surjective:
Its image is the set of all subspaces that are not contained in the base subspace
2 of the flat / .
The linear span function is one-to-one and its inverse is intersection with / ,
7 c < ~ < q /
for any subspace < not contained in 2 .
The affine geometry 7Â²/Â³ is, as we have remarked, somewhat incomplete. In
the case dimÂ²/Â³ ~ , every pair of points determines a line but not every pair
of lines determines a point.
Now, since the linear span function 7 is injective, we can identify 7Â²/Â³ with
its image 7 Â²7Â²/Â³Â³, which is the set of all subspaces of = not contained in the
base subspace 2 . This view of 7Â²/Â³ allows us to â€œcompleteâ€ 7Â²/Â³ by
including the base subspace 2 . In the three-dimensional case of Figure 16.1, the
base plane, in effect, adds a projective line at infinity. With this inclusion, every
pair of lines intersects, parallel lines intersecting at a point on the line at infinity.
This two-dimensional projective geometry is called the projective plane.
Definition Let = be a vector space. The set I Â²= Â³ of all subspaces of = is
called the projective geometry of = . The projective dimension pdimÂ²:Â³ of
: Â I Â²= Â³ is defined as
pdimÂ²:Â³ ~ dimÂ²:Â³ c 
The projective dimension of FÂ²= Â³ is defined to be pdimÂ²= Â³ ~ dimÂ²= Â³ c . A
subspace of projective dimension  is called a projective point and a subspace
of projective dimension  is called a projective line.
Thus, referring to Figure 16.1, a projective point is a line through the origin and,
provided that it is not contained in the base plane 2 , it meets / in an affine
point. Similarly, a projective line is a plane through the origin and, provided that
it is not 2 , it will meet / in an affine line. In short,
spanÂ²affine pointÂ³ ~ line through the origin ~ projective point
spanÂ²affine lineÂ³ ~ plane through the origin ~ projective line
The linear span function has the following properties.

Affine Geometry

439

Theorem 16.12 The linear span function 7 Â¢ 7Â²/Â³ Â¦ I Â²= Â³ from the affine
geometry 7Â²/Â³ to the projective geometry I Â²= Â³ defined by 7 ? ~ Âº?Â»
satisfies the following properties:
1) The linear span function is injective, with inverse given by
7 c < ~ < q /
for all subspaces < not contained in the base subspace 2 of / .
2) The image of the span function is the set of all subspaces of = that are not
contained in the base subspace 2 of / .
3) ? Â‹ @ if and only if Âº?Â» Â‹ Âº@ Â»
4) If ? are flats in / with nonempty intersection, then
span4

spanÂ²? Â³

? 5 ~

Â2

Â2

5) For any collection of flats in / ,
span8  ? 9 ~  spanÂ²? Â³
Â2

Â2

6) The linear span function preserves dimension, in the sense that
pdimÂ²spanÂ²?Â³Â³ ~ dimÂ²?Â³
7) ? Â” @ if and only if one of Âº?Â» q 2 and Âº@ Â» q 2 is contained in the
other.
Proof. To prove part 1), let % b : be a flat in / . Then % Â / and so
/ ~ % b 2 , which implies that : Â‹ 2 . Note also that Âº% b :Â» ~ Âº%Â» b : and
' Â Âº% b :Â» q / ~ Â²Âº%Â» b :Â³ q Â²% b 2Â³ Â¬ ' ~ % b ~ % b 
for some Â : ,  Â 2 and  Â - . This implies that Â² c Â³% Â 2 , which
implies that either % Â 2 or  ~ . But % Â / implies % Â¤ 2 and so  ~ ,
which implies that ' ~ % b Â % b : . In other words,
Âº% b :Â» q / Â‹ % b :
Since the reverse inclusion is clear, we have
Âº% b :Â» q / ~ % b :
This establishes 1).
To prove 2), let < be a subspace of = that is not contained in 2 . We wish to
show that < is in the image of the linear span function. Note first that since
< Â‹
Â“ 2 and dimÂ²2Â³ ~ dimÂ²= Â³ c , we have < b 2 ~ = and so
dimÂ²< q 2Â³ ~ dimÂ²< Â³ b dimÂ²2Â³ c dimÂ²< b 2Â³ ~ dimÂ²< Â³ c 

440

Advanced Linear Algebra

Now let  Â£ % Â < c 2 . Then
% Â¤ 2 Â¬ Âº%Â» b 2 ~ =
Â¬ % b  Â / for some  Â£  Â - Ã  Â 2
Â¬ % Â /
Thus, % Â < q / for some  Â£  Â - . Hence, the flat % b Â²< q 2Â³ lies in /
and
dimÂ²% b Â²< q 2Â³Â³ ~ dimÂ²< q 2Â³ ~ dimÂ²< Â³ c 
which implies that spanÂ²% b Â²< q 2Â³Â³ ~ Âº%Â» b Â²< q 2Â³ lies in < and has
the same dimension as < . In other words,
spanÂ²% b Â²< q 2Â³Â³ ~ Âº%Â» b Â²< q 2Â³ ~ <
We leave proof of the remaining parts of the theorem as exercises.

Exercises
1.
2.

Show that if % Ã Ãƒ Ã % Â = , then the set : ~ Â¸' % Â“ ' ~ Â¹ is a
subspace of = .
Prove that if ? Â‹ = is nonempty then
affhullÂ²?Â³ ~ % b Âº? c %Â»

Prove that the set ? ~ Â¸Â²Ã Â³Ã Â²Ã Â³Ã Â²Ã Â³Â¹ in Â²{ Â³ is closed under the
formation of lines, but not affine hulls.
4. Prove that a flat contains the origin  if and only if it is a subspace.
5. Prove that a flat ? is a subspace if and only if for some % Â ? we have
% Â ? for some  Â£  Â - .
6. Show that the join of a collection 9 ~ Â¸% b : Â“  Â 2Â¹ of flats in = is the
intersection of all flats that contain all flats in 9 .
7. Is the collection of all flats in = a lattice under set inclusion? If not, how
can you â€œfixâ€ this?
8. Suppose that ? ~ % b : and @ ~ & b ; . Prove that if dimÂ²?Â³ ~ dimÂ²@ Â³
and ? Â” @ , then : ~ ; .
9. Suppose that ? ~ % b : and @ ~ & b ; are disjoint hyperplanes in = .
Show that : ~ ; .
10. (The parallel postulate) Let ? be a flat in = and # Â¤ ? . Show that there is
exactly one flat containing #, parallel to ? and having the same dimension
as ? .
11. a) Find an example to show that the join ? v @ of two flats may not be
the set of all lines connecting all points in the union of these flats.
b) Show that if ? and @ are flats with ? q @ Â£ J, then ? v @ is the
union of all lines %& where % Â ? and & Â @ .
12. Show that if ? Â” @ and ? q @ ~ J, then
3.

dimÂ²? v @ Â³ ~ maxÂ¸dimÂ²?Â³Ã dimÂ²@ Â³Â¹ b 

Affine Geometry

441

13. Let dimÂ²= Â³ ~ . Prove the following:
a) The join of any two distinct points is a line.
b) The intersection of any two nonparallel lines is a point.
14. Let dimÂ²= Â³ ~ . Prove the following:
a) The join of any two distinct points is a line.
b) The intersection of any two nonparallel planes is a line.
c) The join of any two lines whose intersection is a point is a plane.
d) The intersection of two coplanar nonparallel lines is a point.
e) The join of any two distinct parallel lines is a plane.
f) The join of a line and a point not on that line is a plane.
g) The intersection of a plane and a line not on that plane is a point.
15. Prove that  Â¢ = Â¦ = is a surjective affine transformation if and only if
 ~  k ;$ for some $ Â = and  Â BÂ²= Â³.
16. Verify the group-theoretic remarks about the group homomorphism
Â¢ affÂ²= Â³ Â¦ BÂ²= Â³ and the subgroup transÂ²= Â³ of affÂ²= Â³.

Chapter 17

Singular Values and the Mooreâ€“Penrose
Inverse

Singular Values
Let < and = be finite-dimensional inner product spaces over d or s and let
 Â BÂ²< Ã = Â³. The spectral theorem applied to  i  can be of considerable help
in understanding the relationship between  and its adjoint  i . This relationship
is shown in Figure 17.1. Note that < and = can be decomposed into direct sums
< ~(l)

and = ~ * l +

in such a manner that  Â¢ ( Â¦ * and  i Â¢ * Â¦ ( act symmetrically in the sense
that
 Â¢ " Âª

 #

and

 i Â¢ # Âª

 "

Also, both  and  i are zero on ) and +, respectively.
We begin by noting that  i  Â BÂ²< Â³ is a positive Hermitian operator. Hence, if
 ~ rkÂ² Â³ ~ rkÂ² i  Â³, then < has an ordered orthonormal basis
8 ~ Â²" Ã Ãƒ Ã " Ã "b Ã Ãƒ Ã " Â³
of eigenvectors for  i  , where the corresponding eigenvalues can be arranged
so that
 Â‚ Ã„ Â‚  Â€  ~ b ~ Ã„ ~ 
The set Â²"b Ã Ãƒ Ã " Â³ is an ordered orthonormal basis for kerÂ² i  Â³ ~ kerÂ² Â³
and so Â²" Ã Ãƒ Ã " Â³ is an ordered orthonormal basis for kerÂ² Â³Âž ~ imÂ² i Â³.

444

Advanced Linear Algebra

im(W)

im(W*)
u1
ONB of
eigenvectors
for W*W

v1

W(uk)=skvk

ur

W (vk)=skuk

vr

ur+1

W 

vr+1

un

W 

vm

ONB of
eigenvectors
for WW*

ker(W*)

ker(W)
Figure 17.1

 ~ j are called the singular values

For  ~ Ã Ãƒ Ã , the positive numbers
of  . If we set  ~  for  Â€ , then

 i  " ~


 "

for  ~ Ã Ãƒ Ã . We can achieve some â€œsymmetryâ€ here between  and  i by
setting # ~ Â²Â°  Â³ " for each  Â , giving
#
" ~ F  


Â
Â€

"
 i # ~ F  


Â
Â€

and

The vectors # Ã Ãƒ Ã # are orthonormal, since if Ã  Â , then
Âº# Ã # Â» ~


 

Âº "  Ã  "  Â» ~


 

Âº i  " Ã " Â» ~




Âº" Ã " Â» ~ Ã

Hence, Â²# Ã Ãƒ Ã # Â³ is an orthonormal basis for imÂ² Â³ ~ kerÂ² i Â³Âž , which can be
extended to an orthonormal basis 9 ~ Â²# Ã Ãƒ Ã # Â³ for = , the extension
Â²#b Ã Ãƒ Ã # Â³ being an orthonormal basis for kerÂ² i Â³. Moreover, since
 i # ~

  " ~


 #

the vectors # Ã Ãƒ Ã # are eigenvectors for  i with the same eigenvalues
 ~  as for  i  . This completes the picture in Figure 17.1.

Singular Values and the Mooreâ€“Penrose Inverse

445

Theorem 17.1 Let < and = be finite-dimensional inner product spaces over d
or s and let  Â BÂ²< Ã = Â³ have rank . Then there are ordered orthonormal
bases 8 and 9 for < and = , respectively, for which
8 ~ Â² " Ã Ãƒ
Ã " Ã "b Ã
Ãƒ Ã " Â³
Â’Â•Â“Â•Â”
Â’Â•Â•Â•Â“Â•Â•Â•Â”
ONB for imÂ² i Â³

ONB for kerÂ² Â³

and
9 ~ Â² # Ã Ãƒ
Ã # Ã #b Ã
Ãƒ Ã # Â³
Â’Â•Â“Â•Â”
Â’Â•Â•Â•Â“Â•Â•Â•Â”
ONB for imÂ² Â³

ONB for kerÂ² i Â³

Moreover, for  Â  Â ,
 " ~
 i # ~
where

 #
 "

 Â€  are called the singular values of  , defined by

 i  " ~


 " Ã  Â€ 

for  Â . The vectors " Ã Ãƒ Ã " are called the right singular vectors for  and
the vectors # Ã Ãƒ Ã # are called the left singular vectors for  .
The matrix version of the previous discussion leads to the well-known singularvalue decomposition of a matrix. Let ( Â CÃ Â²- Â³ and let 8 ~ Â²" Ã Ãƒ Ã " Â³
and 9 ~ Â²# Ã Ãƒ Ã # Â³ be the orthonormal bases from < and = , respectively, in
Theorem 17.1, for the operator ( . Then
Â´ Âµ8Ã9 ~ ' ~ diagÂ²  Ã  Ã Ãƒ Ã  Ã Ã Ãƒ Ã Â³
A change of orthonormal bases from the standard bases to 9 and : gives
( ~ Â´( Âµ; Ã; ~ 49Ã; Â´( Âµ8Ã9 4; Ã8 ~ 7 '8i
where 7 ~ 49Ã; and 8 ~ 48Ã; are unitary/orthogonal. This is the singularvalue decomposition of (.
As to uniqueness, if ( ~ 7 '8i , where 7 and 8 are unitary and ' is diagonal,
with diagonal entries  , then
(i ( ~ Â²7 '8i Â³i 7 '8i ~ 8'i '8i
and since 'i ' ~ diagÂ² Ã Ãƒ Ã  Â³, it follows that the  's are eigenvalues of
(i (, that is, they are the squares of the singular values along with a sufficient
number of 's. Hence, ' is uniquely determined by (, up to the order of the
diagonal elements.

446

Advanced Linear Algebra

We state without proof the following uniqueness facts and refer the reader to
[48] for details. If  Â  and if the eigenvalues  are distinct, then 7 is
uniquely determined up to multiplication on the right by a diagonal matrix of the
form + ~ diagÂ²' Ã Ãƒ Ã ' Â³ with (' ( ~ . If   , then 8 is never uniquely
determined. If  ~  ~ , then for any given 7 there is a unique 8. Thus, we
see that, in general, the singular-value decomposition is not unique.

The Mooreâ€“Penrose Generalized Inverse
Singular values lead to a generalization of the inverse of an operator that applies
to all linear transformations. The setup is the same as in Figure 17.1. Referring
to that figure, we are prompted to define a linear transformation  b Â¢ = Â¦ < by


 b # ~ F 


"

for  Â 
for  Â€ 

since then
Â² b  Â³OÂº" ÃÃƒÃ" Â» ~ 
Â² b  Â³OÂº"b ÃÃƒÃ" Â» ~ 
and
Â² b Â³OÂº# ÃÃƒÃ# Â» ~ 
Â² b Â³OÂº#b ÃÃƒÃ# Â» ~ 
Hence, if  ~  ~ , then  b ~  c . The transformation  b is called the
Mooreâ€“Penrose generalized inverse or Mooreâ€“Penrose pseudoinverse of  .
We abbreviate this as MP inverse.
Note that the composition  b  is the identity on the largest possible subspace of
< on which any composition of the form  could be the identity, namely, the
orthogonal complement of the kernel of  . A similar statement holds for the
composition  b . Hence,  b is as â€œcloseâ€ to an inverse for  as is possible.
We have said that if  is invertible, then  b ~  c . More is true: If  is
injective, then  b  ~  and so  b is a left inverse for  . Also, if  is surjective,
then  b is a right inverse for  . Hence the MP inverse  b generalizes the onesided inverses as well.
Here is a characterization of the MP inverse.
Theorem 17.2 Let  Â BÂ²< Ã = Â³. The MP inverse  b of  is completely
characterized by the following four properties:
1)  b  ~ 
2)  b  b ~  b
3)  b is Hermitian
4)  b  is Hermitian

Singular Values and the Mooreâ€“Penrose Inverse

447

Proof. We leave it to the reader to show that  b does indeed satisfy conditions
1)â€“4) and prove only the uniqueness. Suppose that  and  satisfy 1)â€“4) when
substituted for  b . Then
 ~  
~ Â²  Â³ i 
~  i i 
~ Â²   Â³ i  i 
~  i  i  i i 
~ Â²  Â³ i  i  i 
~  i i 
~    
~  
and
 ~  
~  Â²  Â³ i
~ i  i
~ i Â²  Â³i
~ i  i i  i
~ i  i Â² Â³i
~ i  i  
~   
~  
which shows that  ~ .
The MP inverse can also be defined for matrices. In particular, if ( Â 4Ã Â²- Â³,
then the matrix operator ( has an MP inverse (b . Since this is a linear
transformation from -  to -  , it is just multiplication by a matrix (b ~ ) .
This matrix ) is the MP inverse for ( and is denoted by (b .
Since (b ~ (b and () ~ ( ) , the matrix version of Theorem 17.2 implies
that (b is completely characterized by the four conditions
1)
2)
3)
4)

((b ( ~ (
(b ((b ~ (b
((b is Hermitian
(b ( is Hermitian

Moreover, if
( ~ < '<i
is the singular-value decomposition of (, then

448

Advanced Linear Algebra

(b ~ < 'Z <i
where 'Z is obtained from ' by replacing all nonzero entries by their
multiplicative inverses. This follows from the characterization above and also
from the fact that for  Â ,
< 'Z <i # ~ < 'Z  ~

c
 <  ~

c
 "

and for  Â€ ,
< 'Z <i # ~ < 'Z  ~ 

Least Squares Approximation
Let us now discuss the most important use of the MP inverse. Consider the
system of linear equations
(% ~ #
where ( Â 4Ã Â²- Â³. (As usual, - ~ d or - ~ s.) This system has a solution
if and only if # Â imÂ²( Â³. If the system has no solution, then it is of considerable
practical importance to be able to solve the system
(% ~ V#
where V# is the unique vector in imÂ²( Â³ that is closest to #, as measured by the
unitary (or Euclidean) distance. This problem is called the linear least squares
problem. Any solution to the system (% ~ V# is called a least squares solution
to the system (% ~ #. Put another way, a least squares solution to (% ~ # is a
vector % for which )(% c #) is minimized.
Suppose that $ and ' are least squares solutions to (% ~ #. Then
($ ~ V# ~ ('
and so $ c ' Â kerÂ²(Â³. (We will write ( for ( .) Thus, if $ is a particular least
squares solution, then the set of all least squares solutions is $ b kerÂ²(Â³.
Among all solutions, the most interesting is the solution of minimum norm.
Note that if there is a least squares solution $ that lies in kerÂ²(Â³Âž , then for any
' Â kerÂ²(Â³, we have
)$ b ' ) ~ )$) b )' ) Â‚ )$)
and so $ will be the unique least squares solution of minimum norm.
Before proceeding, we recall (Theorem 9.14) that if : is a subspace of a finitedimensional inner product space = , then the best approximation to a vector
# Â = from within : is the unique vector V# Â : for which # c V# Âž : . Now we
can see how the MP inverse comes into play.

Singular Values and the Mooreâ€“Penrose Inverse

449

Theorem 17.3 Let ( Â 4Ã Â²- Â³. Among the least squares solutions to the
system
(% ~ V#
there is a unique solution of minimum norm, given by (b #, where (b is the MP
inverse of (.
Proof. A vector $ is a least squares solution if and only if ($ ~ V#. Using the
characterization of the best approximation V#, we see that $ is a solution to
($ ~ V# if and only if
($ c # Âž imÂ²(Â³
Since imÂ²(Â³Âž ~ kerÂ²(i Â³ this is equivalent to
(i Â²($ c #Â³ ~ 
or
(i ($ ~ (i #
This system of equations is called the normal equations for (% ~ #. Its
solutions are precisely the least squares solutions to the system (% ~ #.
To see that $ ~ (b # is a least squares solution, recall that, in the notation of
Figure 17.1,
#
((b # ~ F 


Â
Â€

and so
(i (Â²(b # Â³ ~ F

(i #


Â
~ ( i #
Â€

and since 9 ~ Â²# Ã Ãƒ Ã # Â³ is a basis for = , we conclude that (b # satisfies the
normal equations. Finally, since (b # Â kerÂ²(Â³Âž , we deduce by the preceding
remarks that (b # is the unique least squares solution of minimum norm.

Exercises
1.
2.

Let  Â BÂ²< Â³. Show that the singular values of  i are the same as those of
.
Find the singular values and the singular value decomposition of the matrix
(~>

3.




?

Find (b .
Find the singular values and the singular value decomposition of the matrix

450

Advanced Linear Algebra

(~>

4.
5.








?

Find (b . Hint: Is it better to work with (i ( or ((i ?
Let ? ~ Â²% % Ã„ % Â³! be a column matrix over d. Find a singular-value
decomposition of ? .
Let ( Â 4Ã Â²- Â³ and let ) Â 4bÃb Â²- Â³ be the square matrix
)~>


(i

(
 ?block

Show that, counting multiplicity, the nonzero eigenvalues of ) are
precisely the singular values of ( together with their negatives. Hint: Let
( ~ < '<i be a singular-value decomposition of ( and try factoring )
into a product < :< i where < is unitary. Do not read the following second
hint unless you get stuck. Second Hint: Verify the block factorization
)~>

6.

7.

8.


<

<

 ?> '


'i
 ?> <i

<i
 ?

What are the eigenvalues of the middle factor on the right? (Try  b b
and  c b .)
Use the results of the previous exercise to show that a matrix
( Â 4Ã Â²- Â³, its adjoint (i , its transpose (! and its conjugate ( all have
the same singular values. Show also that if < and < Z are unitary, then (
and < (< Z have the same singular values.
Let ( Â 4 Â²- Â³ be nonsingular. Show that the following procedure
produces a singular-value decomposition ( ~ < '<i of (.
a) Write ( ~ < +< i where + ~ diagÂ² Ã Ãƒ Ã  Â³ and the  's are
positive and the columns of < form an orthonormal basis of
eigenvectors for (. (We never said that this was a practical procedure.)
Â°
Â°
b) Let ' ~ diagÂ² Ã Ãƒ Ã  Â³ where the square roots are nonnegative.
Also let < ~ < and U ~ (i < 'c .
If ( ~ Â²Ã Â³ is an  d  matrix, then the Frobenius norm of ( is
Â°

)()- ~ 8 Ã
9
Ã

Show that )()- ~   is the sum of the squares of the singular values of
(.

Chapter 18

An Introduction to Algebras

Motivation
We have spent considerable time studying the structure of a linear operator
 Â B- Â²= Â³ on a finite-dimensional vector space = over a field - . In our
studies, we defined the - Â´%Âµ-module = and used the decomposition theorems
for modules over a principal ideal domain to dissect this module. We
concentrated on an individual operator  , rather than the entire vector space
B- Â²= Â³. In fact, we have made relatively little use of the fact that B- Â²= Â³ is an
algebra under composition. In this chapter, we give a brief introduction to the
theory of algebras, of which B- Â²= Â³ is the most general, in the sense of Theorem
18.2 below.

Associative Algebras
An algebra is a combination of a ring and a vector space, with an axiom that
links the ring product with scalar multiplication.
Definition An (associative) algebra ( over a field - , or an - -algebra, is a
nonempty set (, together with three operations, called addition (denoted by
b ), multiplication (denoted by juxtaposition) and scalar multiplication (also
denoted by juxtaposition), for which the following properties hold:
1) ( is a vector space over - under addition and scalar multiplication.
2) ( is a ring with identity under addition and multiplication.
3) If  Â - and Ã  Â (, then
Â²Â³ ~ Â²Â³ ~ Â²Â³
An algebra is finite-dimensional if it is finite-dimensional as a vector space. An
algebra is commutative if ( is a commutative ring. An element  Â ( is
invertible if there is  Â ( for which  ~  ~ .
Our definition requires that ( have a multiplicative identity. Such algebras are
called unital algebras. Algebras without unit are also of great importance, but

452

Advanced Linear Algebra

we will not study them here. Also, in this chapter, we will assume that all
algebras are associative. Nonassociative algebras, such as Lie algebras and
Jordan algebras, are important as well.

The Center of an Algebra
Definition The center of an - -algebra ( is the set
AÂ²(Â³ ~ Â¸ Â ( Â“ % ~ % for all % Â (Â¹
of all elements of ( that commute with every element of (.
The center of an algebra is never trivial since it contains a copy of - :
Â¸ Â“  Â - Â¹ Â‹ AÂ²(Â³
Definition An - -algebra ( is central if its center is as small as possible, that
is, if
AÂ²(Â³ ~ Â¸ Â“  Â - Â¹

From a Vector Space to an Algebra
If = is a vector space over a field - and if 8 ~ Â¸ Â“  Â 0Â¹ is a basis for = ,
then it is natural to wonder whether we can form an - -algebra simply by
defining a product for the basis elements and then using the distributive laws to
extend the product to = . In particular, we choose a set of constants Ã with the
property that for each pair Â²Ã Â³, only finitely many of the Ã are nonzero. Then
we set
  ~  Ã 


and make multiplication bilinear, that is,




8   9 ~    
~

~





 8   9 ~    
~

~





and
8   9 ~   
~

~

for  Â - . It is easy to see that this does define a nonunital associative algebra
( provided that

An Introduction to Algebras

453

Â²  Â³ ~  Â²  Â³
for all Ã Ã  Â 0 and that ( is commutative if and only if
   ~    
for all Ã  Â 0 . The constants Ã are called the structure constants for the
algebra (. To get a unital algebra, we can take for a given  Â 0 , the structure
constants to be
Ã ~ Ã ~ Ã
in which case  is the multiplicative identity. (An alternative is to adjoin a new
element to the basis and define its structure constants in this way.)

Examples
The following examples will make it clear why algebras are important.
Example 18.1 If - Â , are fields, then , is a vector space over - . This vector
space structure, along with the ring structure of , , is an algebra over - .
Example 18.2 The ring - Â´%Âµ of polynomials is an algebra over - .
Example 18.3 The ring C Â²- Â³ of all  d  matrices over a field - is an
algebra over - , where scalar multiplication is defined by
4 ~ Â²Ã Â³Ã

Â-

Â¬

4 ~ Â²Ã Â³

Example 18.4 The set B- Â²= Â³ of all linear operators on a vector space = over a
field - is an - -algebra, where addition is addition of functions, multiplication is
composition of functions and scalar multiplication is given by
Â²Â³Â²#Â³ ~ Â´#Âµ
The identity map  Â B- Â²= Â³ is the multiplicative identity and the zero map
 Â B- Â²= Â³ is the additive identity. This algebra is also denoted by End- Â²= Â³,
since the linear operators on = are also called endomorphisms of = .
Example 18.5 If . is a group and - is a field, then we can form a vector space
- Â´.Âµ over - by taking all formal - -linear combinations of elements of . and
treating . as a basis for - Â´.Âµ. This vector space can be made into an - -algebra
where the structure constants are determined by the group product, that is, if
  ~ " , then Ã ~ Ã" . The group identity  ~  is the algebra identity
since   ~  and so Ã ~ Ã and similarly, Ã ~ Ã .
The resulting associative algebra - Â´.Âµ is called the group algebra over - .
Specifically, the elements of - Â´.Âµ have the form

