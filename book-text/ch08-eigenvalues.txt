190

Advanced Linear Algebra

The Jordan Canonical Form
One of the virtues of the rational canonical form is that every linear operator on
a finite-dimensional vector space has a rational canonical form. However, as
mentioned earlier, the rational canonical form may be far from the ideal of
simplicity that we had in mind for a set of simple canonical forms and is really
more of a theoretical tool than a practical tool.
When the minimal polynomial  Â²%Â³ of  splits over - ,
 Â²%Â³ ~ Â²% c  Â³ Ã„Â²% c  Â³
there is another set of canoncial forms that is arguably simpler than the set of
rational canonical forms.
In some sense, the complexity of the rational canonical form comes from the
choice of basis for the cyclic submodules ÂºÂº#Ã Â»Â». Recall that the  -cyclic bases
have the form
8Ã ~ 2#Ã Ã  #Ã Ã Ãƒ Ã  Ã c #Ã 3


where Ã ~ degÂ² Ã Â³. With this basis, all of the complexity comes at the end,
so to speak, when we attempt to express
 Â² Ã c Â²#Ã Â³Â³ ~  Ã Â²#Ã Â³
as a linear combination of the basis vectors.
However, since 8Ã has the form
2#Ã  #Ã   #Ã Ãƒ Ã  c #3
any ordered set of the form
 Â² Â³#Ã  Â² Â³#Ã Ãƒ Ã c Â² Â³#
where degÂ² Â²%Â³Â³ ~  will also be a basis for ÂºÂº#Ã Â»Â». In particular, when  Â²%Â³
splits over - , the elementary divisors are


 Ã Â²%Â³ ~ Â²% c  Â³Ã
and so the set
9Ã ~ 2#Ã Ã Â² c  Â³#Ã Ã Ãƒ Ã Â² c  Â³Ã c #Ã 3
is also a basis for ÂºÂº#Ã Â»Â».
If we temporarily denote the  th basis vector in 9Ã by  , then for
 ~ Ã Ãƒ Ã Ã c ,

Eigenvalues and Eigenvectors

191

  ~  Â´Â² c  Â³ Â²#Ã Â³Âµ
~ Â² c  b  Â³Â´Â² c  Â³ Â²#Ã Â³Âµ
~ Â² c  Â³b Â²#Ã Â³ b  Â² c  Â³ Â²#Ã Â³
~ b b  
For  ~ Ã c , a similar computation, using the fact that
Â² c  Â³b Â²#Ã Â³ ~ Â² c  Â³Ã Â²#Ã Â³ ~ 
gives
 Â²Ã c Â³ ~  Ã c
Thus, for this basis, the complexity is more or less spread out evenly, and the
matrix of  OÂºÂº#Ã Â»Â» with respect to 9Ã is the Ã d Ã matrix
v 
x
x
@ Â² Ã Ã Â³ ~ x 
x
Ã…
w




Ã†
Ã„

Ã„ Ã„ y
Ã†
Ã… {
{
Ã† Ã† Ã… {
{
Ã† Ã† 

  z

which is called a Jordan block associated with the scalar  . Note that a Jordan
block has  's on the main diagonal, 's on the subdiagonal and 's elsewhere.
Let us refer to the basis
9 ~ 9Ã
as a Jordan basis for  .
Theorem 8.6 (The Jordan canonical form) Suppose that the minimal
polynomial of  Â BÂ²= Â³ splits over the base field - , that is,
 Â²%Â³ ~ Â²% c  Â³ Ã„Â²% c  Â³
where  Â - .
1) The matrix of  with respect to a Jordan basis 9 is
diag@ Â² Ã Ã Â³Ã Ãƒ Ã @ Â² Ã Ã Â³Ã Ãƒ Ã @ Â² Ã Ã Â³Ã Ãƒ Ã @ Â² Ã Ã Â³
where the polynomials Â²% c  Â³Ã are the elementary divisors of  . This
block diagonal matrix is said to be in Jordan canonical form and is called
the Jordan canonical form of  .
2) If - is algebraically closed, then up to order of the block diagonal
matrices, the set of matrices in Jordan canonical form constitutes a set of
canonical forms for similarity.
Proof. For part 2), the companion matrix and corresponding Jordan block are
similar:

192

Advanced Linear Algebra

*Â´Â²% c  Â³Ã Âµ Â— @ Â² Ã Ã Â³
since they both represent the same operator  on the subspace ÂºÂº#Ã Â»Â». It follows
that the rational canonical matrix and the Jordan canonical matrix for  are
similar.
Note that the diagonal elements of the Jordan canonical form @ of  are
precisely the eigenvalues of  , each appearing a number of times equal to its
algebraic multiplicity. In general, the rational canonical form does not â€œexposeâ€
the eigenvalues of the matrix, even when these eigenvalues lie in the base field.

Triangularizability and Schur's Lemma
We have discussed two different canonical forms for similarity: the rational
canonical form, which applies in all cases and the Jordan canonical form, which
applies only when the base field is algebraically closed. Moreover, there is an
annoying sense in which these sets of canoncial forms leave something to be
desired: One is too complex and the other does not always exist.
Let us now drop the rather strict requirements of canonical forms and look at
two classes of matrices that are too large to be canonical forms (the upper
triangular matrices and the almost upper triangular matrices) and one class of
matrices that is too small to be a canonical form (the diagonal matrices).
The upper triangular matrices (or lower triangular matrices) have some nice
algebraic properties and it is of interest to know when an arbitrary matrix is
similar to a triangular matrix. We confine our attention to upper triangular
matrices, since there are direct analogs for lower triangular matrices as well.
Definition A linear operator  Â BÂ²= Â³ is upper triangularizable if there is an
ordered basis 8 ~ Â²# Ã Ãƒ Ã # Â³ of = for which the matrix Â´ Âµ8 is upper
triangular, or equivalently, if
# Â Âº# Ã Ãƒ Ã # Â»
for all  ~ Ã Ãƒ Ã .
As we will see next, when the base field is algebraically closed, all operators are
upper triangularizable. However, since two distinct upper triangular matrices
can be similar, the class of upper triangular matrices is not a canonical form for
similarity. Simply put, there are just too many upper triangular matrices.
Theorem 8.7 (Schur's theorem) Let = be a finite-dimensional vector space
over a field - .
1) If the characteristic polynomial (or minimal polynomial) of  Â BÂ²= Â³ splits
over - , then  is upper triangularizable.
2) If - is algebraically closed, then all operators are upper triangularizable.

Eigenvalues and Eigenvectors

193

Proof. Part 2) follows from part 1). The proof of part 1) is most easily
accomplished by matrix means, namely, we prove that every square matrix
( Â 4 Â²- Â³ whose characteristic polynomial splits over - is similar to an upper
triangular matrix. If  ~  there is nothing to prove, since all  d  matrices are
upper triangular. Assume the result is true for  c  and let ( Â 4 Â²- Â³.
Let # be an eigenvector associated with the eigenvalue  Â - of ( and extend
Â¸# Â¹ to an ordered basis 8 ~ Â²# Ã Ãƒ Ã # Â³ for s . The matrix of ( with respect
to 8 has the form
Â´( Âµ8 ~ >




i
( ?block

for some ( Â 4c Â²- Â³. Since Â´( Âµ8 and ( are similar, we have
det Â²%0 c (Â³ ~ det Â²%0 c Â´( Âµ8 Â³ ~ Â²% c  Â³ det Â²%0 c ( Â³
Hence, the characteristic polynomial of ( also splits over - and the induction
hypothesis implies that there is an invertible matrix 7 Â 4c Â²- Â³ for which
< ~ 7 ( 7 c
is upper triangular. Hence, if
8~>





7 ?block

then 8 is invertible and
8Â´(Âµ8 8c ~ >






7 ?> 


i
( ?> 



~
7 c ? > 

i
<?

is upper triangular.

The Real Case
When the base field is - ~ s, an operator  is upper triangularizable if and
only if its characteristic polynomial splits over s. (Why?) We can, however,
always achieve a form that is close to triangular by permitting values on the first
subdiagonal.
Before proceeding, let us recall Theorem 7.11, which says that for a module >
of prime order  Â²%Â³, the following are equivalent:
1)
2)
3)
4)
5)

> is cyclic
> is indecomposable
 Â²%Â³ is irreducible
 is nonderogatory, that is,  Â²%Â³ ~  Â²%Â³
dimÂ²> Â³ ~ degÂ²Â²%Â³Â³.

194

Advanced Linear Algebra

Now suppose that - ~ s and  Â²%Â³ ~ % b % b ! is an irreducible quadratic.
If 8 is a  -cyclic basis for > , then
Â´ Âµ8 ~ >




c!
c ?

However, there is a more appealing matrix representation of  . To this end, let
( be the matrix above. As a complex matrix, ( has two distinct eigenvalues:
 ~c f


j! c 


Now, a matrix of the form
)~>




c
 ?

has characteristic polynomial Â²%Â³ ~ Â²% c Â³ b   and eigenvalues  f  . So
if we set
~c

and  ~ c



j! c 


then ) has the same two distinct eigenvalues as ( and so ( and ) have the
same Jordan canonical form over d. It follows that ( and ) are similar over d
and therefore also over s, by Theorem 7.20. Thus, there is an ordered basis 9
for which Â´ Âµ9 ~ ) .
Theorem 8.8 If - ~ s and > is cyclic and degÂ² Â²%Â³Â³ ~ , then there is an
ordered basis 9 for which
Â´ Âµ9 ~ >




c
 ?

Now we can proceed with the real version of Schur's theorem. For the sake of
the exposition, we make the following definition.
Definition A matrix ( Â 4 Â²- Â³ is almost upper triangular if it has the form

where

v (
x
(~x

(

i

w



Ã†

y
{
{
( zblock

Eigenvalues and Eigenvectors

( ~ Â´Âµ or ( ~ >




195

c
 ?

for Ã  Â - . A linear operator  Â BÂ²= Â³ is almost upper triangularizable if
there is an ordered basis 8 for which Â´ Âµ8 is almost upper triangular.
To see that every real linear operator is almost upper triangularizable, we use
Theorem 7.19, which states that if Â²%Â³ is a prime factor of  Â²%Â³, then = has a
cyclic submodule > of order Â²%Â³. Hence, > is a  -cyclic subspace of
dimension degÂ²Â²%Â³Â³ and  O> has characteristic polynomial Â²%Â³.
Now, the minimal polynomial of a real operator  Â BÂ²= Â³ factors into a product
of linear and irreducible quadratic factors. If  Â²%Â³ has a linear factor over - ,
then = has a one-dimensional  -invariant subspace > . If  Â²%Â³ has an
irreducible quadratic factor Â²%Â³, then = has a cyclic submodule > of order
Â²%Â³ and so a matrix representation of  on > is given by the matrix
(~>




c
 ?

This is the basis for an inductive proof, as in the complex case.
Theorem 8.9 (Schur's theorem: real case) If = is a real vector space, then
every linear operator on = is almost upper triangularizable.
Proof. As with the complex case, it is simpler to proceed using matrices, by
showing that any  d  real matrix ( is similar to an almost upper triangular
matrix. The result is clear if  ~ . Assume for the purposes of induction that
any square matrix of size less than  d  is almost upper triangularizable.
We have just seen that -  has a one-dimensional ( -invariant subspace > or a
two-dimensional ( -cyclic subspace > , where ( has irreducible characteristic
polynomial on > . Hence, we may choose a basis 8 for -  for which the first
one or first two vectors are a basis for > . Then
Â´( Âµ8 ~ >

(


i
( ?block

where
( ~ Â´Âµ

or ( ~ >




c
 ?

and ( has size  d  . The induction hypothesis applied to ( gives an
invertible matrix 7 Â 4 for which
< ~ 7 ( 7 c

196

Advanced Linear Algebra

is almost upper triangular. Hence, if
0
8 ~ > c



7 ?block

then 8 is invertible and
0
8Â´(Âµ8 8c ~ > c



(
7 ?> 

0c
i
( ?> 


(
~
7 c ? > 

i
<?

is almost upper triangular.

Unitary Triangularizability
Although we have not yet discussed inner product spaces and orthonormal
bases, the reader may very well be familiar with these concepts. For those who
are, we mention that when = is a real or complex inner product space, then if an
operator  on = can be triangularized (or almost triangularized) using an
ordered basis 8 , it can also be triangularized (or almost triangularized) using an
orthonormal ordered basis E .
To see this, suppose we apply the Gramâ€“Schmidt orthogonalization process to a
basis 8 ~ Â²# Ã Ãƒ Ã # Â³ that triangularizes (or almost triangularizes)  . The
resulting ordered orthonormal basis E ~ Â²" Ã Ãƒ Ã " Â³ has the property that
Âº# Ã Ãƒ Ã # Â» ~ Âº" Ã Ãƒ Ã " Â»
for all  Â . Since Â´ Âµ8 is (almost) upper triangular, that is,
# Â Âº# Ã Ãƒ Ã # Â»
for all  Â , it follows that
 " Â Âº # Ã Ãƒ Ã  # Â» Â‹ Âº# Ã Ãƒ Ã # Â» ~ Âº" Ã Ãƒ Ã " Â»
and so the matrix Â´ ÂµE is also (almost) upper triangular.
A linear operator  is unitarily upper triangularizable if there is an ordered
orthonormal basis with respect to which  is upper triangular. Accordingly,
when = is an inner product space, we can replace the term â€œupper
triangularizableâ€ with â€œunitarily upper triangularizableâ€ in Schur's theorem. (A
similar statement holds for almost upper triangular matrices.)

Diagonalizable Operators
Definition A linear operator  Â BÂ²= Â³ is diagonalizable if there is an ordered
basis 8 ~ Â²# Ã Ãƒ Ã # Â³ of = for which the matrix Â´ Âµ8 is diagonal, or
equivalently, if

Eigenvalues and Eigenvectors

197

 # ~  #
for all  ~ Ã Ãƒ Ã .
The previous definition leads immediately to the following simple
characterization of diagonalizable operators.
Theorem 8.10 Let  Â BÂ²= Â³. The following are equivalent:
1)  is diagonalizable.
2) = has a basis consisting entirely of eigenvectors of  .
3) = has the form
= ~ ; l Ã„ l ;
where  Ã Ãƒ Ã  are the distinct eigenvalues of  .
Diagonalizable operators can also be characterized in a simple way via their
minimal polynomials.
Theorem 8.11 A linear operator  Â BÂ²= Â³ on a finite-dimensional vector space
is diagonalizable if and only if its minimal polynomial is the product of distinct
linear factors.
Proof. If  is diagonalizable, then
= ~ ; l Ã„ l ;
and Theorem 7.7 implies that  Â²%Â³ is the least common multiple of the
minimal polynomials % c  of  restricted to ; . Hence,  Â²%Â³ is a product of
distinct linear factors. Conversely, if  Â²%Â³ is a product of distinct linear
factors, then the primary decomposition of = has the form
= ~ = l Ã„ l =
where
= ~ Â¸# Â = Â“ Â² c  Â³# ~ Â¹ ~ ;
and so  is diagonalizable.

Spectral Resolutions
We have seen (Theorem 2.25) that resolutions of the identity on a vector space
= correspond to direct sum decompositions of = . We can do something similar
for any diagonalizable linear operator  on = (not just the identity operator).
Suppose that  has the form
 ~   b Ã„ b  
where  b Ã„ b  ~  is a resolution of the identity and the  Â - are
distinct. This is referred to as a spectral resolution of  .

198

Advanced Linear Algebra

We claim that the  's are the eigenvalues of  and imÂ² Â³ ~ ; . Theorem 2.25
implies that
= ~ imÂ² Â³ l Ã„ l imÂ² Â³
If  # Â imÂ² Â³, then
 Â² #Â³ ~ Â²  b Ã„ b   Â³ # ~  Â² #Â³
and so  # Â ; . Hence, imÂ² Â³ Â‹ ; and so
= ~ imÂ² Â³ l Ã„ l imÂ² Â³ Â‹ ; l Ã„ l ; Â‹ =
which implies that imÂ² Â³ ~ ; and
= ~ ; l Ã„ l ;
The converse also holds, for if = ~ ; l Ã„ l ; and if  is projection onto
; along the direct sum of the other eigenspaces, then
 b Ã„ b  ~ 
and since  ~   , it follows that
 ~  Â² b Ã„ b  Â³ ~   b Ã„ b  
Theorem 8.12 A linear operator  Â BÂ²= Â³ is diagonalizable if and only if it
has a spectral resolution
 ~   b Ã„ b  
In this case, Â¸ Ã Ãƒ Ã  Â¹ is the spectrum of  and
imÂ² Â³ ~ ;

and

kerÂ² Â³ ~  ;
Â£

Exercises
1.
2.
3.
4.
5.
6.

Let 1 be the  d  matrix all of whose entries are equal to . Find the
minimal polynomial and characteristic polynomial of 1 and the
eigenvalues.
Prove that the eigenvalues of a matrix do not form a complete set of
invariants under similarity.
Show that  Â BÂ²= Â³ is invertible if and only if  is not an eigenvalue of  .
Let ( be an  d  matrix over a field - that contains all roots of the
characteristic polynomial of (. Prove that detÂ²(Â³ is the product of the
eigenvalues of (, counting multiplicity.
Show that if  is an eigenvalue of  , then Â²Â³ is an eigenvalue of Â² Â³, for
any polynomial Â²%Â³. Also, if  Â£ , then c is an eigenvalue for  c .
An operator  Â BÂ²= Â³ is nilpotent if   ~  for some positive  Â o.

Eigenvalues and Eigenvectors

199

a) Show that if  is nilpotent, then the spectrum of  is Â¸Â¹.
b) Find a nonnilpotent operator  with spectrum Â¸Â¹.
7. Show that if Ã  Â BÂ²= Â³ and one of  and  is invertible, then  Â— 
and so  and  have the same eigenvalues, counting multiplicty.
8. (Halmos)
a) Find a linear operator  that is not idempotent but for which
  Â² c  Â³ ~ .
b) Find a linear operator  that is not idempotent but for which
 Â² c  Â³  ~  .
c) Prove that if   Â² c  Â³ ~  Â² c  Â³ ~ , then  is idempotent.
9. An involution is a linear operator for which  ~ . If  is idempotent
what can you say about  c  ? Construct a one-to-one correspondence
between the set of idempotents on = and the set of involutions.
10. Let (Ã ) Â 4 Â²dÂ³ and suppose that ( ~ )  ~ 0Ã ()( ~ ) c but
( Â£ 0 and ) Â£ 0 . Show that if * Â 4 Â²dÂ³ commutes with both ( and ) ,
then * ~ 0 for some scalar  Â d.
11. Let  Â BÂ²= Â³ and let
: ~ Âº#Ã  #Ã Ãƒ Ã  c #Â»
be a  -cyclic submodule of = with minimal polynomial Â²%Â³ where Â²%Â³
is prime of degree . Let  ~ Â² Â³ restricted to Âº#Â». Show that : is the
direct sum of  -cyclic submodules each of dimension , that is,
: ~ ; l Ã„ l ; 
Hint: For each  Â    , consider the set
8 ~ Â¸  #Ã Â² Â³  #Ã Ãƒ Ã Â² Â³c   #Â»
12. Fix  Â€ . Show that any complex matrix is similar to a matrix that looks
just like a Jordan matrix except that the entries that are equal to  are
replaced by entries with value  , where  is any complex number. Thus, any
complex matrix is similar to a matrix that is â€œalmostâ€ diagonal. Hint:
consider the fact that
v

w





 yv 


 zw 





 yv 


 zw 


c


 y v

~ 
c z w 





y

z

13. Show that the Jordan canonical form is not very robust in the sense that a
small change in the entries of a matrix ( may result in a large jump in the
entries of the Jordan form 1 . Hint: consider the matrix
( ~ >





?

What happens to the Jordan form of ( as  Â¦ ?

200

Advanced Linear Algebra

14. Give an example of a complex nonreal matrix all of whose eigenvalues are
real. Show that any such matrix is similar to a real matrix. What about the
type of the invertible matrices that are used to bring the matrix to Jordan
form?
15. Let 1 ~ Â´ Âµ8 be the Jordan form of a linear operator  Â BÂ²= Â³. For a given
Jordan block of 1 Â²Ã Â³ let < be the subspace of = spanned by the basis
vectors of 8 associated with that block.
a) Show that  O< has a single eigenvalue  with geometric multiplicity .
In other words, there is essentially only one eigenvector (up to scalar
multiple) associated with each Jordan block. Hence, the geometric
multiplicity of  for  is the number of Jordan blocks for  . Show that
the algebraic multiplicity is the sum of the dimensions of the Jordan
blocks associated with  .
b) Show that the number of Jordan blocks in 1 is the maximum number
of linearly independent eigenvectors of  .
c) What can you say about the Jordan blocks if the algebraic multiplicity
of every eigenvalue is equal to its geometric multiplicity?
16. Assume that the base field - is algebraically closed. Then assuming that the
eigenvalues of a matrix ( are known, it is possible to determine the Jordan
form 1 of ( by looking at the rank of various matrix powers. A matrix ) is
nilpotent if )  ~  for some  Â€ . The smallest such exponent is called
the index of nilpotence.
a) Let 1 ~ 1 Â²Ã Â³ be a single Jordan block of size  d . Show that
1 c 0 is nilpotent of index . Thus,  is the smallest integer for
which rkÂ²1 c 0Â³ ~ .
Now let 1 be a matrix in Jordan form but possessing only one eigenvalue
.
b) Show that 1 c 0 is nilpotent. Let  be its index of nilpotence. Show
that  is the maximum size of the Jordan blocks of 1 and that
rkÂ²1 c 0Â³c is the number of Jordan blocks in 1 of maximum size.
c) Show that rkÂ²1 c 0Â³c is equal to  times the number of Jordan
blocks of maximum size plus the number of Jordan blocks of size one
less than the maximum.
d) Show that the sequence rkÂ²1 c 0Â³ for  ~ Ã Ãƒ Ã  uniquely
determines the number and size of all of the Jordan blocks in 1 , that is,
it uniquely determines 1 up to the order of the blocks.
e) Now let 1 be an arbitrary Jordan matrix. If  is an eigenvalue for 1
show that the sequence rkÂ²1 c 0Â³ for  ~ Ã Ãƒ Ã  where  is the
first integer for which rkÂ²1 c 0Â³ ~ rkÂ²1 c 0Â³b uniquely
determines 1 up to the order of the blocks.
f) Prove that for any matrix ( with spectrum Â¸ Ã Ãƒ Ã  Â¹ the sequence
rkÂ²( c  0Â³ for  ~ Ã Ãƒ Ã and  ~ Ã Ãƒ Ã  where  is the first
integer for which rkÂ²( c  0Â³ ~ rkÂ²( c  0Â³b uniquely
determines the Jordan matrix 1 for ( up to the order of the blocks.
17. Let ( Â C Â²- Â³.

Eigenvalues and Eigenvectors
a)

201

If all the roots of the characteristic polynomial of ( lie in - prove that
( is similar to its transpose (! . Hint: Let ) be the matrix
v
xÃ…
)~x

w

Ã„  y
Ã‡  {
{
Ã‡ Ã‡ Ã…
 Ã„ z

with 's on the diagonal that moves up from left to right and 's
elsewhere. Let 1 be a Jordan block of the same size as ) . Show that
)1 ) c ~ 1 ! .
b) Let (Ã ) Â C Â²- Â³. Let 2 be a field containing - . Show that if ( and
) are similar over 2 , that is, if ) ~ 7 (7 c where 7 Â C Â²2Â³, then
( and ) are also similar over - , that is, there exists 8 Â C Â²- Â³ for
which ) ~ 8(8c .
c) Show that any matrix is similar to its transpose.

The Trace of a Matrix
18. Let ( Â C Â²- Â³. Verify the following statements.
a) trÂ²AÂ³ ~  trÂ²(Â³, for  Â - .
b) trÂ²( b )Â³ ~ trÂ²(Â³ b trÂ²)Â³.
c) trÂ²()Â³ ~ trÂ²)(Â³.
d) trÂ²()*Â³ ~ trÂ²*()Â³ ~ trÂ²)*(Â³. Find an example to show that
trÂ²()*Â³ may not equal trÂ²(*)Â³.
e) The trace is an invariant under similarity.
f) If - is algebraically closed, then the trace of ( is the sum of the
eigenvalues of (.
19. Use the concept of the trace of a matrix, as defined in the previous exercise,
to prove that there are no matrices (, ) Â C Â²dÂ³ for which
() c )( ~ 0
20. Let ; Â¢ C Â²- Â³ Â¦ - be a function with the following properties. For all
matrices (Ã ) Â C Â²- Â³ and  Â - ,
1) ; Â²AÂ³ ~ ; Â²(Â³
2) ; Â²( b )Â³ ~ ; Â²(Â³ b ; Â²)Â³
3) ; Â²()Â³ ~ ; Â²)(Â³
Show that there exists
Â - for which ; Â²(Â³ ~ trÂ²(Â³, for all
( Â C Â²- Â³.

Commuting Operators
Let
< ~ Â¸ Â BÂ²= Â³ Â“  Â ? Â¹
be a family of operators on a vector space = . Then < is a commuting family if
every pair of operators commutes, that is,  ~  for all Ã  Â < . A subspace

202

Advanced Linear Algebra

< of = is < -invariant if it is  -invariant for every  Â < . It is often of interest
to know whether a family < of linear operators on = has a common
eigenvector, that is, a single vector # Â = that is an eigenvector for every
 Â < (the corresponding eigenvalues may be different for each operator,
however).
21. A pair of linear operators Ã  Â BÂ²= Â³ is simultaneously diagonalizable if
there is an ordered basis 8 for = for which Â´ Âµ8 and Â´Âµ8 are both diagonal,
that is, 8 is an ordered basis of eigenvectors for both  and  . Prove that
two diagonalizable operators  and  are simultaneously diagonalizable if
and only if they commute, that is,  ~  . Hint: If  ~  , then the
eigenspaces of  are invariant under  .
22. Let Ã  Â BÂ²= Â³. Prove that if  and  commute, then every eigenspace of
 is  -invariant. Thus, if < is a commuting family, then every eigenspace
of any member of < is < -invariant.
23. Let < be a family of operators in BÂ²= Â³ with the property that each operator
in < has a full set of eigenvalues in the base field - , that is, the
characteristic polynomial splits over - . Prove that if < is a commuting
family, then < has a common eigenvector # Â = .
24. What do the real matrices
(~>


c



and ) ~ >
?
c


?

have to do with the issue of common eigenvectors?

GerÅ¡gorin Disks
It is generally impossible to determine precisely the eigenvalues of a given
complex operator or matrix ( Â C Â²dÂ³, for if  Â‚ , then the characteristic
equation has degree
and cannot in general be solved. As a result, the
approximation of eigenvalues is big business. Here we consider one aspect of
this approximation problem, which also has some interesting theoretical
consequences.
Let ( Â C Â²dÂ³ and suppose that (# ~ # where # ~ Â² Ã Ãƒ Ã  Â³! . Comparing
th rows gives


(  ~ 
~

which can also be written in the form


 Â² c ( Â³ ~ ( 
~
Â£

If  has the property that ( ( Â‚ ( ( for all , we have

Eigenvalues and Eigenvectors



203



( (( c ( ( Â (( (( ( Â ( ((( (
~
Â£

~
Â£

and thus


( c ( ( Â (( (

(8.7)

~
Â£

The right-hand side is the sum of the absolute values of all entries in the  th row
of ( except the diagonal entry ( . This sum 9 Â²(Â³ is the  th deleted absolute
row sum of (. The inequality (8.7) says that, in the complex plane, the
eigenvalue  lies in the disk centered at the diagonal entry ( with radius equal
to 9 Â²(Â³. This disk
GR Â²(Â³ ~ Â¸' Â d Â“ (' c ( ( Â 9 Â²(Â³Â¹
is called the GerÅ¡gorin row disk for the  th row of (. The union of all of the
GerÅ¡gorin row disks is called the GerÅ¡gorin row region for (.
Since there is no way to know in general which is the index  for which
( ( Â‚ ( (, the best we can say in general is that the eigenvalues of ( lie in the
union of all GerÅ¡gorin row disks, that is, in the GerÅ¡gorin row region of (.
Similar definitions can be made for columns and since a matrix has the same
eigenvalues as its transpose, we can say that the eigenvalues of ( lie in the
GerÅ¡gorin column region of (. The GerÅ¡gorin region .Â²(Â³ of a matrix
( Â 4 Â²- Â³ is the intersection of the GerÅ¡gorin row region and the GerÅ¡gorin
column region and we can say that all eigenvalues of ( lie in the GerÅ¡gorin
region of (. In symbols, ( Â‹ .(.
25. Find and sketch the GerÅ¡gorin region and the eigenvalues for the matrix
(~

v

w



y
z

26. A matrix ( Â 4 Â²dÂ³ is diagonally dominant if for each  ~ Ã Ãƒ Ã ,
(( ( Â‚ 9 Â²(Â³
and it is strictly diagonally dominant if strict inequality holds. Prove that
if ( is strictly diagonally dominant, then it is invertible.
27. Find a matrix ( Â 4 Â²dÂ³ that is diagonally dominant but not invertible.
28. Find a matrix ( Â 4 Â²dÂ³ that is invertible but not strictly diagonally
dominant.

Chapter 9

Real and Complex Inner Product Spaces

We now turn to a discussion of real and complex vector spaces that have an
additional function defined on them, called an inner product, as described in the
following definition. In this chapter, - will denote either the real or complex
field. Also, the complex conjugate of  Â d is denoted by .
Definition Let = be a vector space over - ~ s or - ~ d. An inner product
on = is a function ÂºÃ Â»Â¢ = d = Â¦ - with the following properties:
1) (Positive definiteness) For all # Â = ,
Âº#Ã #Â» Â‚ 

and Âº#Ã #Â» ~  Â¯ # ~ 

2) For - ~ d: (Conjugate symmetry)
Âº"Ã #Â» ~ Âº#Ã "Â»
For - ~ s: (Symmetry)
Âº"Ã #Â» ~ Âº#Ã "Â»
3) (Linearity in the first coordinate) For all "Ã # Â = and Ã Â Âº" b #Ã $Â» ~ Âº"Ã $Â» b Âº#Ã $Â»
A real (or complex) vector space = , together with an inner product, is called a
real (or complex) inner product space.
If ?Ã @ Â‹ = , then we let
Âº?Ã @ Â» ~ Â¸Âº%Ã &Â» Â“ % Â ?Ã & Â @ Â¹
and
Âº#Ã ?Â» ~ Â¸Âº#Ã %Â» Â“ % Â ?Â¹
Note that a vector subspace : of an inner product space = is also an inner
product space under the restriction of the inner product of = to : .

206

Advanced Linear Algebra

We will study bilinear forms (also called inner products) on vector spaces over
fields other than s or d in Chapter 11. Note that property 1) implies that Âº#Ã #Â»
is always real, even if = is a complex vector space.
If - ~ s, then properties 2) and 3) imply that the inner product is linear in both
coordinates, that is, the inner product is bilinear. However, if - ~ d, then
Âº$Ã " b #Â» ~ Âº" b #Ã $Â» ~ Âº"Ã $Â» b Âº#Ã $Â» ~ Âº$Ã "Â» b Âº$Ã #Â»
This is referred to as conjugate linearity in the second coordinate. Specifically,
a function  Â¢ = Â¦ > between complex vector spaces is conjugate linear if
 Â²" b #Â³ ~  Â²"Â³ b  Â²#Â³
and
 Â²"Â³ ~  Â²"Â³
for all "Ã # Â = and  Â d. Thus, a complex inner product is linear in its first
coordinate and conjugate linear in its second coordinate. This is often described
by saying that a complex inner product is sesquilinear. (Sesqui means â€œone and
a half times.â€)
Example 9.1
1) The vector space s is an inner product space under the standard inner
product, or dot product, defined by
ÂºÂ² Ã Ãƒ Ã  Â³Ã Â²  Ã Ãƒ Ã  Â³Â» ~   b Ã„ b  
The inner product space s is often called -dimensional Euclidean
space.
2) The vector space d is an inner product space under the standard inner
product defined by
ÂºÂ² Ã Ãƒ Ã  Â³Ã Â²  Ã Ãƒ Ã  Â³Â» ~   b Ã„ b  
This inner product space is often called -dimensional unitary space.
3) The vector space *Â´Ã Âµ of all continuous complex-valued functions on the
closed interval Â´Ã Âµ is a complex inner product space under the inner
product


Âº Ã Â» ~

 Â²%Â³Â²%Â³ %


Example 9.2 One of the most important inner product spaces is the vector space
M of all real (or complex) sequences Â²  Â³ with the property that
(  (  B

Real and Complex Inner Product Spaces

207

under the inner product
B

ÂºÂ²  Â³Ã Â²! Â³Â» ~   !
~

Such sequences are called square summable. Of course, for this inner product
to make sense, the sum on the right must converge. To see this, note that if
Â²  Â³Ã Â²! Â³ Â M , then
 Â Â²(  ( c (! (Â³ ~ (  ( c (  ((! ( b (! (
and so
(  ! ( Â (  ( b (! (
which implies that Â²  ! Â³ Â M . We leave it to the reader to verify that M is an
inner product space.
The following simple result is quite useful.
Lemma 9.1 If = is an inner product space and Âº"Ã %Â» ~ Âº#Ã %Â» for all % Â = ,
then " ~ #.
The next result points out one of the main differences between real and complex
inner product spaces and will play a key role in later work.
Theorem 9.2 Let = be an inner product space and let  Â BÂ²= Â³.
1)
Âº #Ã $Â» ~  for all #Ã $ Â =

Â¬

 ~

Â¬

 ~

2) If = is a complex inner product space, then
Âº #Ã #Â» ~  for all # Â =

but this does not hold in general for real inner product spaces.
Proof. Part 1) follows directly from Lemma 9.1. As for part 2), let # ~ % b &,
for %Ã & Â = and  Â - . Then
 ~ Âº Â²% b &Â³Ã % b &Â»
~ (( Âº %Ã %Â» b Âº &Ã &Â» b Âº %Ã &Â» b Âº &Ã %Â»
~ Âº %Ã &Â» b Âº &Ã %Â»
Setting  ~  gives
Âº %Ã &Â» b Âº &Ã %Â» ~ 
and setting  ~  gives

208

Advanced Linear Algebra

Âº %Ã &Â» c Âº &Ã %Â» ~ 
These two equations imply that Âº %Ã &Â» ~  for all %Ã & Â = and so part 1)
implies that  ~ . For the last statement, rotation by  degrees in the real
plane s has the property that Âº #Ã #Â» ~  for all #.

Norm and Distance
If = is an inner product space, the norm, or length of # Â = is defined by
)#) ~ jÂº#Ã #Â»

(9.1)

A vector # is a unit vector if )#) ~ . Here are the basic properties of the norm.
Theorem 9.3
1) )#) Â‚  and )#) ~  if and only if # ~ .
2) For all  Â - and # Â = ,
)#) ~ (()#)
3) (The Cauchyâ€“Schwarz inequality) For all "Ã # Â = ,
(Âº"Ã #Â»( Â )"))#)
with equality if and only if one of " and # is a scalar multiple of the other.
4) (The triangle inequality) For all "Ã # Â = ,
)" b #) Â )") b )#)
with equality if and only if one of " and # is a scalar multiple of the other.
5) For all "Ã #Ã % Â = ,
)" c #) Â )" c %) b )% c #)
6) For all "Ã # Â = ,
()") c )#)( Â )" c #)
7) (The parallelogram law) For all "Ã # Â = ,
)" b #) b )" c #) ~ )") b )#)
Proof. We prove only Cauchyâ€“Schwarz and the triangle inequality. For
Cauchyâ€“Schwarz, if either " or # is zero the result follows, so assume that
"Ã # Â£ . Then, for any scalar  Â - ,
 Â )" c #)
~ Âº" c #Ã " c #Â»
~ Âº"Ã "Â» c Âº"Ã #Â» c Â´Âº#Ã "Â» c Âº#Ã #Â»Âµ
Choosing  ~ Âº#Ã "Â»Â°Âº#Ã #Â» makes the value in the square brackets equal to 

Real and Complex Inner Product Spaces

209

and so
 Â Âº"Ã "Â» c

Âº#Ã "Â»Âº"Ã #Â»
(Âº"Ã #Â»(
~ )") c
Âº#Ã #Â»
)#)

which is equivalent to the Cauchyâ€“Schwarz inequality. Furthermore, equality
holds if and only if )" c #) ~ , that is, if and only if " c # ~ , which is
equivalent to " and # being scalar multiples of one another.
To prove the triangle inequality, the Cauchyâ€“Schwarz inequality gives
)" b #) ~ Âº" b #Ã " b #Â»
~ Âº"Ã "Â» b Âº"Ã #Â» b Âº#Ã "Â» b Âº#Ã #Â»
Â )") b )"))#) b )#)
~ Â²)") b )#)Â³
from which the triangle inequality follows. The proof of the statement
concerning equality is left to the reader.
Any vector space = , together with a function ) h )Â¢ = Â¦ s that satisfies
properties 1), 2) and 4) of Theorem 9.3, is called a normed linear space and the
function ) h ) is called a norm. Thus, any inner product space is a normed linear
space, under the norm given by (9.1).
It is interesting to observe that the inner product on = can be recovered from the
norm. Thus, knowing the length of all vectors in = is equivalent to knowing all
inner products of vectors in = .
Theorem 9.4 (The polarization identities)
1) If = is a real inner product space, then
Âº"Ã #Â» ~


Â²)" b #) c )" c #) Â³


2) If = is a complex inner product space, then
Âº"Ã #Â» ~



Â²)" b #) c )" c #) Â³ b Â²)" b #) c )" c #) Â³



The norm can be used to define the distance between any two vectors in an
inner product space.
Definition Let = be an inner product space. The distance Â²"Ã #Â³ between any
two vectors " and # in = is
Â²"Ã #Â³ ~ )" c #)
Here are the basic properties of distance.

(9.2)

