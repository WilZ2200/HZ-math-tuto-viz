Structure Theory for Normal Operators

233

2) Conversely, if
= ~ : p Ã„ p :
and if  is orthogonal projection onto : , then  b Ã„ b  ~  is an
orthogonal resolution of the identity.
Proof. To prove 1), if  b Ã„ b  ~  is an orthogonal resolution of the
identity, Theorem 2.25 implies that
= ~ imÂ² Â³ l Ã„ l imÂ² Â³
However, since the  's are pairwise orthogonal and self-adjoint, it follows that
Âº #Ã  $Â» ~ Âº#Ã   $Â» ~ Âº#Ã Â» ~ 
and so
= ~ imÂ² Â³ p Ã„ p imÂ² Â³
For the converse, Theorem 2.25 implies that  b Ã„ b  ~  is a resolution of
the identity where  is projection onto imÂ² Â³ along
kerÂ² Â³ ~  imÂ² Â³ ~ imÂ² Â³Â
Â£

Hence,  is orthogonal.

Unitary Diagonalizability
We have seen (Theorem 8.10) that a linear operator  Â BÂ²= Â³ on a finitedimensional vector space = is diagonalizable if and only if
= ~ ; l Ã„ l ;
Of course, each eigenspace ; has an orthonormal basis E , but the union of
these bases need not be an orthonormal basis for = .
Definition A linear operator  Â BÂ²= Â³ is unitarily diagonalizable (when = is
complex) and orthogonally diagonalizable (when = is real) if there is an
ordered orthonormal basis E ~ Â²" Ã Ãƒ Ã " Â³ of = for which the matrix Â´ ÂµE is
diagonal, or equivalently, if
 " ~  "
for all  ~ Ã Ãƒ Ã .
Here is the counterpart of Theorem 8.10 for inner product spaces.
Theorem 10.7 Let = be a finite-dimensional inner product space and let
 Â BÂ²= Â³. The following are equivalent:
1)  is unitarily (orthogonally) diagonalizable.
2) = has an orthonormal basis that consists entirely of eigenvectors of  .

234

Advanced Linear Algebra

3) = has the form
= ~ ; p Ã„ p ;
where  Ã Ãƒ Ã  are the distinct eigenvalues of  .
For simplicity in exposition, we will tend to use the term unitarily
diagonalizable for both cases. Since unitarily diagonalizable operators are so
well behaved, it is natural to seek a characterization of such operators.
Remarkably, there is a simple one, as we will see next.

Normal Operators
Operators that commute with their own adjonts are very special.
Definition
1) A linear operator  on an inner product space = is normal if it commutes
with its adjoint:
 i ~  i 
2) A matrix ( Â C Â²- Â³ is normal if ( commutes with its adjoint (i .
If  is normal and E is an ordered orthonormal basis of = , then
Â´ ÂµE Â´ ÂµiE ~ Â´ ÂµE Â´ i ÂµE ~ Â´ i ÂµE
and
Â´ ÂµiE Â´ ÂµE ~ Â´ i ÂµE Â´ ÂµE ~ Â´ i  ÂµE
and so  is normal if and only if Â´ ÂµE is normal for some, and hence all,
orthonormal bases for = . Note that this does not hold for bases that are not
orthonormal.
Normal operators have some very special properties.
Theorem 10.8 Let  Â BÂ²= Â³ be normal.
1) The following are also normal:
a)  O: , if  reduces Â²:Ã : Â Â³
b)  i
c)  c , if  is invertible
d) Â² Â³, for any polynomial Â²%Â³ Â - Â´%Âµ
2) For any #Ã $ Â = ,
Âº #Ã  $Â» ~ Âº i #Ã  i $Â»
and, in particular,
) #) ~ ) i #)

Structure Theory for Normal Operators

235

and so
kerÂ² i Â³ ~ kerÂ² Â³
3) For any integer  Â‚ ,
kerÂ²  Â³ ~ kerÂ² Â³
4) The minimal polynomial  Â²%Â³ is a product of distinct prime monic
polynomials.
5)
 # ~ #

Â¯

 i # ~ #

6) If : and ; are submodules of = with relatively prime orders, then : Â ; .
7) If  and  are distinct eigenvalues of  , then ; Â ; .
Proof. We leave part 1) for the reader. For part 2), normality implies that
Âº #Ã  $Â» ~ Âº i  #Ã #Â» ~ Âº i #Ã #Â» ~ Âº i #Ã  i #Â»
We prove part 3) first for the operator  ~  i  , which is self-adjoint, that is,
 i ~ Â² i  Â³ i ~  i  ~ 
If  # ~  for  Â€ , then
 ~ Âº #Ã c #Â» ~ Âºc #Ã c #Â»
and so c # ~ . Continuing in this way gives # ~ . Now, if   # ~  for
 Â€ , then
  # ~ Â² i  Â³  # ~ Â²  i Â³    # ~ 
and so # ~ . Hence,
 ~ Âº#Ã #Â» ~ Âº i  #Ã #Â» ~ Âº #Ã  #Â»
and so # ~ .
For part 4), suppose that
 Â²%Â³ ~  Â²%Â³Â²%Â³
where Â²%Â³ is monic and prime. Then for any # Â = ,
 Â² Â³Â´Â² Â³#Âµ ~ 
and since Â² Â³ is also normal, part 3) implies that
Â² Â³Â´Â² Â³#Âµ ~ 
for all # Â = . Hence, Â² Â³Â² Â³ ~ , which implies that  ~ . Thus, the prime
factors of  Â²%Â³ appear only to the first power.

236

Advanced Linear Algebra

Part 5) follows from part 2):
kerÂ² c Â³ ~ kerÂ´Â² c Â³i Âµ ~ kerÂ² i c Â³
For part 6), if Â²:Â³ ~ Â²%Â³ and Â²; Â³ ~ Â²%Â³, then there are polynomials Â²%Â³
and Â²%Â³ for which Â²%Â³Â²%Â³ b Â²%Â³Â²%Â³ ~  and so
Â² Â³Â² Â³ b Â² Â³Â² Â³ ~ 
Now,  ~ Â² Â³Â² Â³ annihilates : and  ~ Â² Â³Â² Â³ annihilates ; . Therefore
 i also annihilates ; and so
Âº:Ã ; Â» ~ ÂºÂ² b  Â³:Ã ; Â» ~ Âº :Ã ; Â» ~ Âº:Ã  i ; Â» ~ Â¸Â¹
Part 7) follows from part 6), since Â²; Â³ ~ % c  and Â²; Â³ ~ % c  are
relatively prime when  Â£ . Alternatively, for # Â ; and $ Â ; , we have
Âº#Ã $Â» ~ Âº #Ã $Â» ~ Âº#Ã  i $Â» ~ Âº#Ã $Â» ~ Âº#Ã $Â»
and so  Â£  implies that Âº#Ã $Â» ~ .

The Spectral Theorem for Normal Operators
Theorem 10.8 implies that when - ~ d, the minimal polynomial  Â²%Â³ splits
into distinct linear factors and so Theorem 8.11 implies that  is diagonalizable,
that is,
= ~ ; l Ã„ l ;
Moreover, since distinct eigenspaces of a normal operator are orthogonal, we
have
= ~ ; p Ã„ p ;
and so  is unitarily diagonalizable.
The converse of this is also true. If = has an orthonormal basis E ~
Â¸# Ã Ãƒ Ã # Â¹ of eigenvectors for  , then since Â´ ÂµE and Â´ i ÂµE ~ Â´ ÂµiE are
diagonal, these matrices commute and therefore so do  i and  .
Theorem 10.9 (The spectral theorem for normal operators: complex case)
Let = be a finite-dimensional complex inner product space and let  Â BÂ²= Â³.
The following are equivalent:
1)  is normal.
2)  is unitarily diagonalizable, that is,
= ~ ; p Ã„ p ;

Structure Theory for Normal Operators

237

3)  has an orthogonal spectral resolution
(10.1)

 ~   b Ã„ b  

where  b Ã„ b  ~  and  is orthogonal for all , in which case,
Â¸ Ã Ãƒ Ã  Â¹ is the spectrum of  and
and

imÂ² Â³ ~ ;

kerÂ² Â³ ~  ;
Â£

Proof. We have seen that 1) and 2) are equivalent. To see that 2) and 3) are
equivalent, Theorem 8.12 says that
= ~ ; l Ã„ l ;
if and only if
 ~   b Ã„ b  
and in this case,
imÂ² Â³ ~ ;

and

kerÂ² Â³ ~  ;
Â£

But ; Â ; for  Â£  if and only if
imÂ² Â³ Â kerÂ² Â³
that is, if and only if each  is orthogonal. Hence, the direct sum = ~
; l Ã„ l ; is an orthogonal sum if and only if each projection is
orthogonal.
The Real Case
If - ~ s, then  Â²%Â³ has the form
 Â²%Â³ ~ Â²% c  Â³Ã„Â²% c  Â³ Â²%Â³Ã„ Â²%Â³
where each  Â²%Â³ is an irreducible monic quadratic. Hence, the primary cyclic
decomposition of = gives
= ~ ; p Ã„ p ; p > p Ã„ p >
where > is cyclic with prime quadratic order  Â²%Â³. Therefore, Theorem 8.8
implies that there is an ordered basis 8 for which

Â´ O> Âµ8 ~ > 


c
 ?

Theorem 10.10 (The spectral theorem for normal operators: real case) A
linear operator  on a finite-dimensional real inner product space is normal if
and only if

238

Advanced Linear Algebra

= ~ ; p Ã„ p ; p > p Ã„ p >
where Â¸ Ã Ãƒ Ã  Â¹ is the spectrum of  and each > is an indecomposable twodimensional  -invariant subspace with an ordered basis 8 for which

Â´ Âµ8 ~ > 


c
 ?

Proof. We need only show that if = has such a decomposition, then  is normal.
But
Â´ Âµ8 Â´ Âµ!8 ~ Â² b  Â³0 ~ Â´ Âµ!8 Â´ Âµ8
and so Â´ Âµ8 is normal. It follows easily that  is normal.

Special Types of Normal Operators
We now want to introduce some special types of normal operators.
Definition Let = be an inner product space.
1)  Â BÂ²= Â³ is self-adjoint (also called Hermitian in the complex case and
symmetric in the real case) if
i ~ 
2)  Â BÂ²= Â³ is skew self-adjoint (also called skew-Hermitian in the
complex case and skew-symmetric in the real case) if
 i ~ c
3)  Â BÂ²= Â³ is unitary in the complex case and orthogonal in the real case if
 is invertible and
 i ~  c
There are also matrix versions of these definitions, obtained simply by replacing
the operator  by a matrix (. Moreover, the operator  is self-adjoint if and only
if any matrix that represents  with respect to an ordered orthonormal basis E is
self-adjoint. Similar statements hold for the other types of operators in the
previous definition.
In some sense, square complex matrices are a generalization of complex
numbers and the adjoint (conjugate transpose) is a generalization of the complex
conjugate. In looking for a better analogy, we could consider just the diagonal
matrices, but this is a bit too restrictive. The next logical choice is the set D of
normal matrices.
Indeed, among the complex numbers, there are some special subsets: the real
numbers, the positive numbers and the numbers on the unit circle. We will soon
see that a complex matrix ( is self-adjoint if and only if its complex eigenvalues

Structure Theory for Normal Operators

239

are real. This would suggest that the analog of the set of real numbers is the set
of self-adjoint matrices. Also, we will see that a complex matrix is unitary if and
only if its eigenvalues have norm , so numbers on the unit circle seem to
correspond to the set of unitary matrices. This leaves open the question of which
normal matrices correspond to the positive real numbers. These are the positive
definite matrices, which we will discuss later in the chapter.

Self-Adjoint Operators
Let us consider the basic properties of self-adjoint operators. The quadratic
form associated with the linear operator  is the function 8 Â¢ = Â¦ - defined
by
8 Â²#Â³ ~ Âº #Ã #Â»
We have seen (Theorem 9.2) that in a complex inner product space,  ~  if and
only if 8 ~  but this does not hold, in general, for real inner product spaces.
However, it does hold for symmetric operators on a real inner product space.
Theorem 10.11 Let = be a finite-dimensional inner product space and let
Ã  Â BÂ²= Â³.
1) If  and  are self-adjoint, then so are the following:
a)  b 
b)  c , if  is invertible
c) Â² Â³, for any real polynomial Â²%Â³ Â sÂ´%Âµ
2) A complex operator  is Hermitian if and only if 8 Â²#Â³ is real for all
#Â=.
3) If  is a complex operator or a real symmetric operator, then
 ~

Â¯

8 ~ 

4) The characteristic polynomial  Â²%Â³ of a self-adjoint operator  splits over
s, that is, all complex roots of  Â²%Â³ are real. Hence, the minimal
polynomial  Â²%Â³ of  is the product of distinct monic linear factors over
s.
Proof. For part 2), if  is Hermitian, then
Âº #Ã #Â» ~ Âº#Ã  #Â» ~ Âº #Ã #Â»
and so 8 Â²#Â³ ~ Âº #Ã #Â» is real. Conversely, if Âº #Ã #Â» Â s, then
Âº#Ã  #Â» ~ Âº #Ã #Â» ~ Âº#Ã  i #Â»
and so  ~  i .
For part 3), we need only prove that 8 ~  implies  ~  when - ~ s. But if
8 ~ , then

240

Advanced Linear Algebra

 ~ Âº Â²% b &Â³Ã % b &Â»
~ Âº %Ã %Â» b Âº &Ã &Â» b Âº %Ã &Â» b Âº &Ã %Â»
~ Âº %Ã &Â» b Âº &Ã %Â»
~ Âº %Ã &Â» b Âº%Ã  &Â»
~ Âº %Ã &Â» b Âº %Ã &Â»
~ Âº %Ã &Â»
and so  ~ .
For part 4), if  is Hermitian (- ~ d) and  # ~ #, then
 # ~  # ~  i # ~ #
and so  ~  is real. If  is symmetric (- ~ s) , we must be a bit careful, since
a nonreal root of  Â²%Â³ is not an eigenvalue of  . However, matrix techniques
can come to the rescue here. If ( ~ Â´ ÂµE for any ordered orthonormal basis E
for = , then  Â²%Â³ ~ ( Â²%Â³. Now, ( is a real symmetric matrix, but can be
thought of as a complex Hermitian matrix with real entries. As such, it
represents a Hermitian linear operator on the complex space d and so, by what
we have just shown, all (complex) roots of its characteristic polynomial are real.
But the characteristic polynomial of ( is the same, whether we think of ( as a
real or a complex matrix and so the result follows.

Unitary Operators and Isometries
We now turn to the basic properties of unitary operators. These are the
workhorse operators, in that a unitary operator is precisely a normal operator
that maps orthonormal bases to orthonormal bases.
Note that  is unitary if and only if
Âº #Ã $Â» ~ Âº#Ã  c $Â»
for all #Ã $ Â = .
Theorem 10.12 Let = be a finite-dimensional inner product space and let
Ã  Â BÂ²= Â³.
1) If  and  are unitary/orthogonal, then so are the following:
a)  , for  Â dÃ (( ~ 
b) 
c)  c , if  is invertible.
2)  is unitary/orthogonal if and only it is an isometric isomorphism.
3)  is unitary/orthogonal if and only if it takes some orthonormal basis to an
orthonormal basis, in which case it takes all orthonormal bases to
orthonormal bases.
4) If  is unitary/orthogonal, then the eigenvalues of  have absolute value .

Structure Theory for Normal Operators

241

Proof. We leave the proof of part 1) to the reader. For part 2), a
unitary/orthogonal map is injective and since = is finite-dimensional, it is
bijective. Moreover, for a bijective linear map  , we have
 is an isometry Â¯ Âº #Ã  $Â» ~ Âº#Ã $Â» for all #Ã $ Â =
Â¯ Âº#Ã  i  $Â» ~ Âº#Ã $Â» for all #Ã $ Â =
Â¯  i ~ 
Â¯  i ~  c
Â¯  is unitary/orthogonal
For part 3), suppose that  is unitary/orthogonal and that E ~ Â¸" Ã Ãƒ Ã " Â¹ is an
orthonormal basis for = . Then
Âº " Ã  " Â» ~ Âº" Ã " Â» ~ Ã
and so E is an orthonormal basis for = . Conversely, suppose that E and E
are orthonormal bases for = . Then
Âº " Ã  " Â» ~ Ã ~ Âº" Ã " Â»
which implies that Âº #Ã  $Â» ~ Âº#Ã $Â» for all #Ã $ Â =
unitary/orthogonal.

and so  is

For part 4), if  is unitary and  # ~ #, then
Âº#Ã #Â» ~ Âº#Ã #Â» ~ Âº #Ã  #Â» ~ Âº#Ã #Â»
and so (( ~  ~ , which implies that (( ~ .
We also have the following theorem concerning unitary (and orthogonal)
matrices.
Theorem 10.13 Let ( be an  d  matrix over - ~ d or - ~ s.
1) The following are equivalent:
a) ( is unitary/orthogonal.
b) The columns of ( form an orthonormal set in -  .
c) The rows of ( form an orthonormal set in -  .
2) If ( is unitary, then (detÂ²(Â³( ~ . If ( is orthogonal, then detÂ²(Â³ ~ f.
Proof. The matrix ( is unitary if and only if ((i ~ 0 , which is equivalent to
the rows of ( being orthonormal. Similarly, ( is unitary if and only if
(i ( ~ 0 , which is equivalent to the columns of ( being orthonormal. As for
part 2),
((i ~ 0

Â¬

detÂ²(Â³detÂ²(i Â³ ~ 

from which the result follows.

Â¬

detÂ²(Â³detÂ²(Â³ ~ 

242

Advanced Linear Algebra

Unitary/orthogonal matrices play the role of change of basis matrices when we
restrict attention to orthonormal bases. Let us first note that if 8 ~ Â²" Ã Ãƒ Ã " Â³
is an ordered orthonormal basis and
# ~  " b Ã„ b  "
$ ~  "  b Ã„ b   " 
then
Âº#Ã $Â» ~   b Ã„ b   ~ Â´#Âµ8 h Â´$Âµ8
where the right hand side is the standard inner product in -  and so # Â $ if
and only if Â´#Âµ8 Â Â´$Âµ8 . We can now state the analog of Theorem 2.9.
Theorem 10.14 If we are given any two of the following:
1) A unitary/orthogonal  d  matrix (,
2) An ordered orthonormal basis 8 for -  ,
3) An ordered orthonormal basis 9 for -  ,
then the third is uniquely determined by the equation
( ~ 48 Ã 9
Proof. Let 8 ~ Â¸ Â¹ be a basis for = . If 9 is an orthonormal basis for = , then
Âº Ã  Â» ~ Â´ Âµ9 h Â´ Âµ9
where Â´ Âµ9 is the th column of ( ~ 48Ã9 . Hence, ( is unitary if and only if 8
is orthonormal. We leave the rest of the proof to the reader.

Unitary Similarity
We have seen that the change of basis formula for operators is given by
Â´ Âµ8Z ~ 7 Â´ Âµ8 7 c
where 7 is an invertible matrix. What happens when the bases are orthonormal?
Definition
1) Two complex matrices ( and ) are unitarily similar (also called
unitarily equivalent) if there exists a unitary matrix < for which
) ~ < (< c ~ < (< i
The equivalence classes associated with unitary similarity are called
unitary similarity classes.
2) Similarly, two real matrices ( and ) are orthogonally similar (also called
orthogonally equivalent) if there exists an orthogonal matrix 6 for which
) ~ 6(6c ~ 6(6!
The equivalence classes associated with orthogonal similarity are called
orthogonal similarity classes.

Structure Theory for Normal Operators

243

The analog of Theorem 2.19 is the following.
Theorem 10.15 Let = be an inner product space of dimension . Then two
 d  matrices ( and ) are unitarily/orthogonally similar if and only if they
represent the same linear operator  Â BÂ²= Â³ with respect to (possibly different)
ordered orthonormal bases. In this case, ( and ) represent exactly the same
set of linear operators in BÂ²= Â³ with respect to ordered orthonormal bases.
Proof. If ( and ) represent  Â BÂ²= Â³, that is, if
( ~ Â´ Âµ8

and

) ~ Â´  Âµ9

for ordered orthonormal bases 8 and 9, then
) ~ 48Ã9 (49Ã8
and according to Theorem 10.14, 48Ã9 is unitary/orthogonal. Hence, ( and )
are unitarily/orthogonally similar.
Now suppose that ( and ) are unitarily/orthogonally similar, say
) ~ < (< c
where < is unitary/orthogonal. Suppose also that ( represents a linear operator
 Â BÂ²= Â³ for some ordered orthonormal basis 8 , that is,
( ~ Â´ Âµ8
Theorem 10.14 implies that there is a unique ordered orthonormal basis 9 for =
for which < ~ 48Ã9 . Hence
) ~ 48Ã9 Â´ Âµ8 48c
Ã9 ~ Â´ Âµ9
and so ) also represents  . By symmetry, we see that ( and ) represent the
same set of linear operators, under all possible ordered orthonormal bases.
We have shown (see the discussion of Schur's theorem) that any complex matrix
( is unitarily similar to an upper triangular matrix, that is, that ( is unitarily
upper triangularizable. However, upper triangular matrices do not form a set of
canonical forms under unitary similarity. Indeed, the subject of canonical forms
for unitary similarity is rather complicated and we will not discuss it in this
book, but instead refer the reader to the survey article [28].

Reflections
The following defines a very special type of unitary operator.

244

Advanced Linear Algebra

Definition For a nonzero # Â = , the unique operator /# for which
/# # ~ c#Ã /# $ ~ $ for all $ Â Âº#Â»Â
is called a reflection or a Householder transformation.
It is easy to verify that
/# % ~ % c

Âº%Ã #Â»
#
Âº#Ã #Â»

Moreover, /# % ~ c% for % Â£  if and only if % ~ # for some  Â - and so
we can uniquely identify # by the behavior of the reflection on = .
If /# is a reflection and if we extend # to an ordered orthonormal basis 8 for = ,
then Â´/# Âµ8 is the matrix obtained from the identity matrix by replacing the upper
left entry by c,
v c
x
Â´/# Âµ8 ~ x
w

y
{
{


Ã†

z

Thus, a reflection is both unitary and Hermitian, that is,
/#i ~ /#c ~ /#
Given two nonzero vectors of equal length, there is precisely one reflection that
interchanges these vectors.
Theorem 10.16 Let #Ã $ Â = be distinct nonzero vectors of equal length. Then
/#c$ is the unique reflection sending # to $ and $ to #.
Proof. If )#) ~ )$), then Â²# c $Â³ Â Â²# b $Â³ and so
/#c$ Â²# c $Â³ ~ $ c #
/#c$ Â²# b $Â³ ~ # b $
from which it follows that /#c$ Â²#Â³ ~ $ and /#c$ Â²$Â³ ~ #. As to uniqueness,
suppose /% is a reflection for which /% Â²#Â³ ~ $. Since /%c ~ /% , we have
/% Â²$Â³ ~ # and so
/% Â²# c $Â³ ~ cÂ²# c $Â³
which implies that /% ~ /#c$ .
Reflections can be used to characterize unitary operators.
Theorem 10.17 Let = be a finite-dimensional inner product space. The
following are equivalent for an operator  Â BÂ²= Â³:

Structure Theory for Normal Operators

245

1)  is unitary/orthogonal
2)  is a product of reflections.
Proof. Since reflections are unitary/orthogonal and the product of unitary/
orthogonal operators is unitary, it follows that 2) implies 1). For the converse,
let  be unitary. Let 8 ~ Â²" Ã Ãƒ Ã " Â³ be an orthonormal basis for = . Then
/ " c" Â² " Â³ ~ "
and so if % ~  " c " then
Â²/%  Â³" ~ "
that is,  Â• /%  is the identity on Âº" Â». Suppose that we have found
reflections /%c Ã Ãƒ Ã /% for which c Â• /%c Ã„/%  is the identity on
Âº" Ã Ãƒ Ã "c Â». Then
/c " c" Â²c " Â³ ~ "
Moreover, we claim that Â²c " c " Â³ Â " for    , since
Âºc " c " Ã " Â» ~ ÂºÂ²/%c Ã„/%  Â³" Ã " Â»
~ Âº " Ã /% Ã„/%c " Â»
~ Âº "  Ã  "  Â»
~ Âº" Ã " Â»
~
Hence, if % ~ c " c " , then
Â²/% Ã„/%  Â³" ~ /% " ~ "
and so  Â• /% Ã„/%  is the identity on Âº" Ã Ãƒ Ã " Â». Thus, for  ~  we
have /% Ã„/%  ~  and so  ~ /% Ã„/% , as desired.

The Structure of Normal Operators
The following theorem includes the spectral theorems stated above for real and
complex normal operators, along with some further refinements related to selfadjoint and unitary/orthogonal operators.
Theorem 10.18 (The structure theorem for normal operators)
1) (Complex case) Let = be a finite-dimensional complex inner product
space.
a) The following are equivalent for  Â BÂ²= Â³:
i)  is normal
ii)  is unitarily diagonalizable
iii)  has an orthogonal spectral resolution
 ~    b Ã„ b    

246

Advanced Linear Algebra

b) Among the normal operators, the Hermitian operators are precisely
those for which all complex eigenvalues are real.
c) Among the normal operators, the unitary operators are precisely those
for which all complex eigenvalues have norm .
2) (Real case) Let = be a finite-dimensional real inner product space.
a)  Â BÂ²= Â³ is normal if and only if
= ~ ; p Ã„ p ; p > p Ã„ p >
where Â¸ Ã Ãƒ Ã  Â¹ is the spectrum of  and each > is a twodimensional indecomposable  -invariant subspace with an ordered
basis 8 for which

Â´ Âµ8 ~ > 


c
 ?

b) Among the real normal operators, the symmetric operators are those
for which there are no subspaces > in the decomposition of part 2a).
Hence, the following are equivalent for  Â BÂ²= Â³:
i)  is symmetric.
ii)  is orthogonally diagonalizable.
iii)  has the orthogonal spectral resolution
 ~   b Ã„ b  
c)

Among the real normal operators, the orthogonal operators are
precisely those for which the eigenvalues are equal to f and the
matrices Â´ Âµ8 described in part 2a) have rows (and columns) of norm
, that is,
Â´ Âµ8 ~ >

sin
cos

ccos
sin ?

for some Â s.
Proof. We have proved part 1a). As to part 1b), it is only necessary to look at a
diagonal matrix ( representing  . This matrix has the eigenvalues of  on its
main diagonal and so it is Hermitian if and only if the eigenvalues of  are real.
Similarly, ( is unitary if and only if the eigenvalues of  have absolute value
equal to .
We have proved part 2a). Parts 2b) and 2c) follow by looking at the matrix
( ~ Â´ Âµ8 where 8 ~ 8 . This matrix is symmetric if and only if ( is diagonal,
and ( is orthogonal if and only if  ~ f and the matrices Â´ Âµ8 have
orthonormal rows.

Matrix Versions
We can formulate matrix versions of the structure theorem for normal operators.

Structure Theory for Normal Operators

247

Theorem 10.19 (The structure theorem for normal matrices)
1) (Complex case)
a) A complex matrix ( is normal if and only if it is unitarily
diagonalizable, that is, if and only if there is a unitary matrix < for
which
< (< i ~ diagÂ² Ã Ãƒ Ã  Â³
b) A complex matrix ( is Hermitian if and only if 1a) holds, where all
eigenvalues  are real.
c) A complex matrix ( is unitary if and only if 1a) holds, where all
eigenvalues  have norm .
2) (Real case)
a) A real matrix ( is normal if and only if there is an orthogonal matrix
6 for which

6(6! ~ diag6 Ã Ãƒ Ã  Ã > 


c

ÃÃƒÃ> 
 ?


c
 ?7

b) A real matrix ( is symmetric if and only if it is orthogonally
diagonalizable, that is, if and only if there is an orthogonal matrix 6
for which
6(6! ~ diagÂ² Ã Ãƒ Ã  Â³
c)

A real matrix ( is orthogonal if and only if there is an orthogonal
matrix 6 for which
6(6!
~ diag6 Ã Ãƒ Ã  Ã >
for some

sin 
cos 

sin 
ccos 
ÃÃƒÃ>
sin  ?
cos 

ccos 
sin  ?7

 Ã Ãƒ Ã  Â s.

Functional Calculus
Let  be a normal operator on a finite-dimensional inner product space = and let
 have spectral resolution
 ~   b Ã„ b  
Since each  is idempotent, we have  ~  for all  Â‚ . The pairwise
orthogonality of the projections implies that
  ~ Â²  b Ã„ b   Â³ ~   b Ã„ b  
More generally, for any polynomial Â²%Â³ over - ,
Â² Â³ ~ Â² Â³ b Ã„ b Â² Â³
Note that a polynomial of degree  c  is uniquely determined by specifying an

248

Advanced Linear Algebra

arbitrary set of  of its values at the distinct points  Ã Ãƒ Ã  . This follows from
the Lagrange interpolation formula
c
v
% c  y
Â²%Â³ ~  Â² Â³ 
w Â£  c  z
~

Therefore, we can define a unique polynomial Â²%Â³ by specifying the values
Â² Â³, for  ~ Ã Ãƒ Ã  .
For example, for a given  Â  Â  , if  Â²%Â³ is a polynomial for which
 Â² Â³ ~ Ã
for  ~ Ã Ãƒ Ã  , then
 Â² Â³ ~ 
and so each projection  is a polynomial function of  . As another example, if
 is invertible and Â² Â³ ~ c , then
Â² Â³ ~ c  b Ã„ b c  ~  c
as can easily be verified by direct calculation. Finally, if Â² Â³ ~  , then since
each  is self-adjoint, we have
Â² Â³ ~   b Ã„ b   ~  i
and so  i is a polynomial in  .
We can extend this idea further by defining, for any function
 Â¢ Â¸ Ã Ãƒ Ã  Â¹ Â¦ the linear operator  Â² Â³ by
 Â² Â³ ~  Â² Â³ b Ã„ b  Â² Â³
For example, we may define j Ã  c Ã  and so on. Notice, however, that
since the spectral resolution of  is a finite sum, we gain nothing (but
convenience) by using functions other than polynomials, for we can always find
a polynomial Â²%Â³ for which Â² Â³ ~  Â² Â³ for  ~ Ã Ãƒ Ã  and so
 Â² Â³ ~ Â² Â³. The study of the properties of functions of an operator  is
referred to as the functional calculus of  .
According to the spectral theorem, if = is complex and  is normal, then  Â² Â³ is
a normal operator whose eigenvalues are  Â² Â³. Similarly, if = is real and  is
symmetric, then  Â² Â³ is symmetric, with eigenvalues  Â² Â³.

Structure Theory for Normal Operators

249

Commutativity
The functional calculus can be applied to the study of the commutativity
properties of operators. Here are two simple examples.
Theorem 10.20 Let = be a finite-dimensional complex inner product space.
For  Ã  Â BÂ²= Â³, we write  Â©  to denote the fact that  and  commute. Let
 and  have spectral resolutions
 ~   b Ã„ b  
 ~    b Ã„ b    
Then
1) For any  Â BÂ²= Â³,
Â©

Â¯

 Â©  for all 

 Â©

Â¯

 Â©  , for all Ã 

2)

3) If  Â¢ Â¸ Ã Ãƒ Ã  Â¹ Â¦ - and Â¢ Â¸ Ã Ãƒ Ã  Â¹ Â¦ - are injective functions,
then
 Â² Â³ Â© Â²Â³

Â¯

 Â©

Proof. For 1), if  Â©  for all , then  Â©  and the converse follows from the
fact that  is a polynomial in  . Part 2) is similar. For part 3),  Â©  clearly
implies  Â² Â³ Â© Â²Â³. For the converse, let $ ~ Â¸ Ã Ãƒ Ã  Â¹. Since  is
injective, the inverse function  c Â¢  Â²$Â³ Â¦ $ is well-defined and
 c Â² Â² Â³Â³ ~  . Thus,  is a function of  Â² Â³. Similarly,  is a function of Â²Â³.
It follows that  Â² Â³ Â© Â²Â³ implies  Â©  .
Theorem 10.21 Let  and  be normal operators on a finite-dimensional
complex inner product space = . Then  and  commute if and only if they have
the form
 ~ Â²Â² Ã Â³Â³
 ~ Â²Â² Ã Â³Â³
where Â²%Â³Ã Â²%Â³ and Â²%Ã &Â³ are polynomials.
Proof. If  and  are polynomials in ~ Â² Ã Â³, then they clearly commute.
For the converse, suppose that  ~  and let
 ~   b Ã„ b  
and
 ~    b Ã„ b    
be the orthogonal spectral resolutions of  and .

250

Advanced Linear Algebra

Then Theorem 10.20 implies that   ~   . Hence,
   ~ Â²  b Ã„ b   Â³ Â²  b Ã„ b   Â³
~ Â²  b Ã„ b   Â³Â²  b Ã„ b   Â³
~     
Ã

It follows that for any polynomial Â²%Ã &Â³ in two variables,
Â² Ã Â³ ~ Â² Ã  Â³ 
Ã

So if we choose Â²%Ã &Â³ with the property that Ã ~ Â² Ã  Â³ are distinct, then
Â² Ã Â³ ~ Ã  
Ã

and we can also choose Â²%Â³ and Â²%Â³ so that Â²Ã Â³ ~  for all  and
Â²Ã Â³ ~  for all . Then
Â²Â² Ã Â³Â³ ~ Â²Ã Â³  ~   
Ã

Ã

~ 8  98 9 ~   ~ 






and similarly, Â²Â² Ã Â³Â³ ~  .

Positive Operators
One of the most important cases of the functional calculus is  Â²%Â³ ~ j%.
Recall that the quadratic form associated with a linear operator  is
8 Â²#Â³ ~ Âº #Ã #Â»
Definition A self-adjoint linear operator  Â BÂ²= Â³ is
1) positive if 8 Â²#Â³ Â‚  for all # Â =
2) positive definite if 8 Â²#Â³ Â€  for all # Â£ .
Theorem 10.22 A self-adjoint operator  on a finite-dimensional inner product
space is
1) positive if and only if all of its eigenvalues are nonnegative
2) positive definite if and only if all of its eigenvalues are positive.
Proof. If 8 Â²#Â³ Â‚  and  # ~ #, then
 Â Âº #Ã #Â» ~ Âº#Ã #Â»

Structure Theory for Normal Operators

251

and so  Â‚ . Conversely, if all eigenvalues of  are nonnegative, then
 ~   b Ã„ b   Ã  Â‚ 
and since  ~  b Ã„ b  ,
Âº #Ã #Â» ~  Âº #Ã  #Â» ~  ) #) Â‚ 
Ã



and so  is positive. Part 2) is proved similarly.
If  is a positive operator, with spectral resolution
 ~   b Ã„ b   Ã  Â‚ 
then we may take the positive square root of  ,
j ~ j  b Ã„ b j 
where j is the nonnegative square root of  . It is clear that
Â²j Â³ ~ 
and it is not hard to see that j is the only positive operator whose square is  .
In other words, every positive operator has a unique positive square root.
Conversely, if  has a positive square root, that is, if  ~  , for some positive
operator , then  is positive. Hence, an operator  is positive if and only if it
has a positive square root.
If  is positive, then j is self-adjoint and so
Â²j Â³i j ~ 
Conversely, if  ~ i  for some operator , then  is positive, since it is clearly
self-adjoint and
Âº #Ã #Â» ~ Âºi #Ã #Â» ~ Âº#Ã #Â» Â‚ 
Thus,  is positive if and only if it has the form  ~ i  for some operator  .
(A complex number ' is nonnegative if and only if has the form ' ~ $$ for
some complex number $.)
Theorem 10.23 Let  Â BÂ²= Â³.
1)  is positive if and only if it has a positive square root.
2)  is positive if and only if it has the form  ~ i  for some operator  .
Here is an application of square roots.
Theorem 10.24 If  and  are positive operators and  ~  , then  is
positive.

252

Advanced Linear Algebra

Proof. Since  is a positive operator, it has a positive square root j , which is
a polynomial in  . A similar statement holds for  . Therefore, since  and 
commute, so do j and j. Hence,
Â²j jÂ³ ~ Â²j Â³ Â²jÂ³ ~ 
Since j and j are self-adjoint and commute, their product is self-adjoint
and so  is positive.

The Polar Decomposition of an Operator
It is well known that any nonzero complex number ' can be written in the polar
form ' ~  , where  is a positive number and is real. We can do the same
for any nonzero linear operator  on a finite-dimensional complex inner product
space.
Theorem 10.25 Let  be a nonzero linear operator on a finite-dimensional
complex inner product space = .
1) There exist a positive operator  and a unitary operator  for which
 ~ . Moreover,  is unique and if  is invertible, then  is also unique.
2) Similarly, there exist a positive operator  and a unitary operator  for
which  ~ . Moreover,  is unique and if  is invertible, then  is also
unique.
Proof. Let us suppose for a moment that  ~ . Then
 i ~ Â²Â³i ~ i  i ~  c
and so
 i  ~  c  ~ 
Also, if # Â = , then
 # ~  Â²#Â³
These equations give us a clue as to how to define  and  .
Let us define  to be the unique positive square root of the positive operator
 i  . Then
)#) ~ Âº#Ã #Â» ~ Âº #Ã #Â» ~ Âº i  #Ã #Â» ~ ) #)

(10.2)

Define  on imÂ²Â³ by
 Â²#Â³ ~  #
for all # Â = . Equation (10.2) shows that % ~ & implies that  % ~  & and so
this definition of  on imÂ²Â³ is well-defined.
Moreover,  is an isometry on imÂ²Â³, since (10.2) gives

Structure Theory for Normal Operators

253

) Â²#Â³) ~ ) #) ~ )#)
Thus, if 8 ~ Â¸ Ã Ãƒ Ã  Â¹ is an orthonormal basis for imÂ²Â³, then
8 ~ Â¸  Ã Ãƒ Ã   Â¹ is an orthonormal basis for  Â²imÂ²Â³Â³ ~ imÂ² Â³. Finally, we
may extend both orthonormal bases to orthonormal bases for = and then extend
the definition of  to an isometry on = for which  ~ .
As for the uniqueness, we have seen that  must satisfy  ~  i  and since 
has a unique positive square root, we deduce that  is uniquely defined. Finally,
if  is invertible, then so is  since kerÂ²Â³ Â‹ kerÂ² Â³. Hence,  ~ c is
uniquely determined by  .
Part 2) can be proved by applying the previous theorem to the map  i , to get
 ~ Â² i Â³i ~ Â²Â³i ~  c ~ 
where  is unitary.
We leave it as an exercise to show that any unitary operator  has the form
 ~  , where  is a self-adjoint operator. This gives the following corollary.
Corollary 10.26 (Polar decomposition) Let  be a nonzero linear operator on
a finite-dimensional complex inner product space. Then there is a positive
operator  and a self-adjoint operator  for which  has the polar
decomposition
 ~ 
Moreover,  is unique and if  is invertible, then  is also unique.
Normal operators can be characterized using the polar decomposition.
Theorem 10.27 Let  ~  be a polar decomposition of a nonzero linear
operator  . Then  is normal if and only if  ~ .
Proof. Since
 i ~  c  ~ 
and
 i  ~ c  ~ c  
we see that  is normal if and only if
c   ~ 

254

Advanced Linear Algebra

or equivalently,
  ~  
Now,  is a polynomial in  and  is a polynomial in  and so this holds if
and only if  ~ .

Exercises
1.
2.

Let  Â BÂ²< Ã = Â³. If  is surjective, find a formula for the right inverse of 
in terms of  i . If  is injective, find a formula for a left inverse of  in terms
of  i . Hint: Consider  i and  i  .
Let  Â BÂ²= Â³ where = is a complex vector space and let
 ~



Â² b  i Â³ and  ~ Â² c  i Â³



Show that  and  are self-adjoint and that
 ~  b  and  i ~  c 
What can you say about the uniqueness of these representations of  and
 i?
3. Prove that all of the roots of the characteristic polynomial of a skewHermitian matrix are pure imaginary.
4. Give an example of a normal operator that is neither self-adjoint nor
unitary.
5. Prove that if ) #) ~ ) i Â²#Â³) for all # Â = , where = is complex, then  is
normal.
6. Let  be a normal operator on a complex finite-dimensional inner product
space = or a self-adjoint operator on a real finite-dimensional inner product
space.
a) Show that  i ~ Â² Â³, for some polynomial Â²%Â³ Â dÂ´%Âµ.
b) Show that for any  Â BÂ²= Â³,  ~  implies  i ~  i  . In other
words,  i commutes with all operators that commute with  .
7. Show that a linear operator  on a finite-dimensional complex inner product
space = is normal if and only if whenever : is an invariant subspace under
, so is : Â .
8. Let = be a finite-dimensional inner product space and let  be a normal
operator on = .
a) Prove that if  is idempotent, then it is also self-adjoint.
b) Prove that if  is nilpotent, then  ~ .
c) Prove that if   ~   , then  is idempotent.
9. Show that if  is a normal operator on a finite-dimensional complex inner
product space, then the algebraic multiplicity is equal to the geometric
multiplicity for all eigenvalues of  .
10. Show that two orthogonal projections  and  are orthogonal to each other
if and only if imÂ²Â³ Â imÂ²Â³.

Structure Theory for Normal Operators

255

11. Let  be a normal operator and let  be any operator on = . If the
eigenspaces of  are  -invariant, show that  and  commute.
12. Prove that if  and  are normal operators on a finite-dimensional complex
inner product space and if  ~  for some operator then  i ~ i .
13. Prove that if two normal  d  complex matrices are similar, then they are
unitarily similar, that is, similar via a unitary matrix.
14. If  is a unitary operator on a complex inner product space, show that there
exists a self-adjoint operator  for which  ~  .
15. Show that a positive operator has a unique positive square root.
16. Prove that if  has a square root, that is, if  ~  , for some positive
operator , then  is positive.
17. Prove that if  Â  (that is,  c  is positive) and if is a positive operator
that commutes with both  and  then  Â  .
18. Using the 89 factorization, prove the following result, known as the
Cholsky decomposition. An invertible linear operator  Â BÂ²= Â³ is positive
if and only if it has the form  ~ i  where  is upper triangularizable.
Moreover,  can be chosen with positive eigenvalues, in which case the
factorization is unique.
19. Does every self-adjoint operator on a finite-dimensional real inner product
space have a square root?
20. Let  be a linear operator on d and let  Ã Ãƒ Ã  be the eigenvalues of  ,
each one written a number of times equal to its algebraic multiplicity. Show
that
( ( Â trÂ² i  Â³


where tr is the trace. Show also that equality holds if and only if  is
normal.
21. If  Â BÂ²= Â³ where = is a real inner product space, show that the Hilbert
space adjoint satisfies Â² i Â³d ~ Â² d Â³i .

Part IIâ€”Topics

Chapter 11

Metric Vector Spaces: The Theory of
Bilinear Forms

In this chapter, we study vector spaces over arbitrary fields that have a bilinear
form defined on them.
Unless otherwise mentioned, all vector spaces are assumed to be finitedimensional. The symbol - denotes an arbitrary field and - denotes a finite
field of size  .

Symmetric, Skew-Symmetric and Alternate Forms
We begin with the basic definition.
Definition Let = be a vector space over - . A mapping ÂºÃ Â»Â¢ = d = Â¦ - is
called a bilinear form if it is linear in each coordinate, that is, if
Âº% b  &Ã 'Â» ~ Âº%Ã 'Â» b  Âº&Ã 'Â»
and
Âº'Ã % b  &Â» ~ Âº'Ã %Â» b  Âº'Ã &Â»
A bilinear form is
1) symmetric if
Âº%Ã &Â» ~ Âº&Ã %Â»
for all %Ã & Â = .
2) skew-symmetric (or antisymmetric) if
Âº%Ã &Â» ~ cÂº&Ã %Â»
for all %Ã & Â = .

260

Advanced Linear Algebra

3) alternate (or alternating) if
Âº%Ã %Â» ~ 
for all % Â = .
A bilinear form that is either symmetric, skew-symmetric, or alternate is
referred to as an inner product and a pair Â²= Ã ÂºÃ Â»Â³, where = is a vector space
and ÂºÃ Â» is an inner product on = , is called a metric vector space or inner
product space. As usual, we will refer to = as a metric vector space when the
form is understood.
4) A metric vector space = with a symmetric form is called an orthogonal
geometry over - .
5) A metric vector space = with an alternate form is called a symplectic
geometry over - .
The term symplectic, from the Greek for â€œintertwined,â€ was introduced in 1939
by the famous mathematician Hermann Weyl in his book The Classical Groups,
as a substitute for the term complex. According to the dictionary, symplectic
means â€œrelating to or being an intergrowth of two different minerals.â€ An
example is ophicalcite, which is marble spotted with green serpentine.
Example 11.1 Minkowski space 44 is the four-dimensional real orthogonal
geometry s with inner product defined by
Âº Ã  Â» ~ Âº Ã  Â» ~ Âº3 Ã 3 Â» ~ 
Âº4 Ã 4 Â» ~ c
Âº Ã  Â» ~  for  Â£ 
where  Ã Ãƒ Ã 4 is the standard basis for s .
As is traditional, when the inner product is understood, we will use the phrase
â€œlet = be a metric vector space.â€
The real inner products discussed in Chapter 9 are inner products in the present
sense and have the additional property of being positive definiteâ€”a notion that
does not even make sense if the base field is not ordered. Thus, a real inner
product space is an orthogonal geometry. On the other hand, the complex inner
products of Chapter 9, being sesquilinear, are not inner products in the present
sense. For this reason, we use the term metric vector space in this chapter, rather
than inner product space.
If : is a vector subspace of a metric vector space = , then : inherits the metric
structure from = . With this structure, we refer to : as a subspace of = .
The concepts of being symmetric, skew-symmetric and alternate are not
independent. However, their relationship depends on the characteristic of the
base field - , as do many other properties of metric vector spaces. In fact, the

Metric Vector Spaces: The Theory of Bilinear Forms

261

next theorem tells us that we do not need to consider skew-symmetric forms per
se, since skew-symmetry is always equivalent to either symmetry or
alternateness.
Theorem 11.1 Let = be a vector space over a field - .
1) If charÂ²- Â³ ~ , then
alternate Â¬ symmetric Â¯ skew-symmetric
2) If charÂ²- Â³ Â£ , then
alternate Â¯ skew-symmetric
Also, the only form that is both alternate and symmetric is the zero form:
Âº%Ã &Â» ~  for all %Ã & Â = .
Proof. First note that for an alternating form over any base field,
 ~ Âº% b &Ã % b &Â» ~ Âº%Ã &Â» b Âº&Ã %Â»
and so
Âº%Ã &Â» ~ cÂº&Ã %Â»
which shows that the form is skew-symmetric. Thus, alternate always implies
skew-symmetric.
If charÂ²- Â³ ~ , then c ~  and so the definitions of symmetric and skewsymmetric are equivalent, which proves 1). If charÂ²- Â³ Â£  and the form is
skew-symmetric, then for any % Â = , we have Âº%Ã %Â» ~ cÂº%Ã %Â» or Âº%Ã %Â» ~ ,
which implies that Âº%Ã %Â» ~ . Hence, the form is alternate. Finally, if the form
is alternate and symmetric, then it is also skew-symmetric and so
Âº"Ã #Â» ~ cÂº"Ã #Â» for all "Ã # Â = , that is, Âº"Ã #Â» ~  for all "Ã # Â = .
Example 11.2 The standard inner product on = Â²Ã Â³, defined by
Â²% Ã Ãƒ Ã % Â³ h Â²& Ã Ãƒ Ã & Â³ ~ % & b Ã„ b % &
is symmetric, but not alternate, since
Â²Ã Ã Ãƒ Ã Â³ h Â²Ã Ã Ãƒ Ã Â³ ~  Â£ 

The Matrix of a Bilinear Form
If 8 ~ Â² Ã Ãƒ Ã  Â³ is an ordered basis for a metric vector space = , then a
bilinear form is completely determined by the  d  matrix of values
48 ~ Â²Ã Â³ ~ Â²Âº Ã  Â»Â³
This is referred to as the matrix of the form (or the matrix of = ) with respect to
the ordered basis 8 . Moreover, any  d  matrix over - is the matrix of some
bilinear form on = .

262

Advanced Linear Algebra

Note that if % ~ '  then
48 Â´%Âµ8 ~

v Âº Ã %Â» y
Ã…
w Âº Ã %Â» z

and
Â´%Âµ!8 48 ~  Âº%Ã  Â» Ã„ Âº%Ã  Â» 
It follows that if & ~    , then
Â´%Âµ!8 48 Â´&Âµ8 ~  Âº%Ã  Â» Ã„ Âº%Ã  Â» 

v y
Ã… ~ Âº%Ã &Â»
w z

and this uniquely defines the matrix 48 , that is, if Â´%Âµ!8 (Â´&Âµ8 ~ Âº%Ã &Â» for all
%Ã & Â = , then ( ~ 48 .
A matrix is alternate if it is skew-symmetric and has 's on the main diagonal.
Thus, we can say that a form is symmetric (skew-symmetric, alternate) if and
only if the matrix 48 is symmetric (skew-symmetric, alternate).
Now let us see how the matrix of a form behaves with respect to a change of
basis. Let 9 ~ Â² Ã Ãƒ Ã  Â³ be an ordered basis for = . Recall from Chapter 2 that
the change of basis matrix 49Ã8 , whose th column is Â´ Âµ8 , satisfies
Â´#Âµ8 ~ 49Ã8 Â´#Âµ9
Hence,
Âº%Ã &Â» ~ Â´%Âµ!8 48 Â´&Âµ8
~ Â²Â´%Âµ!9 49!Ã8 Â³48 Â²49Ã8 Â´&Âµ9 Â³
~ Â´%Âµ!9 Â²49!Ã8 48 49Ã8 Â³Â´&Âµ9
and so
49 ~ 49!Ã8 48 49Ã8
This prompts the following definition.
Definition Two matrices (Ã ) Â C Â²- Â³ are congruent if there exists an
invertible matrix 7 for which
( ~ 7 ! )7
The equivalence classes under congruence are called congruence classes.

Metric Vector Spaces: The Theory of Bilinear Forms

263

Thus, if two matrices represent the same bilinear form on = , they must be
congruent. Conversely, if ) ~ 48 represents a bilinear form on = and
( ~ 7 ! )7
where 7 is invertible, then there is an ordered basis 9 for = for which
7 ~ 49 Ã 8
and so
( ~ 49!Ã8 48 49Ã8
Thus, ( ~ 49 represents the same form with respect to 9.
Theorem 11.2 Let 8 ~ Â² Ã Ãƒ Ã  Â³ be an ordered basis for an inner product
space = , with matrix
48 ~ Â²Âº Ã  Â»Â³
1) The form can be recovered from the matrix by the formula
Âº%Ã &Â» ~ Â´%Âµ!8 48 Â´&Âµ8
2) If 9 ~ Â² Ã Ãƒ Ã  Â³ is also an ordered basis for = , then
49 ~ 49!Ã8 48 49Ã8
where 49Ã8 is the change of basis matrix from 9 to 8 .
3) Two matrices ( and ) represent the same bilinear form on a vector space
= if and only if they are congruent, in which case they represent the same
set of bilinear forms on = .
In view of the fact that congruent matrices have the same rank, we may define
the rank of a bilinear form (or of = ) to be the rank of any matrix that represents
that form.

The Discriminant of a Form
If ( and ) are congruent matrices, then
detÂ²(Â³ ~ detÂ²7 ! )7 Â³ ~ detÂ²7 Â³ detÂ²)Â³
and so detÂ²(Â³ and detÂ²)Â³ differ by a square factor. The discriminant " of a
bilinear form is the set of determinants of all of the matrices that represent the
form. Thus, if 8 is an ordered basis for = , then
" ~ -  detÂ²48 Â³ ~ Â¸ detÂ²48 Â³ Â“  Â£  Â - Â¹

