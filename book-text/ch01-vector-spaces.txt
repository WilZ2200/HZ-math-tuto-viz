Vector Spaces

37

Â² Ã Ãƒ Ã  Â³ ~ Â² Ã Ãƒ Ã  Â³
When convenient, we will also write the elements of -  in column form.
When - is a finite field - with  elements, we write = Â²Ã Â³ for - .
4) Many sequence spaces are vector spaces. The set SeqÂ²- Â³ of all infinite
sequences with members from a field - is a vector space under the
componentwise operations
Â²  Â³ b Â²! Â³ ~ Â²  b ! Â³
and
Â²  Â³ ~ Â²  Â³
In a similar way, the set  of all sequences of complex numbers that
converge to  is a vector space, as is the set MB of all bounded complex
sequences. Also, if  is a positive integer, then the set M of all complex
sequences Â²  Â³ for which
B

 (  (  B
~

is a vector space under componentwise operations. To see that addition is a
binary operation on M , one verifies Minkowski's inequality
B


8 (  b ! ( 9
~

Â°

B

Â 8  (  ( 9
~

Â°

B

Â°

b 8 (! ( 9
~

which we will not do here.

Subspaces
Most algebraic structures contain substructures, and vector spaces are no
exception.
Definition A subspace of a vector space = is a subset : of = that is a vector
space in its own right under the operations obtained by restricting the
operations of = to : . We use the notation : Â = to indicate that : is a
subspace of = and :  = to indicate that : is a proper subspace of = , that is,
: Â = but : Â£ = . The zero subspace of = is Â¸Â¹.
Since many of the properties of addition and scalar multiplication hold a fortiori
in a nonempty subset : , we can establish that : is a subspace merely by
checking that : is closed under the operations of = .
Theorem 1.1 A nonempty subset : of a vector space = is a subspace of = if
and only if : is closed under addition and scalar multiplication or, equivalently,

38

Advanced Linear Algebra

: is closed under linear combinations, that is,
Ã  Â - Ã "Ã # Â : Â¬ " b # Â :
Example 1.2 Consider the vector space = Â²Ã Â³ of all binary -tuples, that is,
-tuples of 's and 's. The weight M Â²#Â³ of a vector # Â = Â²Ã Â³ is the number
of nonzero coordinates in #. For instance, M Â²Â³ ~ . Let , be the set of
all vectors in = of even weight. Then , is a subspace of = Â²Ã Â³.
To see this, note that
M Â²" b #Â³ ~ M Â²"Â³ b M Â²#Â³ c M Â²" q #Â³
where " q # is the vector in = Â²Ã Â³ whose th component is the product of the
th components of " and #, that is,
Â²" q #Â³ ~ " h #
Hence, if M Â²"Â³ and M Â²#Â³ are both even, so is M Â²" b #Â³. Finally, scalar
multiplication over - is trivial and so , is a subspace of = Â²Ã Â³, known as
the even weight subspace of = Â²Ã Â³.
Example 1.3 Any subspace of the vector space = Â²Ã Â³ is called a linear code.
Linear codes are among the most important and most studied types of codes,
because their structure allows for efficient encoding and decoding of
information.

The Lattice of Subspaces
The set IÂ²= Â³ of all subspaces of a vector space = is partially ordered by set
inclusion. The zero subspace Â¸Â¹ is the smallest element in I Â²= Â³ and the entire
space = is the largest element.
If :Ã ; Â I Â²= Â³, then : q ; is the largest subspace of = that is contained in
both : and ; . In terms of set inclusion, : q ; is the greatest lower bound of :
and ; :
: q ; ~ glbÂ¸:Ã ; Â¹
Similarly, if Â¸: Â“  Â 2Â¹ is any collection of subspaces of = , then their
intersection is the greatest lower bound of the subspaces:
: ~ glbÂ¸: Â“  Â 2Â¹
Â2

On the other hand, if :Ã ; Â I Â²= Â³ (and - is infinite), then : r ; Â I Â²= Â³ if
and only if : Â‹ ; or ; Â‹ : . Thus, the union of two subspaces is never a
subspace in any â€œinterestingâ€ case. We also have the following.

Vector Spaces

39

Theorem 1.2 A nontrivial vector space = over an infinite field - is not the
union of a finite number of proper subspaces.
Proof. Suppose that = ~ : r Ã„ r : , where we may assume that
: Â‹
\ : r Ã„ r :
Let $ Â : Â± Â²: r Ã„ r : Â³ and let # Â¤ : . Consider the infinite set
( ~ Â¸$ b # Â“  Â - Â¹
which is the â€œlineâ€ through #, parallel to $. We want to show that each :
contains at most one vector from the infinite set (, which is contrary to the fact
that = ~ : r Ã„ r : . This will prove the theorem.
If $ b # Â : for  Â£ , then $ Â : implies # Â : , contrary to assumption.
Next, suppose that  $ b # Â : and  $ b # Â : , for  Â‚ , where  Â£  .
Then
: Â Â² $ b #Â³ c Â² $ b #Â³ ~ Â² c  Â³$
and so $ Â : , which is also contrary to assumption.
To determine the smallest subspace of = containing the subspaces : and ; , we
make the following definition.
Definition Let : and ; be subspaces of = . The sum : b ; is defined by
: b ; ~ Â¸" b # Â“ " Â :Ã # Â ; Â¹
More generally, the sum of any collection Â¸: Â“  Â 2Â¹ of subspaces is the set
of all finite sums of vectors from the union : :
: ~ H  b Ã„ b  c  Â  : I
Â2

Â2

It is not hard to show that the sum of any collection of subspaces of = is a
subspace of = and that the sum is the least upper bound under set inclusion:
: b ; ~ lubÂ¸:Ã ; Â¹
More generally,
: ~ lubÂ¸: Â“  Â 2Â¹
Â2

If a partially ordered set 7 has the property that every pair of elements has a
least upper bound and greatest lower bound, then 7 is called a lattice. If 7 has
a smallest element and a largest element and has the property that every
collection of elements has a least upper bound and greatest lower bound, then 7

40

Advanced Linear Algebra

is called a complete lattice. The least upper bound of a collection is also called
the join of the collection and the greatest lower bound is called the meet.
Theorem 1.3 The set IÂ²= Â³ of all subspaces of a vector space = is a complete
lattice under set inclusion, with smallest element Â¸Â¹, largest element = , meet
glbÂ¸: Â“  Â 2Â¹ ~

:
Â2

and join
lubÂ¸: Â“  Â 2Â¹ ~  :
Â2

Direct Sums
As we will see, there are many ways to construct new vector spaces from old
ones.

External Direct Sums
Definition Let = Ã Ãƒ Ã = be vector spaces over a field - . The external direct
sum of = Ã Ãƒ Ã = , denoted by
= ~ = ^ Ã„ ^ =
is the vector space = whose elements are ordered -tuples:
= ~ Â¸Â²# Ã Ãƒ Ã # Â³ Â“ # Â = Ã  ~ Ã Ãƒ Ã Â¹
with componentwise operations
Â²" Ã Ãƒ Ã " Â³ b Â²# Ã Ãƒ Ã # Â³ ~ Â²" b # Ã Ãƒ Ã " b #Â³
and
Â²# Ã Ãƒ Ã # Â³ ~ Â²# Ã Ãƒ Ã # Â³
for all  Â - .
Example 1.4 The vector space -  is the external direct sum of  copies of - ,
that is,
- ~ - ^ Ã„ ^ where there are  summands on the right-hand side.
This construction can be generalized to any collection of vector spaces by
generalizing the idea that an ordered -tuple Â²# Ã Ãƒ Ã # Â³ is just a function
 Â¢ Â¸Ã Ãƒ Ã Â¹ Â¦ = from the index set Â¸Ã Ãƒ Ã Â¹ to the union of the spaces
with the property that  Â²Â³ Â = .

Vector Spaces

41

Definition Let < ~ Â¸= Â“  Â 2Â¹ be any family of vector spaces over - . The
direct product of < is the vector space
 = ~ H Â¢ 2 Â¦  = d  Â²Â³ Â = I

Â2

Â2

thought of as a subspace of the vector space of all functions from 2 to = .
It will prove more useful to restrict the set of functions to those with finite
support.
Definition Let < ~ Â¸= Â“  Â 2Â¹ be a family of vector spaces over - . The
support of a function  Â¢ 2 Â¦ = is the set
suppÂ² Â³ ~ Â¸ Â 2 Â“  Â²Â³ Â£ Â¹
Thus, a function  has finite support if  Â²Â³ ~  for all but a finite number of
 Â 2. The external direct sum of the family < is the vector space
ext

 = ~ H Â¢ 2 Â¦  = d  Â²Â³ Â = ,  has finite supportI
Â2

Â2

thought of as a subspace of the vector space of all functions from 2 to = .
An important special case occurs when = ~ = for all  Â 2 . If we let = 2
denote the set of all functions from 2 to = and Â²= 2 Â³ denote the set of all
functions in = 2 that have finite support, then
ext

 = ~ = 2 and  = ~ Â²= 2 Â³
Â2

Â2

Note that the direct product and the external direct sum are the same for a finite
family of vector spaces.

Internal Direct Sums
An internal version of the direct sum construction is often more relevant.
Definition A vector space = is the (internal) direct sum of a family
< ~ Â¸: Â“  Â 0Â¹ of subspaces of = , written
= ~ <

or = ~ :
Â0

if the following hold:

42

Advanced Linear Algebra

1) (Join of the family) = is the sum (join) of the family < :
= ~ :
Â0

2) (Independence of the family) For each  Â 0 ,
: q

p

:

q Â£

s
t

~ Â¸Â¹

In this case, each : is called a direct summand of = . If < ~ Â¸: Ã Ãƒ Ã : Â¹ is a
finite family, the direct sum is often written
= ~ : l Ã„ l :
Finally, if = ~ : l ; , then ; is called a complement of : in = .
Note that the condition in part 2) of the previous definition is stronger than
saying simply that the members of < are pairwise disjoint:
: q : ~ J
for all  Â£  Â 0 .
A word of caution is in order here: If : and ; are subspaces of = , then we may
always say that the sum : b ; exists. However, to say that the direct sum of :
and ; exists or to write : l ; is to imply that : q ; ~ Â¸Â¹. Thus, while the
sum of two subspaces always exists, the direct sum of two subspaces does not
always exist. Similar statements apply to families of subspaces of = .
The reader will be asked in a later chapter to show that the concepts of internal
and external direct sum are essentially equivalent (isomorphic). For this reason,
the term â€œdirect sumâ€ is often used without qualification.
Once we have discussed the concept of a basis, the following theorem can be
easily proved.
Theorem 1.4 Any subspace of a vector space has a complement, that is, if : is a
subspace of = , then there exists a subspace ; for which = ~ : l ; .
It should be emphasized that a subspace generally has many complements
(although they are isomorphic). The reader can easily find examples of this in
s .
We can characterize the uniqueness part of the definition of direct sum in other
useful ways. First a remark. If : and ; are distinct subspaces of = and if
%Ã & Â : q ; , then the sum % b & can be thought of as a sum of vectors from the

Vector Spaces

43

same subspace (say : ) or from different subspacesâ€”one from : and one from
; . When we say that a vector # cannot be written as a sum of vectors from the
distinct subspaces : and ; , we mean that # cannot be written as a sum % b &
where % and & can be interpreted as coming from different subspaces, even if
they can also be interpreted as coming from the same subspace. Thus, if
%Ã & Â : q ; , then # ~ % b & does express # as a sum of vectors from distinct
subspaces.
Theorem 1.5 Let < ~ Â¸: Â“  Â 0Â¹ be a family of distinct subspaces of = . The
following are equivalent:
1) (Independence of the family) For each  Â 0 ,
: q

p

:

q Â£

s
t

~ Â¸Â¹

2) (Uniqueness of expression for ) The zero vector  cannot be written as a
sum of nonzero vectors from distinct subspaces of < .
3) (Uniqueness of expression) Every nonzero # Â = has a unique, except for
order of terms, expression as a sum
#~

bÃ„b 

of nonzero vectors from distinct subspaces in < .
Hence, a sum
= ~ :
Â0

is direct if and only if any one of 1)â€“3) holds.
Proof. Suppose that 2) fails, that is,
~
where the nonzero

 b Ã„ b 

 's are from distinct subspaces : . Then  Â€  and so

c  ~

 b Ã„ b 

which violates 1). Hence, 1) implies 2). If 2) holds and
#~

bÃ„b 

where the terms are nonzero and the
similarily for the ! 's, then
~

and # ~ ! b Ã„ b !
 's belong to distinct subspaces in < and

 b Ã„ b  c ! c Ã„ c ! 

By collecting terms from the same subspaces, we may write
 ~ Â²  c ! Â³ b Ã„ b Â²  c ! Â³ b

b b Ã„ b

 c !b c Ã„ c !

44

Advanced Linear Algebra

Then 2) implies that  ~  ~  and
implies 3).

" ~ !" for all " ~ Ã Ãƒ Ã  . Hence, 2)

Finally, suppose that 3) holds. If
 Â£ # Â : q
then # ~

:

q Â£

s
t

 Â : and
 ~

where

p

 b Ã„ b 

 Â : are nonzero. But this violates 3).

Example 1.5 Any matrix ( Â C can be written in the form
(~



Â²( b (! Â³ b Â²( c (! Â³ ~ ) b *



(1.1)

where (! is the transpose of (. It is easy to verify that ) is symmetric and * is
skew-symmetric and so (1.1) is a decomposition of ( as the sum of a symmetric
matrix and a skew-symmetric matrix.
Since the sets Sym and SkewSym of all symmetric and skew-symmetric
matrices in C are subspaces of C , we have
C ~ Sym b SkewSym
Furthermore, if : b ; ~ : Z b ; Z , where : and : Z are symmetric and ; and ; Z
are skew-symmetric, then the matrix
< ~ : c :Z ~ ; Z c ;
is both symmetric and skew-symmetric. Hence, provided that charÂ²- Â³ Â£ , we
must have < ~  and so : ~ : Z and ; ~ ; Z . Thus,
C ~ Sym l SkewSym

Spanning Sets and Linear Independence
A set of vectors spans a vector space if every vector can be written as a linear
combination of some of the vectors in that set. Here is the formal definition.
Definition The subspace spanned (or subspace generated) by a nonempty set
: of vectors in = is the set of all linear combinations of vectors from : :
Âº:Â» ~ spanÂ²:Â³ ~ Â¸ # b Ã„ b  # Â“  Â - Ã # Â :Â¹

Vector Spaces

45

When : ~ Â¸# Ã Ãƒ Ã # Â¹ is a finite set, we use the notation Âº# Ã Ãƒ Ã # Â» or
spanÂ²# Ã Ãƒ Ã # Â³. A set : of vectors in = is said to span = , or generate = , if
= ~ spanÂ²:Â³.
It is clear that any superset of a spanning set is also a spanning set. Note also
that all vector spaces have spanning sets, since = spans itself.

Linear Independence
Linear independence is a fundamental concept.
Definition Let = be a vector space. A nonempty set : of vectors in = is
linearly independent if for any distinct vectors  Ã Ãƒ Ã  in : ,
  b Ã„ b   ~ 

Â¬

 ~ 

for all 

In words, : is linearly independent if the only linear combination of vectors
from : that is equal to  is the trivial linear combination, all of whose
coefficients are . If : is not linearly independent, it is said to be linearly
dependent.
It is immediate that a linearly independent set of vectors cannot contain the zero
vector, since then  h  ~  violates the condition of linear independence.
Another way to phrase the definition of linear independence is to say that : is
linearly independent if the zero vector has an â€œas unique as possibleâ€ expression
as a linear combination of vectors from : . We can never prevent the zero vector
from being written in the form  ~   b Ã„ b   , but we can prevent  from
being written in any other way as a linear combination of the vectors in : .
For the introspective reader, the expression  ~  b Â² c   Â³ has two
interpretations. One is  ~   b   where  ~  and  ~ c, but this does
not involve distinct vectors so is not relevant to the question of linear
independence. The other interpretation is  ~  b ! where ! ~ c  Â£ 
(assuming that  Â£ ). Thus, if : is linearly independent, then : cannot
contain both  and c  .
Definition Let : be a nonempty set of vectors in = . To say that a nonzero
vector # Â = is an essentially unique linear combination of the vectors in : is
to say that, up to order of terms, there is one and only one way to express # as a
linear combination
# ~   b Ã„ b   
where the  's are distinct vectors in : and the coefficients  are nonzero. More
explicitly, # Â£  is an essentially unique linear combination of the vectors in :
if # Â Âº:Â» and if whenever

46

Advanced Linear Algebra

# ~   b Ã„ b   

and # ~  ! b Ã„ b  !

where the  's are distinct, the ! 's are distinct and all coefficients are nonzero,
then  ~  and after a reindexing of the  ! 's if necessary, we have  ~  and
 ~ ! for all  ~ Ã Ãƒ Ã . (Note that this is stronger than saying that
  ~  ! .)
We may characterize linear independence as follows.
Theorem 1.6 Let : Â£ Â¸Â¹ be a nonempty set of vectors in = . The following are
equivalent:
1) : is linearly independent.
2) Every nonzero vector # Â spanÂ²:Â³ is an essentially unique linear
combination of the vectors in : .
3) No vector in : is a linear combination of other vectors in : .
Proof. Suppose that 1) holds and that
 Â£ # ~   b Ã„ b    ~   !  b Ã„ b   ! 
where the  's are distinct, the ! 's are distinct and the coefficients are nonzero.
By subtracting and grouping 's and !'s that are equal, we can write
 ~ Â² c  Â³  b Ã„ b Â² c  Â³ 
b b b b Ã„ b  
c b !b c Ã„ c  !
and so 1) implies that  ~  ~  and " ~ " and
Thus, 1) implies 2).
If 2) holds and

" ~ !" for all  ~ Ã Ãƒ Ã  .

Â : can be written as
~   b Ã„ b  

where  Â : are different from , then we may collect like terms on the right
and then remove all terms with  coefficient. The resulting expression violates
2). Hence, 2) implies 3). If 3) holds and
  b Ã„ b   ~ 
where the

 's are distinct and  Â£ , then  Â€  and we may write
 ~c


Â²  b Ã„ b   Â³


which violates 3).
The following key theorem relates the notions of spanning set and linear
independence.

Vector Spaces

47

Theorem 1.7 Let : be a set of vectors in = . The following are equivalent:
1) : is linearly independent and spans = .
2) Every nonzero vector # Â = is an essentially unique linear combination of
vectors in : .
3) : is a minimal spanning set, that is, : spans = but any proper subset of :
does not span = .
4) : is a maximal linearly independent set, that is, : is linearly independent,
but any proper superset of : is not linearly independent.
A set of vectors in = that satisfies any (and hence all) of these conditions is
called a basis for = .
Proof. We have seen that 1) and 2) are equivalent. Now suppose 1) holds. Then
: is a spanning set. If some proper subset : Z of : also spanned = , then any
vector in : c : Z would be a linear combination of the vectors in : Z ,
contradicting the fact that the vectors in : are linearly independent. Hence 1)
implies 3).
Conversely, if : is a minimal spanning set, then it must be linearly independent.
For if not, some vector Â : would be a linear combination of the other vectors
in : and so : c Â¸ Â¹ would be a proper spanning subset of : , which is not
possible. Hence 3) implies 1).
Suppose again that 1) holds. If : were not maximal, there would be a vector
# Â = c : for which the set : r Â¸#Â¹ is linearly independent. But then # is not
in the span of : , contradicting the fact that : is a spanning set. Hence, : is a
maximal linearly independent set and so 1) implies 4).
Conversely, if : is a maximal linearly independent set, then : must span = , for
if not, we could find a vector # Â = c : that is not a linear combination of the
vectors in : . Hence, : r Â¸#Â¹ would be a linearly independent proper superset of
:, which is a contradiction. Thus, 4) implies 1).
Theorem 1.8 A finite set : ~ Â¸# Ã Ãƒ Ã # Â¹ of vectors in = is a basis for = if
and only if
= ~ Âº# Â» l Ã„ l Âº# Â»
Example 1.6 The th standard vector in -  is the vector  that has 's in all
coordinate positions except the th, where it has a . Thus,
 ~ Â²Ã Ã Ãƒ Ã Â³Ã

 ~ Â²Ã Ã Ãƒ Ã Â³ Ã Ãƒ Ã

 ~ Â²Ã Ãƒ Ã Ã Â³

The set Â¸ Ã Ãƒ Ã  Â¹ is called the standard basis for -  .
The proof that every nontrivial vector space has a basis is a classic example of
the use of Zorn's lemma.

48

Advanced Linear Algebra

Theorem 1.9 Let = be a nonzero vector space. Let 0 be a linearly independent
set in = and let : be a spanning set in = containing 0 . Then there is a basis 8
for = for which 0 Â‹ 8 Â‹ : . In particular,
1) Any vector space, except the zero space Â¸Â¹, has a basis.
2) Any linearly independent set in = is contained in a basis.
3) Any spanning set in = contains a basis.
Proof. Consider the collection 7 of all linearly independent subsets of =
containing 0 and contained in : . This collection is not empty, since 0 Â 7 .
Now, if
9 ~ Â¸0 Â“  Â 2Â¹
is a chain in 7 , then the union
< ~  0
Â2

is linearly independent and satisfies 0 Â‹ < Â‹ : , that is, < Â 7 . Hence, every
chain in 7 has an upper bound in 7 and according to Zorn's lemma, 7 must
contain a maximal element 8 , which is linearly independent.
Now, 8 is a basis for the vector space Âº:Â» ~ = , for if any Â : is not a linear
combination of the elements of 8 , then 8 r Â¸ Â¹ Â‹ : is linearly independent,
contradicting the maximality of 8 . Hence : Â‹ Âº8 Â» and so = ~ Âº:Â» Â‹ Âº8 Â».
The reader can now show, using Theorem 1.9, that any subspace of a vector
space has a complement.

The Dimension of a Vector Space
The next result, with its classical elegant proof, says that if a vector space = has
a finite spanning set : , then the size of any linearly independent set cannot
exceed the size of : .
Theorem 1.10 Let = be a vector space and assume that the vectors # Ã Ãƒ Ã #
are linearly independent and the vectors  Ã Ãƒ Ã  span = . Then  Â .
Proof. First, we list the two sets of vectors: the spanning set followed by the
linearly independent set:
 Ã Ãƒ Ã  Ã‚ # Ã Ãƒ Ã # 
Then we move the first vector # to the front of the first list:
# Ã  Ã Ãƒ Ã  Ã‚ # Ã Ãƒ Ã #
Since  Ã Ãƒ Ã  span = , # is a linear combination of the  's. This implies that
we may remove one of the  's, which by reindexing if necessary can be  ,
from the first list and still have a spanning set
# Ã  Ã Ãƒ Ã  Ã‚ # Ã Ãƒ Ã #

Vector Spaces

49

Note that the first set of vectors still spans = and the second set is still linearly
independent.
Now we repeat the process, moving # from the second list to the first list
# Ã # Ã  Ã Ãƒ Ã  Ã‚ # Ã Ãƒ Ã #
As before, the vectors in the first list are linearly dependent, since they spanned
= before the inclusion of # . However, since the # 's are linearly independent,
any nontrivial linear combination of the vectors in the first list that equals 
must involve at least one of the  's. Hence, we may remove that vector, which
again by reindexing if necessary may be taken to be  and still have a spanning
set
# Ã # Ã  Ã Ãƒ Ã  Ã‚ # Ã Ãƒ Ã #
Once again, the first set of vectors spans = and the second set is still linearly
independent.
Now, if   , then this process will eventually exhaust the
list
# Ã # Ã Ãƒ Ã # Ã‚ #b Ã Ãƒ Ã #

 's and lead to the

where # Ã # Ã Ãƒ Ã # span = , which is clearly not possible since # is not in the
span of # Ã # Ã Ãƒ Ã # . Hence,  Â .
Corollary 1.11 If = has a finite spanning set, then any two bases of = have the
same size.
Now let us prove the analogue of Corollary 1.11 for arbitrary vector spaces.
Theorem 1.12 If = is a vector space, then any two bases for = have the same
cardinality.
Proof. We may assume that all bases for = are infinite sets, for if any basis is
finite, then = has a finite spanning set and so Corollary 1.11 applies.
Let 8 ~ Â¸ Â“  Â 0Â¹ be a basis for = and let 9 be another basis for = . Then any
vector  Â 9 can be written as a finite linear combination of the vectors in 8 ,
where all of the coefficients are nonzero, say
 ~    
Â<

But because 9 is a basis, we must have
 < ~ 0
Â9

50

Advanced Linear Algebra

for if the vectors in 9 can be expressed as finite linear combinations of the
vectors in a proper subset 8 Z of 8 , then 8 Z spans = , which is not the case.
Since (< (  L for all  Â 9, Theorem 0.17 implies that
(8 ( ~ (0 ( Â L (9( ~ (9(
But we may also reverse the roles of 8 and 9, to conclude that (9( Â (8 ( and so
the SchroÌˆderâ€“Bernstein theorem implies that (8 ( ~ (9(.
Theorem 1.12 allows us to make the following definition.
Definition A vector space = is finite-dimensional if it is the zero space Â¸Â¹, or
if it has a finite basis. All other vector spaces are infinite-dimensional. The
dimension of the zero space is  and the dimension of any nonzero vector
space = is the cardinality of any basis for = . If a vector space = has a basis of
cardinality  , we say that = is -dimensional and write dimÂ²= Â³ ~  .
It is easy to see that if : is a subspace of = , then dimÂ²:Â³ Â dimÂ²= Â³. If in
addition, dimÂ²:Â³ ~ dimÂ²= Â³  B, then : ~ = .
Theorem 1.13 Let = be a vector space.
1) If 8 is a basis for = and if 8 ~ 8 r 8 and 8 q 8 ~ J, then
= ~ Âº8 Â» l Âº8 Â»
2) Let = ~ : l ; . If 8 is a basis for : and 8 is a basis for ; , then
8 q 8 ~ J and 8 ~ 8 r 8 is a basis for = .
Theorem 1.14 Let : and ; be subspaces of a vector space = . Then
dimÂ²:Â³ b dimÂ²; Â³ ~ dimÂ²: b ; Â³ b dimÂ²: q ; Â³
In particular, if ; is any complement of : in = , then
dimÂ²:Â³ b dimÂ²; Â³ ~ dimÂ²= Â³
that is,
dimÂ²: l ; Â³ ~ dimÂ²:Â³ b dimÂ²; Â³
Proof. Suppose that 8 ~ Â¸ Â“  Â 0Â¹ is a basis for : q ; . Extend this to a basis
7 r 8 for : where 7 ~ Â¸ Â“  Â 1 Â¹ is disjoint from 8 . Also, extend 8 to a
basis 8 r 9 for ; where 9 ~ Â¸ Â“  Â 2Â¹ is disjoint from 8 . We claim that
7 r 8 r 9 is a basis for : b ; . It is clear that Âº7 r 8 r 9Â» ~ : b ; .
To see that 7 r 8 r 9 is linearly independent, suppose to the contrary that

Vector Spaces

51

 # b Ã„ b  # ~ 
where # Â 7 r 8 r 9 and  Â£  for all . There must be vectors # in this
expression from both 7 and 9, since 7 r 8 and 8 r 9 are linearly independent.
Isolating the terms involving the vectors from 7 on one side of the equality
shows that there is a nonzero vector in % Â Âº7Â» q Âº8 r 9Â». But then % Â : q ;
and so % Â Âº7Â» q Âº8 Â», which implies that % ~ , a contradiction. Hence,
7 r 8 r 9 is linearly independent and a basis for : b ; .
Now,
dimÂ²:Â³ b dimÂ²; Â³ ~ (7 r 8 ( b (8 r 9(
~ (7( b (8 ( b (8 ( b (9(
~ (7( b (8 ( b (9( b dimÂ²: q ; Â³
~ dimÂ²: b ; Â³ b dimÂ²: q ; Â³
as desired.
It is worth emphasizing that while the equation
dimÂ²:Â³ b dimÂ²; Â³ ~ dimÂ²: b ; Â³ b dimÂ²: q ; Â³
holds for all vector spaces, we cannot write
dimÂ²: b ; Â³ ~ dimÂ²:Â³ b dimÂ²; Â³ c dimÂ²: q ; Â³
unless : b ; is finite-dimensional.

Ordered Bases and Coordinate Matrices
It will be convenient to consider bases that have an order imposed on their
members.
Definition Let = be a vector space of dimension . An ordered basis for = is
an ordered -tuple Â²# Ã Ãƒ Ã # Â³ of vectors for which the set Â¸# Ã Ãƒ Ã # Â¹ is a
basis for = .
If 8 ~ Â²# Ã Ãƒ Ã # Â³ is an ordered basis for = , then for each # Â = there is a
unique ordered -tuple Â² Ã Ãƒ Ã  Â³ of scalars for which
# ~   # b Ã„ b   # 
Accordingly, we can define the coordinate map 8 Â¢ = Â¦ -  by
8 Â²#Â³ ~ Â´#Âµ8 ~

v  y
Ã…
w  z

(1.3)

52

Advanced Linear Algebra

where the column matrix Â´#Âµ8 is known as the coordinate matrix of # with
respect to the ordered basis 8 . Clearly, knowing Â´#Âµ8 is equivalent to knowing #
(assuming knowledge of 8 ).
Furthermore, it is easy to see that the coordinate map 8 is bijective and
preserves the vector space operations, that is,
8 Â² # b Ã„ b  # Â³ ~  8 Â²# Â³ b Ã„ b  8 Â²# Â³
or equivalently
Â´ # b Ã„ b  # Âµ8 ~  Â´# Âµ8 b Ã„ b  Â´# Âµ8
Functions from one vector space to another that preserve the vector space
operations are called linear transformations and form the objects of study in the
next chapter.

The Row and Column Spaces of a Matrix
Let ( be an  d  matrix over - . The rows of ( span a subspace of -  known
as the row space of ( and the columns of ( span a subspace of -  known as
the column space of (. The dimensions of these spaces are called the row rank
and column rank, respectively. We denote the row space and row rank by
rsÂ²(Â³ and rrkÂ²(Â³ and the column space and column rank by csÂ²(Â³ and crkÂ²(Â³.
It is a remarkable and useful fact that the row rank of a matrix is always equal to
its column rank, despite the fact that if  Â£ , the row space and column space
are not even in the same vector space!
Our proof of this fact hinges on the following simple observation about
matrices.
Lemma 1.15 Let ( be an  d  matrix. Then elementary column operations do
not affect the row rank of (. Similarly, elementary row operations do not affect
the column rank of (.
Proof. The second statement follows from the first by taking transposes. As to
the first, the row space of ( is
rsÂ²(Â³ ~ Âº (Ã Ãƒ Ã  (Â»
where  are the standard basis vectors in -  . Performing an elementary
column operation on ( is equivalent to multiplying ( on the right by an
elementary matrix , . Hence the row space of (, is
rsÂ²(,Â³ ~ Âº (,Ã Ãƒ Ã  (,Â»
and since , is invertible,

Vector Spaces

53

rrkÂ²(Â³ ~ dimÂ²rsÂ²(Â³Â³ ~ dimÂ²rsÂ²(,Â³Â³ ~ rrkÂ²(,Â³
as desired.
Theorem 1.16 If ( Â CÃ , then rrkÂ²(Â³ ~ crkÂ²(Â³. This number is called the
rank of ( and is denoted by rkÂ²(Â³.
Proof. According to the previous lemma, we may reduce ( to reduced column
echelon form without affecting the row rank. But this reduction does not affect
the column rank either. Then we may further reduce ( to reduced row echelon
form without affecting either rank. The resulting matrix 4 has the same row
and column ranks as (. But 4 is a matrix with 's followed by 's on the main
diagonal (entries 4Ã Ã 4Ã Ã Ãƒ ) and 's elsewhere. Hence,
rrkÂ²(Â³ ~ rrkÂ²4 Â³ ~ crkÂ²4 Â³ ~ crkÂ²(Â³
as desired.

The Complexification of a Real Vector Space
If > is a complex vector space (that is, a vector space over d), then we can
think of > as a real vector space simply by restricting all scalars to the field s.
Let us denote this real vector space by >s and call it the real version of > .
On the other hand, to each real vector space = , we can associate a complex
vector space = d . This â€œcomplexificationâ€ process will play a useful role when
we discuss the structure of linear operators on a real vector space. (Throughout
our discussion = will denote a real vector space.)
Definition If = is a real vector space, then the set = d ~ = d = of ordered
pairs, with componentwise addition
Â²"Ã #Â³ b Â²%Ã &Â³ ~ Â²" b %Ã # b &Â³
and scalar multiplication over d defined by
Â² b Â³Â²"Ã #Â³ ~ Â²" c #Ã # b "Â³
for Ã  Â s is a complex vector space, called the complexification of = .
It is convenient to introduce a notation for vectors in = d that resembles the
notation for complex numbers. In particular, we denote Â²"Ã #Â³ Â = d by " b #
and so
= d ~ Â¸" b # Â“ "Ã # Â = Â¹
Addition now looks like ordinary addition of complex numbers,
Â²" b #Â³ b Â²% b &Â³ ~ Â²" b %Â³ b Â²# b &Â³
and scalar multiplication looks like ordinary multiplication of complex numbers,

54

Advanced Linear Algebra

Â² b Â³Â²" b #Â³ ~ Â²" c #Â³ b Â²# b "Â³
Thus, for example, we immediately have for Ã  Â s,
Â²" b #Â³ ~ " b #
Â²" b #Â³ ~ c# b "
Â² b Â³" ~ " b "
Â² b Â³# ~ c# b #
The real part of ' ~ " b # is " Â = and the imaginary part of ' is # Â = .
The essence of the fact that ' ~ " b # Â = d is really an ordered pair is that ' is
 if and only if its real and imaginary parts are both .
We can define the complexification map cpxÂ¢ = Â¦ = d by
cpxÂ²#Â³ ~ # b 
Let us refer to # b  as the complexification, or complex version of # Â = .
Note that this map is a group homomorphism, that is,
cpxÂ²Â³ ~  b 

and

cpxÂ²" f #Â³ ~ cpxÂ²"Â³ f cpxÂ²#Â³

and it is injective:
cpxÂ²"Â³ ~ cpxÂ²#Â³ Â¯ " ~ #
Also, it preserves multiplication by real scalars:
cpxÂ²"Â³ ~ " b  ~ Â²" b Â³ ~ cpxÂ²"Â³
for  Â s. However, the complexification map is not surjective, since it gives
only â€œrealâ€ vectors in = d .
The complexification map is an injective linear transformation (defined in the
next chapter) from the real vector space = to the real version Â²= d Â³s of the
complexification = d , that is, to the complex vector space = d provided that
scalars are restricted to real numbers. In this way, we see that = d contains an
embedded copy of = .

The Dimension of = d
The vector-space dimensions of = and = d are the same. This should not
necessarily come as a surprise because although = d may seem â€œbiggerâ€ than = ,
the field of scalars is also â€œbigger.â€
Theorem 1.17 If 8 ~ Â¸# Â“  Â 0Â¹ is a basis for = over s, then the
complexification of 8 ,
cpxÂ²8 Â³ ~ Â¸# b  Â“ # Â 8 Â¹

Vector Spaces

55

is a basis for the vector space = d over d. Hence,
dimÂ²= d Â³ ~ dimÂ²= Â³
Proof. To see that cpxÂ²8 Â³ spans = d over d, let % b & Â = d . Then %Ã & Â =
and so there exist real numbers  and  (some of which may be ) for which
1

1

% b & ~   # b @  # A
~

~

1

~  Â² # b  # Â³
~
1

~  Â² b  Â³Â²# b Â³
~

To see that cpxÂ²8 Â³ is linearly independent, if
1

 Â² b  Â³Â²# b Â³ ~  b 
~

then the previous computations show that
1

1

  # ~  and   # ~ 
~

~

The independence of 8 then implies that  ~  and  ~  for all .
If # Â = and 8 ~ Â¸# Â“  Â 0Â¹ is a basis for = , then we may write


# ~   # 
~

for  Â s. Since the coefficients are real, we have


# b  ~   Â²# b Â³
~

and so the coordinate matrices are equal:
Â´# b ÂµcpxÂ²8 Â³ ~ Â´#Âµ8

Exercises
1.

Let = be a vector space over - . Prove that # ~  and  ~  for all # Â =
and  Â - . Describe the different 's in these equations. Prove that if
# ~ , then  ~  or # ~ . Prove that # ~ # implies that # ~  or  ~ .

56
2.
3.

4.

Advanced Linear Algebra
Prove Theorem 1.3.
a) Find an abelian group = and a field - for which = is a vector space
over - in at least two different ways, that is, there are two different
definitions of scalar multiplication making = a vector space over - .
b) Find a vector space = over - and a subset : of = that is (1) a
subspace of = and (2) a vector space using operations that differ from
those of = .
Suppose that = is a vector space with basis 8 ~ Â¸ Â“  Â 0Â¹ and : is a
subspace of = . Let Â¸) Ã Ãƒ Ã ) Â¹ be a partition of 8 . Then is it true that


: ~ Â²: q Âº) Â»Â³
~

5.
6.

What if : q Âº) Â» Â£ Â¸Â¹ for all ?
Prove Theorem 1.8.
Let :Ã ; Ã < Â I Â²= Â³. Show that if < Â‹ : , then
: q Â²; b < Â³ ~ Â²: q ; Â³ b <

7.

This is called the modular law for the lattice IÂ²= Â³.
For what vector spaces does the distributive law of subspaces
: q Â²; b < Â³ ~ Â²: q ; Â³ b Â²: q < Â³

hold?
A vector # ~ Â² Ã Ãƒ Ã  Â³ Â s is called strongly positive if  Â€  for all
 ~ Ã Ãƒ Ã .
a) Suppose that # is strongly positive. Show that any vector that is â€œclose
enoughâ€ to # is also strongly positive. (Formulate carefully what â€œclose
enoughâ€ should mean.)
b) Prove that if a subspace : of s contains a strongly positive vector,
then : has a basis of strongly positive vectors.
9. Let 4 be an  d  matrix whose rows are linearly independent. Suppose
that the  columns  Ã Ãƒ Ã  of 4 span the column space of 4 . Let * be
the matrix obtained from 4 by deleting all columns except  Ã Ãƒ Ã  .
Show that the rows of * are also linearly independent.
10. Prove that the first two statements in Theorem 1.7 are equivalent.
11. Show that if : is a subspace of a vector space = , then dimÂ²:Â³ Â dimÂ²= Â³.
Furthermore, if dimÂ²:Â³ ~ dimÂ²= Â³  B then : ~ = . Give an example to
show that the finiteness is required in the second statement.
12. Let dimÂ²= Â³  B and suppose that = ~ < l : ~ < l : . What can you
say about the relationship between : and : ? What can you say if
: Â‹ : ?
13. What is the relationship between : l ; and ; l : ? Is the direct sum
operation commutative? Formulate and prove a similar statement
concerning associativity. Is there an â€œidentityâ€ for direct sum? What about
â€œnegativesâ€?
8.

Vector Spaces

57

14. Let = be a finite-dimensional vector space over an infinite field - . Prove
that if : Ã Ãƒ Ã : are subspaces of = of equal dimension, then there is a
subspace ; of = for which = ~ : l ; for all  ~ Ã Ãƒ Ã  . In other words,
; is a common complement of the subspaces : .
15. Prove that the vector space 9 of all continuous functions from s to s is
infinite-dimensional.
16. Show that Theorem 1.2 need not hold if the base field - is finite.
17. Let : be a subspace of = . The set # b : ~ Â¸# b Â“ Â :Â¹ is called an
affine subspace of = .
a) Under what conditions is an affine subspace of = a subspace of = ?
b) Show that any two affine subspaces of the form # b : and $ b : are
either equal or disjoint.
18. If = and > are vector spaces over - for which (= ( ~ (> (, then does it
follow that dimÂ²= Â³ ~ dimÂ²> Â³?
19. Let = be an -dimensional real vector space and suppose that : is a
subspace of = with dimÂ²:Â³ ~  c . Define an equivalence relation Â– on
the set = Â± : by # Â– $ if the â€œline segmentâ€
3Â²#Ã $Â³ ~ Â¸# b Â² c Â³$ Â“  Â  Â Â¹
has the property that 3Â²#Ã $Â³ q : ~ J. Prove that Â– is an equivalence
relation and that it has exactly two equivalence classes.
20. Let - be a field. A subfield of - is a subset 2 of - that is a field in its
own right using the same operations as defined on - .
a) Show that - is a vector space over any subfield 2 of - .
b) Suppose that - is an -dimensional vector space over a subfield 2 of
- . If = is an -dimensional vector space over - , show that = is also a
vector space over 2 . What is the dimension of = as a vector space
over 2 ?
21. Let - be a finite field of size  and let = be an -dimensional vector space
over - . The purpose of this exercise is to show that the number of
subspaces of = of dimension  is

Â²  c Â³Ã„Â² c Â³
4 5 ~ 
 
Â² c Â³Ã„Â² c Â³Â² c c Â³Ã„Â² c Â³
The expressions Â²  Â³ are called Gaussian coefficients and have properties
similar to those of the binomial coefficients. Let :Â²Ã Â³ be the number of
 -dimensional subspaces of = .
a) Let 5 Â²Ã Â³ be the number of  -tuples of linearly independent vectors
Â²# Ã Ãƒ Ã # Â³ in = . Show that
5 Â²Ã Â³ ~ Â²  c Â³Â²  c Â³Ã„Â²  c  c Â³
b) Now, each of the  -tuples in a) can be obtained by first choosing a
subspace of = of dimension  and then selecting the vectors from this
subspace. Show that for any  -dimensional subspace of = , the number

58

Advanced Linear Algebra
of  -tuples of independent vectors in this subspace is
Â²  c Â³Â²  c Â³Ã„Â²  c  c Â³
c)

Show that
5 Â²Ã Â³ ~ :Â²Ã Â³Â²  c Â³Â²  c Â³Ã„Â²  c  c Â³

How does this complete the proof?
22. Prove that any subspace : of s is a closed set or, equivalently, that its set
complement :  ~ s Â± : is open, that is, for any % Â :  there is an open
ball )Â²%Ã Â³ centered at % with radius  Â€  for which )Â²%Ã Â³ Â‹ :  .
23. Let 8 ~ Â¸ Ã Ãƒ Ã  Â¹ and 9 ~ Â¸ Ã Ãƒ Ã  Â¹ be bases for a vector space = .
Let  Â  Â  c . Show that there is a permutation  of Â¸Ã Ãƒ Ã Â¹ such
that
 Ã Ãƒ Ã  Ã Â²bÂ³ Ã Ãƒ Ã Â²Â³
and
Â²Â³ Ã Ãƒ Ã Â²Â³ Ã b Ã Ãƒ Ã 
are both bases for = . Hint: You may use the fact that if 4 is an invertible
 d  matrix and if  Â  Â , then it is possible to reorder the rows so
that the upper left  d  submatrix and the lower right Â² c Â³ d Â² c Â³
submatrix are both invertible. (This follows, for example, from the general
Laplace expansion theorem for determinants.)
24. Let = be an -dimensional vector space over an infinite field - and
suppose that : Ã Ãƒ Ã : are subspaces of = with dimÂ²: Â³ Â   . Prove
that there is a subspace ; of = of dimension  c  for which
; q : ~ Â¸Â¹ for all .
25. What is the dimension of the complexification = d thought of as a real
vector space?
26. (When is a subspace of a complex vector space a complexification?) Let =
be a real vector space with complexification = d and let < be a subspace of
= d . Prove that there is a subspace : of = for which
< ~ : d ~ Â¸ b ! Â“ Ã ! Â :Â¹
if and only if < is closed under complex conjugation Â¢ = d Â¦ = d defined
by Â²" b #Â³ ~ " c #.

Chapter 2

Linear Transformations

Linear Transformations
Loosely speaking, a linear transformation is a function from one vector space to
another that preserves the vector space operations. Let us be more precise.
Definition Let = and > be vector spaces over a field - . A function  Â¢ = Â¦ >
is a linear transformation if
 Â²" b #Â³ ~  Â²"Â³ b  Â²#Â³
for all scalars Ã Â - and vectors ",# Â = . The set of all linear
transformations from = to > is denoted by BÂ²= Ã > Â³.
1) A linear transformation from = to = is called a linear operator on = . The
set of all linear operators on = is denoted by BÂ²= Â³. A linear operator on a
real vector space is called a real operator and a linear operator on a
complex vector space is called a complex operator.
2) A linear transformation from = to the base field - (thought of as a vector
space over itself) is called a linear functional on = . The set of all linear
functionals on = is denoted by = i and called the dual space of = .
We should mention that some authors use the term linear operator for any linear
transformation from = to > . Also, the application of a linear transformation 
on a vector # is denoted by  Â²#Â³ or by  #, parentheses being used when
necessary, as in  Â²" b #Â³, or to improve readability, as in Â² "Â³ rather than
Â² Â²"Â³Â³.
Definition The following terms are also employed:
1) homomorphism for linear transformation
2) endomorphism for linear operator
3) monomorphism (or embedding) for injective linear transformation
4) epimorphism for surjective linear transformation
5) isomorphism for bijective linear transformation.

