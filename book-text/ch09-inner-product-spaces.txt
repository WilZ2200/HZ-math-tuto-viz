Real and Complex Inner Product Spaces

211

1)  is an isometry if it preserves the inner product, that is, if
Âº "Ã  #Â» ~ Âº"Ã #Â»
for all "Ã # Â = .
2) A bijective isometry is called an isometric isomorphism. When Â¢ = Â¦ >
is an isometric isomorphism, we say that = and > are isometrically
isomorphic.
It is clear that an isometry is injective and so it is an isometric isomorphism
provided it is surjective. Moreover, if
dimÂ²= Â³ ~ dimÂ²> Â³  B
injectivity implies surjectivity and  is an isometry if and only if  is an
isometric isomorphism. On the other hand, the following simple example shows
that this is not the case for infinite-dimensional inner product spaces.
Example 9.3 The map Â¢ M Â¦ M defined by
Â²% Ã % Ã % Ã Ãƒ Â³ ~ Â²Ã % Ã % Ã Ãƒ Â³
is an isometry, but it is clearly not surjective.
Since the norm determines the inner product, the following should not come as a
surprise.
Theorem 9.6 A linear transformation  Â BÂ²= Ã > Â³ is an isometry if and only if
it preserves the norm, that is, if and only if
)#) ~ )#)
for all # Â = .
Proof. Clearly, an isometry preserves the norm. The converse follows from the
polarization identities. In the real case, we have

Â² ) " b  # ) c ) " c  # ) Â³


~ Â²) Â²" b #Â³) c ) Â²" c #Â³) Â³


~ Â²)" b #) c )" c #) Â³

~ Âº"Ã #Â»

Âº "Ã  #Â» ~

and so  is an isometry. The complex case is similar.

Orthogonality
The presence of an inner product allows us to define the concept of
orthogonality.

212

Advanced Linear Algebra

Definition Let = be an inner product space.
1) Two vectors "Ã # Â = are orthogonal, written " Â #, if
Âº"Ã #Â» ~ 
2) Two subsets ?Ã @ Â‹ = are orthogonal, written ? Â @ , if Âº?Ã @ Â» ~ Â¸Â¹,
that is, if % Â & for all % Â ? and & Â @ . We write # Â ? in place of
Â¸#Â¹ Â ?.
3) The orthogonal complement of a subset ? Â‹ = is the set
? Â ~ Â¸# Â = Â“ # Â ?Â¹
The following result is easily proved.
Theorem 9.7 Let = be an inner product space.
1) The orthogonal complement ? Â of any subset ? Â‹ = is a subspace of = .
2) For any subspace : of = ,
: q : Â ~ Â¸Â¹
Definition An inner product space = is the orthogonal direct sum of
subspaces : and ; if
= ~ : l ;Ã

:Â;

In this case, we write
: p;
More generally, = is the orthogonal direct sum of the subspaces : Ã Ãƒ Ã : ,
written
: ~ : p Ã„ p :
if
= ~ : l Ã„ l :

and

: Â : for  Â£ 

Theorem 9.8 Let = be an inner product space. The following are equivalent.
1) = ~ : p ;
2) = ~ : l ; and ; ~ : Â
Proof. If = ~ : p ; , then by definition, ; Â‹ : Â . However, if # Â : Â , then
# ~ b ! where Â : and ! Â ; . Then is orthogonal to both ! and # and so
is orthogonal to itself, which implies that ~  and so # Â ; . Hence, ; ~ : Â .
The converse is clear.

Orthogonal and Orthonormal Sets
Definition A nonempty set E ~ Â¸" Â“  Â 2Â¹ of vectors in an inner product
space is said to be an orthogonal set if " Â " for all  Â£  Â 2 . If, in
addition, each vector " is a unit vector, then E is an orthonormal set. Thus, a

Real and Complex Inner Product Spaces

213

set is orthonormal if
Âº" Ã " Â» ~ Ã
for all Ã  Â 2 , where Ã is the Kronecker delta function.
Of course, given any nonzero vector # Â = , we may obtain a unit vector " by
multiplying # by the reciprocal of its norm:
"~


#
)#)

This process is referred to as normalizing the vector #. Thus, it is a simple
matter to construct an orthonormal set from an orthogonal set of nonzero
vectors.
Note that if " Â #, then
)" b #) ~ )") b )#)
and the converse holds if - ~ s.
Orthogonality is stronger than linear independence.
Theorem 9.9 Any orthogonal set of nonzero vectors in = is linearly
independent.
Proof. If E ~ Â¸" Â“  Â 2Â¹ is an orthogonal set of nonzero vectors and
 " b Ã„ b  " ~ 
then
 ~ Âº " b Ã„ b  " Ã " Â» ~  Âº" Ã " Â»
and so  ~ , for all  . Hence, E is linearly independent.

Gramâ€“Schmidt Orthogonalization
The Gramâ€“Schmidt process can be used to transform a sequence of vectors into
an orthogonal sequence. We begin with the following.
Theorem 9.10 (Gramâ€“Schmidt augmentation) Let = be an inner product
space and let E ~ Â¸" Ã Ãƒ Ã " Â¹ be an orthogonal set of vectors in = . If
# Â¤ Âº" Ã Ãƒ Ã " Â», then there is a nonzero " Â = for which Â¸" Ã Ãƒ Ã " Ã "Â¹ is
orthogonal and
Âº" Ã Ãƒ Ã " Ã "Â» ~ Âº" Ã Ãƒ Ã " Ã #Â»
In particular,

214

Advanced Linear Algebra



" ~ # c   " 
~

where
if " ~ 
if " Â£ 


 ~ H Âº#Ã" Â»

Âº" Ã" Â»

Proof. We simply set
" ~ # c  "  c Ã„ c   " 
and force " Â " for all , that is,
 ~ Âº"Ã " Â» ~ Âº# c  " c Ã„ c  " Ã " Â» ~ Âº#Ã " Â» c  Âº" Ã "Â»
Thus, if " ~ , take  ~  and if " Â£ , take
 ~

Âº#Ã " Â»
Âº" Ã " Â»

The Gramâ€“Schmidt augmentation is traditionally applied to a sequence of
linearly independent vectors, but it also applies to any sequence of vectors.
Theorem 9.11 (The Gramâ€“Schmidt orthogonalization process) Let
8 ~ Â²# Ã # Ã Ãƒ Â³ be a sequence of vectors in an inner product space = . Define a
sequence E ~ Â²" Ã " Ã Ãƒ Â³ by repeated Gramâ€“Schmidt augmentation, that is,
c

" ~ # c  Ã "
~

where " ~ # and

Ã ~ H Âº# Ã" Â»
Âº" Ã" Â»

if " ~ 
if " Â£ 

Then E is an orthogonal sequence in = with the property that
Âº" Ã Ãƒ Ã " Â» ~ Âº# Ã Ãƒ Ã # Â»
for all  Â€ . Also, " ~  if and only if # Â Âº# Ã Ãƒ Ã #c Â».
Proof. The result holds for  ~ . Assume it holds for  c . If
# Â Âº# Ã Ãƒ Ã #c Â», then
# Â Âº# Ã Ãƒ Ã #c Â» ~ Âº" Ã Ãƒ Ã "c Â»
Writing

Real and Complex Inner Product Spaces

215

c

# ~   "
~

we have
Âº# Ã " Â» ~ F


 Âº" Ã " Â»

if " ~ 
if " Â£ 

Therefore,  ~ Ã when " Â£  and so " ~ . Hence,
Âº" Ã Ãƒ Ã " Â» ~ Âº" Ã Ãƒ Ã "c Ã Â» ~ Âº# Ã Ãƒ Ã #c Â» ~ Âº# Ã Ãƒ Ã # Â»
If # Â¤ Âº# Ã Ãƒ Ã #c Â» then
Âº" Ã Ãƒ Ã " Â» ~ Âº# Ã Ãƒ Ã #c Ã " Â» ~ Âº# Ã Ãƒ Ã #c Ã # Â»
Example 9.4 Consider the inner product space sÂ´%Âµ of real polynomials, with
inner product defined by


ÂºÂ²%Â³Ã Â²%Â³Â» ~

Â²%Â³Â²%Â³%
c

Applying the Gramâ€“Schmidt process to the sequence 8 ~ Â²Ã %Ã % Ã % Ã Ãƒ Â³
gives
" Â²%Â³ ~ 
" Â²%Â³ ~ % c

  % %
c

  %

h~%

c
 

  % %
% %

"3 Â²%Â³ ~ % c c
h  c c
h % ~ % c

 %
  % %
c
c
   
 
 
 % Â²% c Â³%
 % %
 % %

3
h >% c ?
"4 Â²%Â³ ~ % c c
h  c c
h % c c


 Â²% c  Â³ %
 %
 % %
c
c
c
3


~ % c %
and so on. The polynomials in this sequence are (at least up to multiplicative
constants) the Legendre polynomials.

The QR Factorization
The Gramâ€“Schmidt process can be used to factor any real or complex matrix
into a product of a matrix with orthogonal columns and an upper triangular
matrix. Suppose that ( ~ Â²# Â“ # Â“ Ã„ Â“ # Â³ is an  d  matrix with columns
# , where  Â . The Gramâ€“Schmidt process applied to these columns gives
orthogonal vectors 6 ~ Â²" Â“ " Â“ Ã„ Â“ " Â³ for which

216

Advanced Linear Algebra

Âº" Ã Ãƒ Ã " Â» ~ Âº# Ã Ãƒ Ã # Â»
for all  Â . In particular,
c

# ~ " b  Ã "
~

where

Ã ~ H Âº# Ã" Â»
Âº" Ã" Â»

if " ~ 
if " Â£ 

In matrix terms,
v
x
Â²# Â“ # Â“ Ã„ Â“ # Â³ ~ Â²" Â“ " Â“ Ã„ Â“ " Â³x

Ã


w

Ã„ Ã y
Ã„ Ã {
{
Ã†
 z

that is, ( ~ 6) where 6 has orthogonal columns and ) is upper triangular.
We may normalize the nonzero columns " of 6 and move the positive
constants to ) . In particular, if  ~ )" ) for " Â£  and  ~  for " ~ , then
v 
" "
" x
Â²# Â“ # Â“ Ã„ Â“ # Â³ ~ 6 c
c Ã„ c 7x
 

w

 Ã


Ã„  Ã y
Ã„  Ã {
{
Ã†
 z

and so
( ~ 89
where the columns of 8 are orthogonal and each column is either a unit vector
or the zero vector and 9 is upper triangular with positive entries on the main
diagonal. Moreover, if the vectors # Ã Ãƒ Ã # are linearly independent, then the
columns of 8 are nonzero. Also, if  ~  and ( is nonsingular, then 8 is
unitary/orthogonal.
If the columns of ( are not linearly independent, we can make one final
adjustment to this matrix factorization. If a column " Â° is zero, then we may
replace this column by any vector as long as we replace the Â²Ã Â³th entry  in 9
by . Therefore, we can take nonzero columns of 8, extend to an orthonormal
basis for the span of the columns of 8 and replace the zero columns of 8 by the
additional members of this orthonormal basis. In this way, 8 is replaced by a
unitary/orthogonal matrix 8Z and 9 is replaced by an upper triangular matrix 9 Z
that has nonnegative entries on the main diagonal.

Real and Complex Inner Product Spaces

217

Theorem 9.12 Let ( Â CÃ Â²- Â³, where - ~ d or - ~ s. There exists a
matrix 8 Â CÃ Â²- Â³ with orthonormal columns and an upper triangular
matrix 9 Â C Â²- Â³ with nonnegative real entries on the main diagonal for
which
( ~ 89
Moreover, if  ~ , then 8 is unitary/orthogonal. If ( is nonsingular, then 9
can be chosen to have positive entries on the main diagonal, in which case the
factors 8 and 9 are unique. The factorization ( ~ 89 is called the 89
factorization of the matrix (. If ( is real, then 8 and 9 may be taken to be
real.
Proof. As to uniqueness, if ( is nonsingular and 89 ~ 8 9 then
c
8c
 8 ~ 9 9

and the right side is upper triangular with nonzero entries on the main diagonal
and the left side is unitary. But an upper triangular matrix with positive entries
on the main diagonal is unitary if and only if it is the identity and so 8 ~ 8
and 9 ~ 9. Finally, if ( is real, then all computations take place in the real
field and so 8 and 9 are real.
The 89 decomposition has important applications. For example, a system of
linear equations (% ~ " can be written in the form
89% ~ "
and since 8c ~ 8i , we have
9% ~ 8i "
This is an upper triangular system, which is easily solved by back substitution;
that is, starting from the bottom and working up.
We mention also that the 89 factorization is associated with an algorithm for
approximating the eigenvalues of a matrix, called the 89 algorithm.
Specifically, if ( ~ ( is an  d  matrix, define a sequence of matrices as
follows:
1) Let ( ~ 8 9 be the 89 factorization of ( and let ( ~ 9 8 .
2) Once ( has been defined, let ( ~ 8 9 be the 89 factorization of (
and let (b ~ 9 8 .
Then ( is unitarily/orthogonally similar to (, since
i
8c ( 8ic ~ 8c Â²9c 8c Â³8c
~ 8c 9c ~ (c

For complex matrices, it can be shown that under certain circumstances, such as
when the eigenvalues of ( have distinct norms, the sequence ( converges

218

Advanced Linear Algebra

(entrywise) to an upper triangular matrix < , which therefore has the eigenvalues
of ( on its main diagonal. Results can be obtained in the real case as well. For
more details, we refer the reader to [48], page 115.

Hilbert and Hamel Bases
Definition A maximal orthonormal set in an inner product space = is called a
Hilbert basis for = .
Zorn's lemma can be used to show that any nontrivial inner product space has a
Hilbert basis. We leave the details to the reader.
Some care must be taken not to confuse the concepts of a basis for a vector
space and a Hilbert basis for an inner product space. To avoid confusion, a
vector space basis, that is, a maximal linearly independent set of vectors, is
referred to as a Hamel basis. We will refer to an orthonormal Hamel basis as an
orthonormal basis.
To be perfectly clear, there are maximal linearly independent sets called
(Hamel) bases and maximal orthonormal sets (called Hilbert bases). If a
maximal linearly independent set (basis) is orthonormal, it is called an
orthonormal basis.
Moreover, since every orthonormal set is linearly independent, it follows that an
orthonormal basis is a Hilbert basis, since it cannot be properly contained in an
orthonormal set. For finite-dimensional inner product spaces, the two types of
bases are the same.
Theorem 9.13 Let = be an inner product space. A finite subset
E ~ Â¸" Ã Ãƒ Ã " Â¹ of = is an orthonormal (Hamel) basis for = if and only if it is
a Hilbert basis for = .
Proof. We have seen that any orthonormal basis is a Hilbert basis. Conversely,
if E is a finite maximal orthonormal set and E Â‰ F , where F is linearly
independent, then we may apply part 1) to extend E to a strictly larger
orthonormal set, in contradiction to the maximality of E . Hence, E is maximal
linearly independent.
The following example shows that the previous theorem fails for infinitedimensional inner product spaces.
Example 9.5 Let = ~ M and let 4 be the set of all vectors of the form
 ~ Â²Ã Ãƒ Ã Ã Ã Ã Ãƒ Â³
where  has a  in the th coordinate and 's elsewhere. Clearly, 4 is an
orthonormal set. Moreover, it is maximal. For if # ~ Â²% Â³ Â M has the property
that # Â 4 , then

Real and Complex Inner Product Spaces

219

% ~ Âº#Ã  Â» ~ 
for all  and so # ~ . Hence, no nonzero vector # Â¤ 4 is orthogonal to 4 .
This shows that 4 is a Hilbert basis for the inner product space M .
On the other hand, the vector space span of 4 is the subspace : of all
sequences in M that have finite support, that is, have only a finite number of
nonzero terms and since spanÂ²4 Â³ ~ : Â£ M , we see that 4 is not a Hamel
basis for the vector space M .

The Projection Theorem and Best Approximations
Orthonormal bases have a great practical advantage over arbitrary bases. From a
computational point of view, if 8 ~ Â¸# Ã Ãƒ Ã # Â¹ is a basis for = , then each
# Â = has the form
# ~  # b Ã„ b  #
In general, determining the coordinates  requires solving a system of linear
equations of size  d .
On the other hand, if E ~ Â¸" Ã Ãƒ Ã " Â¹ is an orthonormal basis for = and
# ~  "  b Ã„ b   " 
then the coefficients  are quite easily computed:
Âº#Ã " Â» ~ Âº " b Ã„ b  " Ã " Â» ~  Âº" Ã " Â» ~ 
Even if E ~ Â¸" Ã Ãƒ Ã " Â¹ is not a basis (but just an orthonormal set), we can
still consider the expansion
V# ~ Âº#Ã " Â»" b Ã„ b Âº#Ã " Â»"
Theorem 9.14 Let E ~ Â¸" Ã Ãƒ Ã " Â¹ be an orthonormal subset of an inner
product space = and let : ~ ÂºEÂ». The Fourier expansion with respect to E of
a vector # Â = is
V# ~ Âº#Ã " Â»" b Ã„ b Âº#Ã " Â»"
Each coefficient Âº#Ã " Â» is called a Fourier coefficient of # with respect to E .
The vector V# can be characterized as follows:
1) V# is the unique vector Â : for which Â²# c Â³ Â : .
2) V# is the best approximation to # from within : , that is, V# is the unique
vector Â : that is closest to #, in the sense that
)# c V#)  )# c )
for all Â : Â± V
Â¸#Â¹.

220

Advanced Linear Algebra

3) Bessel's inequality holds for all # Â = , that is
)V#) Â )#)
Proof. For part 1), since
Âº# c V#Ã " Â» ~ Âº#Ã " Â» c Âº#Ã
V " Â» ~ 
it follows that # c V# Â : Â . Also, if # c Â : Â for Â : , then c V# Â : and
c V# ~ Â²# c V#Â³ c Â²# c Â³ Â : Â
and so
~ V#. For part 2), if
Â²# c V#Â³ Â Â²#
V c Â³ and so

Â : , then # c V# Â : Â implies that

)# c ) ~ )# c V# b V# c ) ~ )# c V#) b )V# c )
Hence, )# c ) is smallest if and only if ~ V# and the smallest value is
)# c V#). We leave proof of Bessel's inequality as an exercise.
Theorem 9.15 (The projection theorem) If : is a finite-dimensional subspace
of an inner product space = , then
: ~ : p :Â
In particular, if # Â = , then
# ~ V# b Â²# c V#Â³ Â : p : Â
It follows that
dimÂ²= Â³ ~ dimÂ²:Â³ b dimÂ²: Â Â³
Proof. We have seen that # c V# Â : Â and so = ~ : b : Â . But : q : Â ~ Â¸Â¹
and so = ~ : p : Â .
The following example shows that the projection theorem may fail if : is not
finite-dimensional. Indeed, in the infinite-dimensional case, : must be a
complete subspace, but we postpone a discussion of this case until Chapter 13.
Example 9.6 As in Example 9.5, let = ~ M and let : be the subspace of all
sequences with finite support, that is, : is spanned by the vectors
 ~ Â²Ã Ãƒ Ã Ã Ã Ã Ãƒ Â³
where  has a  in the th coordinate and 's elsewhere. If % ~ Â²% Â³ Â : Â , then
% ~ Âº%Ã  Â» ~  for all  and so % ~ . Therefore, : Â ~ Â¸Â¹. However,
: p : Â ~ : Â£ M
The projection theorem has a variety of uses.

Real and Complex Inner Product Spaces

221

Theorem 9.16 Let = be an inner product space and let : be a finitedimensional subspace of = .
1) : ÂÂ ~ :
2) If ? Â‹ = and dimÂ²Âº?Â»Â³  B, then
? ÂÂ ~ Âº?Â»
Proof. For part 1), it is clear that : Â‹ : ÂÂ . On the other hand, if # Â : ÂÂ , then
the projection theorem implies that # ~ b Z where Â : and Z Â : Â . Then
Z
is orthogonal to both and # and so Z is orthogonal to itself. Hence, Z ~ 
and # ~ Â : and so : ~ : ÂÂ . We leave the proof of part 2) as an exercise.

Characterizing Orthonormal Bases
We can characterize orthonormal bases using Fourier expansions.
Theorem 9.17 Let E ~ Â¸" Ã Ãƒ Ã " Â¹ be an orthonormal subset of an inner
product space = and let : ~ ÂºEÂ». The following are equivalent:
1) E is an orthonormal basis for = .
2) ÂºEÂ»Â ~ Â¸Â¹
3) Every vector is equal to its Fourier expansion, that is, for all # Â = ,
V# ~ #
4) Bessel's identity holds for all # Â = , that is,
)V#) ~ )#)
5) Parseval's identity holds for all #Ã $ Â = , that is,
Âº#Ã $Â» ~ V
Â´#ÂµE h Â´$Âµ
VE
where
Â´#ÂµE h Â´$Âµ
V
V E ~ Âº#Ã " Â»Âº$Ã " Â» b Ã„ b Âº#Ã " Â»Âº$Ã " Â»
is the standard dot product in -  .
Proof. To see that 1) implies 2), if # Â ÂºEÂ»Â is nonzero, then E r Â¸#Â°)#)Â¹ is
orthonormal and so E is not maximal. Conversely, if E is not maximal, there is
an orthonormal set F for which E Â‰ F . Then any nonzero # Â F Â± E is in
ÂºEÂ»Â . Hence, 2) implies 1). We leave the rest of the proof as an exercise.

The Riesz Representation Theorem
We have been dealing with linear maps for some time. We now have a need for
conjugate linear maps.
Definition A function Â¢ = Â¦ > on complex vector spaces is conjugate linear
if it is additive,
Â²# b # Â³ ~ # b #

222

Advanced Linear Algebra

and
Â²#Â³ ~ #
for all  Â d. A conjugate isomorphism is a bijective conjugate linear map.
If % Â = , then the inner product function Âº h Ã %Â»Â¢ = Â¦ - defined by
Âº h Ã %Â»# ~ Âº#Ã %Â»
is a linear functional on = . Thus, the linear map  Â¢ = Â¦ = i defined by
% ~ Âº h Ã %Â»
is conjugate linear. Moreover, since Âº h Ã %Â» ~ Âº h Ã &Â» implies % ~ &, it follows
that  is injective and therefore a conjugate isomorphism (since = is finitedimensional).
Theorem 9.18 (The Riesz representation theorem) Let = be a finitedimensional inner product space.
1) The map Â¢ = Â¦ = i defined by
% ~ Âº h Ã %Â»
is a conjugate isomorphism. In particular, for each  Â = i , there exists a
unique vector % Â = for which  ~ Âº h Ã %Â», that is,
 # ~ Âº#Ã %Â»
for all # Â = . We call % the Riesz vector for  and denote it by 9 .
2) The map 9Â¢ = i Â¦ = defined by
9 ~ 9
is also a conjugate isomorphism, being the inverse of  . We will call this
map the Riesz map.
Proof. Here is the usual proof that  is surjective. If  ~ , then 9 ~ , so let
us assume that  Â£ . Then 2 ~ kerÂ² Â³ has codimension  and so
= ~ Âº$Â» p 2
for $ Â 2 Â . Letting % ~ $ for  Â - , we require that
 Â²#Â³ ~ Âº#Ã $Â»
and since this clearly holds for any # Â 2 , it is sufficient to show that it holds
for # ~ $, that is,
 Â²$Â³ ~ Âº$Ã $Â» ~ Âº$Ã $Â»
Thus,  ~  Â²$Â³Â°)$) and

Real and Complex Inner Product Spaces

9 ~

 Â²$Â³
) $) 

223

$

For part 2), we have
Âº#Ã 9 b  Â» ~ Â² b Â³Â²#Â³
~  Â²#Â³ b Â²#Â³
~ Âº#Ã 9 Â» b Âº#Ã 9 Â»
~ Âº#Ã 9 b 9 Â»
for all # Â = and so
9 b  ~ 9 b 9
Note that if = ~ s , then 9 ~ Â² Â² Â³Ã Ãƒ Ã  Â² Â³Â³, where Â² Ã Ãƒ Ã  Â³ is the
standard basis for s .

Exercises
1.
2.
3.
4.
5.

Prove that if a matrix 4 is unitary, upper triangular and has positive entries
on the main diagonal, must be the identity matrix.
Use the QR factorization to show that any triangularizable matrix is
unitarily (orthogonally) triangularizable.
Verify the statement concerning equality in the triangle inequality.
Prove the parallelogram law.
Prove the Apollonius identity
)$ c ") b )$ c #) ~

6.
7.



)" c #) b h$ c Â²" b #Â³h





Let = be an inner product space with basis 8 . Show that the inner product
is uniquely defined by the values Âº"Ã #Â», for all "Ã # Â 8 .
Prove that two vectors " and # in a real inner product space = are
orthogonal if and only if
)" b #) ~ )") b )#)

8.
9.

Show that an isometry is injective.
Use Zorn's lemma to show that any nontrivial inner product space has a
Hilbert basis.
10. Prove Bessel's inequality.
11. Prove that an orthonormal set E is a Hilbert basis for a finite-dimensional
vector space = if and only if V# ~ #, for all # Â = .
12. Prove that an orthonormal set E is a Hilbert basis for a finite-dimensional
vector space = if and only if Bessel's identity holds for all # Â = , that is, if
and only if

224

Advanced Linear Algebra

)V#) ~ )#)
for all # Â = .
13. Prove that an orthonormal set E is a Hilbert basis for a finite-dimensional
vector space = if and only if Parseval's identity holds for all #Ã $ Â = , that
is, if and only if
Âº#Ã $Â» ~ V
Â´#ÂµE h Â´$Âµ
VE
for all #Ã $ Â = .
14. Let " ~ Â² Ã Ãƒ Ã  Â³ and # ~ Â²  Ã Ãƒ Ã  Â³ be in s . The Cauchyâ€“Schwarz
inequality states that
(  b Ã„ b   ( Â Â² b Ã„ b  Â³Â²  b Ã„ b  Â³
Prove that we can do better:
Â²(  ( b Ã„ b (  (Â³ Â Â² b Ã„ b  Â³Â²  b Ã„ b


Â³

15. Let = be a finite-dimensional inner product space. Prove that for any subset
? of = , we have ? ÂÂ ~ spanÂ²?Â³.
16. Let F3 be the inner product space of all polynomials of degree at most 3,
under the inner product
B

ÂºÂ²%Â³Ã Â²%Â³Â» ~



Â²%Â³Â²%Â³ c% %

cB

Apply the Gramâ€“Schmidt process to the basis Â¸Ã %Ã % Ã % Â¹, thereby
computing the first four Hermite polynomials (at least up to a
multiplicative constant).
17. Verify uniqueness in the Riesz representation theorem.
18. Let = be a complex inner product space and let : be a subspace of = .
Suppose that # Â = is a vector for which Âº#Ã Â» b Âº Ã #Â» Â Âº Ã Â» for all
Â : . Prove that # Â : Â .
19. If = and > are inner product spaces, consider the function on = ^ >
defined by
ÂºÂ²# Ã $ Â³Ã Â²# Ã $ Â³Â» ~ Âº# Ã # Â» b Âº$ Ã $ Â»
Is this an inner product on = ^ > ?
20. A normed vector space over s or d is a vector space (over s or d)
together with a function ))Â¢ = Â¦ s for which for all "Ã # Â = and scalars 
we have
a) )#) ~ (()#)
b) )" b #) Â )") b )#)
c) )#) ~  if and only if # ~ 
If = is a real normed space (over s) and if the norm satisfies the
parallelogram law

Real and Complex Inner Product Spaces

225

)" b #) b )" c #) ~ )") b )#)
prove that the polarization identity
Âº"Ã #Â» ~


Â²)" b #) c )" c #) Â³


defines an inner product on = . Hint: Evaluate Âº"Ã %Â» b Âº#Ã %Â» to show
that Âº"Ã %Â» ~ Âº"Ã %Â» and Âº"Ã %Â» b Âº#Ã %Â» ~ Âº" b #Ã %Â». Then complete the
proof that Âº"Ã %Â» ~ Âº"Ã %Â».
21. Let : be a subspace of a finite-dimensional inner product space = . Prove
that each coset in = Â°: contains exactly one vector that is orthogonal to : .

Extensions of Linear Functionals
22. Let  be a linear functional on a subspace : of a finite-dimensional inner
product space = . Let  Â²#Â³ ~ Âº#Ã 9 Â». Suppose that  Â = i is an extension
of  , that is, O: ~  . What is the relationship between the Riesz vectors 9
and 9 ?
23. Let  be a nonzero linear functional on a subspace : of a finite-dimensional
inner product space = and let 2 ~ kerÂ² Â³. Show that if  Â = i is an
extension of  , then 9 Â 2 Â Â± : Â . Moreover, for each vector
" Â 2 Â Â± : Â there is exactly one scalar  for which the linear functional
Â²?Â³ ~ Âº?Ã "Â» is an extension of  .

Positive Linear Functionals on s
A vector # ~ Â² Ã Ãƒ Ã  Â³ in s is nonnegative (also called positive), written
# Â‚ , if  Â‚  for all . The vector # is strictly positive, written # Â€ , if # is
nonnegative but not . The set sb of all strictly positive vectors in s is called
the nonnegative orthant in s Ã€ The vector # is strongly positive, written
# Âˆ , if  Â€  for all . The set sbb , of all strongly positive vectors in s is
the strongly positive orthant in s Ã€
Let  Â¢ : Â¦ s be a linear functional on a subspace : of s . Then  is
nonnegative (also called positive), written  Â‚ , if
# Â€  Â¬  Â²#Â³ Â‚ 
for all # Â : and  is strictly positive, written  Â€ , if
# Â€  Â¬  Â²#Â³ Â€ 
for all # Â :Ã€
24. Prove that a linear functional  on s is positive if and only if 9 Â€  and
strictly positive if and only if 9 Âˆ . If : is a subspace of s is it true
that a linear functional  on : is nonnegative if and only if 9 Â€ ?

226

Advanced Linear Algebra

25. Let  Â¢ : Â¦ s be a strictly positive linear functional on a subspace : of s .
Prove that  has a strictly positive extension to s . Use the fact that if
< q s
b ~ Â¸Â¹, where
sb ~ Â¸Â² Ã Ãƒ Ã  Â³ Â“  Â‚  all Â¹
and < is a subspace of s , then < Â contains a strongly positive vector.
26. If = is a real inner product space, then we can define an inner product on its
complexification = d as follows (this is the same formula as for the ordinary
inner product on a complex vector space):
Âº" b #Ã % b &Â» ~ Âº"Ã %Â» b Âº#Ã &Â» b Â²Âº#Ã %Â» c Âº"Ã &Â»Â³
Show that
)Â²" b #Â³) ~ )") b )#)
where the norm on the left is induced by the inner product on = d and the
norm on the right is induced by the inner product on = .

Chapter 10

Structure Theory for Normal Operators

Throughout this chapter, all vector spaces are assumed to be finite-dimensional
unless otherwise noted. Also, the field - is either s or d.

The Adjoint of a Linear Operator
The purpose of this chapter is to study the structure of certain special types of
linear operators on finite-dimensional real and complex inner product spaces. In
order to define these operators, we introduce another type of adjoint (different
from the operator adjoint of Chapter 3).
Theorem 10.1 Let = and > be finite-dimensional inner product spaces over and let  Â BÂ²= Ã > Â³. Then there is a unique function  i Â¢ > Â¦ = , defined by
the condition
Âº #Ã $Â» ~ Âº#Ã  i $Â»
for all # Â = and $ Â > . This function is in BÂ²> Ã = Â³ and is called the adjoint
of  .
Proof. If  i exists, then it is unique, for if
Âº #Ã $Â» ~ Âº#Ã $Â»
then Âº#Ã $Â» ~ Âº#Ã  i $Â» for all # and $ and so  ~  i .
We seek a linear map  i Â¢ > Â¦ = for which
Âº#Ã  i $Â» ~ Âº #Ã $Â»
By way of motivation, the vector  i $, if it exists, looks very much like a linear
map sending # to Âº #Ã $Â». The only problem is that  i # is supposed to be a
vector, not a linear map. But the Riesz representation theorem tells us that linear
maps can be represented by vectors.

228

Advanced Linear Algebra

Specifically, for each $ Â > , the linear functional $ Â = i defined by
$ # ~ Âº #Ã $Â»
has the form
$ # ~ Âº#Ã 9$ Â»
where 9$ Â = is the Riesz vector for $ . If  i Â¢ > Â¦ = is defined by
 i $ ~ 9$ ~ 9Â²$ Â³
where 9 is the Riesz map, then
Âº#Ã  i $Â» ~ Âº#Ã 9$ Â» ~ $ # ~ Âº #Ã $Â»
Finally, since  i ~ 9 k  is the composition of the Riesz map 9 and the map
 Â¢ $ Âª $ and since both of these maps are conjugate linear, their composition
is linear.
Here are some of the basic properties of the adjoint.
Theorem 10.2 Let = and > be finite-dimensional inner product spaces. For
every Ã  Â BÂ²= Ã > Â³ and  Â - ,
1) Â² b  Â³i ~ i b  i
2) Â² Â³i ~  i
3)  ii ~  and so
Âº i #Ã $Â» ~ Âº#Ã  $Â»
4) If = ~ > , then Â² Â³i ~  i i
5) If  is invertible, then Â² c Â³i ~ Â² i Â³c
6) If = ~ > and Â²%Â³ Â sÂ´%Âµ, then Â² Â³i ~ Â² i Â³.
Moreover, if  Â BÂ²= Â³ and : is a subspace of = , then
7) : is  -invariant if and only if : Â is  i -invariant.
8) Â²:Ã : Â Â³ reduces  if and only if : is both  -invariant and  i -invariant, in
which case
Â² O: Â³i ~ Â² i Â³O:
Proof. For part 7), let Â : and ' Â : Â and write
Âº i 'Ã Â» ~ Âº'Ã  Â»
Now, if : is  -invariant, then Âº i 'Ã Â» ~  for all Â : and so  i ' Â : Â and
: Â is  i -invariant. Conversely, if : Â is  i -invariant, then Âº'Ã  Â» ~  for all
' Â : Â and so  Â : ÂÂ ~ : , whence : is  -invariant.
The first statement in part 8) follows from part 7) applied to both : and : Â . For
the second statement, since : is both  -invariant and  i -invariant, if Ã ! Â : ,

Structure Theory for Normal Operators

229

then
Âº Ã Â² i Â³O: Â²!Â³Â» ~ Âº Ã  i !Â» ~ Âº Ã !Â» ~ Âº O: Â² Â³Ã !Â»
Hence, by definition of adjoint, Â² i Â³O: ~ Â² O: Â³i .
Now let us relate the kernel and image of a linear transformation to those of its
adjoint.
Theorem 10.3 Let  Â BÂ²= Ã > Â³, where = and > are finite-dimensional inner
product spaces.
1)
kerÂ² i Â³ ~ imÂ² Â³Â

and

imÂ² i Â³ ~ kerÂ² Â³Â

 surjective
 injective

Â¯
Â¯

 i injective
 i surjective

kerÂ² i  Â³ ~ kerÂ² Â³

and

kerÂ² i Â³ ~ kerÂ² i Â³

imÂ² i  Â³ ~ imÂ² i Â³

and

imÂ² i Â³ ~ imÂ² Â³

and so

2)

3)

4)
Â²:Ã; Â³i ~ ; Â Ã: Â
Proof. For part 1),
" Â kerÂ² i Â³ Â¯  i " ~ 
Â¯ Âº i "Ã = Â» ~ Â¸Â¹
Â¯ Âº"Ã  = Â» ~ Â¸Â¹
Â¯ " Â imÂ² Â³Â
and so kerÂ² i Â³ ~ imÂ² Â³Â . The second equation in part 1) follows by replacing 
by  i and taking complements.
For part 2), it is clear that kerÂ² Â³ Â‹ kerÂ² i  Â³. For the reverse inclusion, we have
 i " ~ 

Â¬

Âº i  "Ã "Â» ~ 

Â¬

Âº "Ã  "Â» ~ 

Â¬

" ~ 

and so kerÂ² i  Â³ Â‹ kerÂ² Â³. The second equation follows from the first by
replacing  with  i . We leave the rest of the proof for the reader.

230

Advanced Linear Algebra

The Operator Adjoint and the Hilbert Space Adjoint
We should make some remarks about the relationship between the operator
adjoint  d of  , as defined in Chapter 3 and the adjoint  i that we have just
defined, which is sometimes called the Hilbert space adjoint. In the first place,
if  Â¢ = Â¦ > , then  d and  i have different domains and ranges:
 dÂ¢ > i Â¦ = i

and

 iÂ¢ > Â¦ =

The two maps are shown in Figure 10.1, along with the conjugate Riesz
isomorphisms 9 = Â¢ = i Â¦ = and 9 > Â¢ > i Â¦ > .

V*

Wx

W*

RV

RW

V

W*
W

W

Figure 10.1
i

i

The composite map Â¢ > Â¦ = defined by
 ~ Â²9 = Â³c k  i k 9 >
is linear. Moreover, for all  Â > i and # Â = ,
Â² d Â² Â³Â³# ~  Â² #Â³
~ Âº #Ã 9 > Â² Â³Â»
~ Âº#Ã  i 9 > Â² Â³Â»
~ Â´Â²9 = Â³c Â² i 9 > Â² Â³Â³ÂµÂ²#Â³
~ Â² Â³#
and so  ~  d . Hence, the relationship between  d and  i is
 d ~ Â²9 = Â³c k  i k 9 >
Loosely speaking, the Riesz functions are like â€œchange of variablesâ€ functions
from linear functionals to vectors, and we can say that  i does to Riesz vectors
what  d does to the corresponding linear functionals. Put another way (and just
as loosely),  and  i are the same, up to conjugate Riesz isomorphism.
In Chapter 3, we showed that the matrix of the operator adjoint  d is the
transpose of the matrix of the map  . For Hilbert space adjoints, the situation is
slightly different (due to the conjugate linearity of the inner product). Suppose
that 8 ~ Â² Ã Ãƒ Ã  Â³ and 9 ~ Â² Ã Ãƒ Ã  Â³ are ordered orthonormal bases for =
and > , respectively. Then

Structure Theory for Normal Operators

231

Â²Â´ i Âµ9Ã8 Â³Ã ~ Âº i  Ã  Â» ~ Âº Ã   Â» ~ Âº  Ã  Â» ~ Â²Â´ Âµ8Ã9 Â³Ã
and so Â´ i Âµ9Ã8 and Â´ Âµ8Ã9 are conjugate transposes. The conjugate transpose of a
matrix ( ~ Â²Ã Â³ is
(i ~ Â²Ã Â³!
and is called the adjoint of (.
Theorem 10.4 Let  Â BÂ²= Ã > Â³, where = and > are finite-dimensional inner
product spaces.
1) The operator adjoint  d and the Hilbert space adjoint  i are related by
 d ~ Â²9 = Â³c k  i k 9 >
where 9 = and 9 > are the conjugate Riesz isomorphisms on = and > ,
respectively.
2) If 8 and 9 are ordered orthonormal bases for = and > , respectively, then
Â´ i Âµ9Ã8 ~ Â²Â´ Âµ8Ã9 Â³i
In words, the matrix of the adjoint  i is the adjoint (conjugate transpose) of
the matrix of  .

Orthogonal Projections
In an inner product space, we can single out some special projection operators.
Definition A projection of the form :Ã: Â is said to be orthogonal.
Equivalently, a projection  is orthogonal if kerÂ²Â³ Â imÂ²Â³.
Some care must be taken to avoid confusion between orthogonal projections and
two projections that are orthogonal to each other, that is, for which
 ~  ~ .
We have seen that an operator  is a projection operator if and only if it is
idempotent. Here is the analogous characterization of orthogonal projections.
Theorem 10.5 Let = be a finite-dimensional inner product space. The following
are equivalent for an operator  on = :
1)  is an orthogonal projection
2)  is idempotent and self-adjoint
3)  is idempotent and does not expand lengths, that is
)#) Â )#)
for all # Â = .

